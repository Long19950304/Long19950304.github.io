---
layout: page
title: "Daily Digest — 2026-01-20"
lang: en
permalink: /digest/2026-01-20/
digest_date: 2026-01-20
card_image: /assets/img/digests/2026-01-20-en.png
translation_key: digest-2026-01-20
---

![Daily digest card](/assets/img/digests/2026-01-20-en.png)

This issue is a reading log and short takeaways; 1–2 items may be expanded later.

## Links

### CSS / AI & society

- [AI Sycophancy: How Users Flag and Respond](https://arxiv.org/abs/2601.10467v1) - Empirical signals for detecting/handling overly agreeable LLM behavior.
  - Takeaway: Examines how users detect and respond to sycophantic assistant behavior, and proposes observable signals relevant to trust and safety evaluation.
  - Open question: How should assistants balance politeness vs disagreement without drifting into sycophancy?

### AI frontier

- [Differential Transformer V2](https://huggingface.co/blog/microsoft/diff-attn-v2) - A new attention variant aimed at more efficient long-context modeling.
  - Takeaway: Proposes a differential attention mechanism (Diff Attn v2) to improve long-context efficiency; a concrete attention variant to compare against standard Transformers.
- [OpenAI partners with Cerebras](https://openai.com/index/cerebras-partnership) - A notable infra/inference partnership; watch for speed/cost implications.
  - Takeaway: Signals continued investment in specialized inference hardware partnerships, with implications for inference latency, cost, and availability.
- [Open Responses: What you need to know](https://huggingface.co/blog/open-responses) - A practical multi-model workflow for comparing and integrating LLM outputs.
  - Takeaway: Outlines an 'open responses' workflow for comparing multiple models and synthesizing outputs, emphasizing diversity and verification.

### Education / learning sciences / edtech

- [The Conversational Exam: A Scalable Assessment Design for the AI Era](https://arxiv.org/abs/2601.10691v1) - An assessment format designed to stay meaningful when students have AI.
  - Takeaway: Proposes conversational exams that assess reasoning through interaction (not only final answers), designed to remain meaningful when AI tools are available.
  - Open question: What guardrails would make conversational assessment fair, scalable, and less subjective?
- [Evaluating 21st-Century Competencies in Postsecondary Curricula with Large Language Models: Performance Benchmarking and Reasoning-Based Prompting Strategies](https://arxiv.org/abs/2601.10983v1) - Benchmarks LLMs for curricular competency evaluation and prompting strategies.
  - Takeaway: Benchmarks LLMs for evaluating 21st-century competencies in curricula and compares prompting strategies, informing more reliable automated curriculum auditing.
- [aiPlato: A Novel AI Tutoring and Step-wise Feedback System for Physics Homework](https://arxiv.org/abs/2601.09965v1) - A tutoring design emphasizing step-wise feedback (closer to how students learn).
  - Takeaway: An AI tutoring design centered on step-wise feedback (rather than full solutions), aligning with scaffolding-oriented learning support.
- [Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations](https://arxiv.org/abs/2601.09953v1) - Uses simulated "LLM students" to estimate item difficulty for assessment design.
  - Takeaway: Uses simulated 'LLM students' to estimate item difficulty, as a method option when real response data is limited (with calibration needs).
