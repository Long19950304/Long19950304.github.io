#!/usr/bin/env python3
"""
Daily digest generator for the personal site.

Workflow:
  1) Draft candidates from RSS/Atom feeds:
       personal-site/bin/digest draft --date 2026-01-20
  2) Edit the generated YAML (fill `why` fields, delete low-signal items).
  3) Build the digest page + image card:
       personal-site/bin/digest build --date 2026-01-20
  4) (Optional) Generate social post text (X thread + LinkedIn copy):
       personal-site/bin/digest social --date 2026-01-20
"""

from __future__ import annotations

import argparse
import datetime as dt
import json
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable

import feedparser
import yaml
from PIL import Image, ImageDraw, ImageFont


REPO_ROOT = Path(__file__).resolve().parents[1]
DATA_SOURCES = REPO_ROOT / "_data" / "digest_sources.yml"
DATA_DIGESTS_DIR = REPO_ROOT / "_data" / "digests"
DIGESTS_DIR = REPO_ROOT / "_digests"
ZH_DIGESTS_DIR = REPO_ROOT / "_digests_zh"
CARD_DIR = REPO_ROOT / "assets" / "img" / "digests"
DEFAULT_SITE_URL = "https://long19950304.github.io"


def _die(msg: str, code: int = 2) -> None:
    print(f"error: {msg}", file=sys.stderr)
    raise SystemExit(code)


def _ensure_dirs() -> None:
    DATA_DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    ZH_DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    CARD_DIR.mkdir(parents=True, exist_ok=True)


def _parse_date(s: str) -> dt.date:
    try:
        return dt.date.fromisoformat(s)
    except ValueError:
        _die(f"invalid --date '{s}', expected YYYY-MM-DD")


def _load_sources() -> list[dict[str, Any]]:
    if not DATA_SOURCES.exists():
        _die(f"missing {DATA_SOURCES}")
    raw = yaml.safe_load(DATA_SOURCES.read_text(encoding="utf-8")) or {}
    sources = raw.get("sources", [])
    if not isinstance(sources, list) or not sources:
        _die(f"no sources defined in {DATA_SOURCES}")
    return sources


def _norm(s: str) -> str:
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s


def _pick_entry_time(entry: Any) -> dt.datetime | None:
    # feedparser yields either .published_parsed or .updated_parsed
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    return dt.datetime(*t[:6], tzinfo=dt.timezone.utc)


def _clean_title(title: str) -> str:
    title = re.sub(r"\s+", " ", title or "").strip()
    # arXiv RSS titles sometimes include extra whitespace/line breaks.
    return title


def _draft_items(
    sources: list[dict[str, Any]],
    since_utc: dt.datetime,
    max_items: int,
) -> list[dict[str, Any]]:
    items: list[dict[str, Any]] = []
    for src in sources:
        url = src.get("url", "").strip()
        if not url:
            continue
        feed = feedparser.parse(url)
        for e in getattr(feed, "entries", []) or []:
            when = _pick_entry_time(e)
            if when and when < since_utc:
                continue
            title = _clean_title(getattr(e, "title", "") or "")
            link = (getattr(e, "link", "") or "").strip()
            if not title or not link:
                continue
            items.append(
                {
                    "category": src.get("category", "").strip() or "Misc",
                    "title": title,
                    "why": "",
                    "url": link,
                    "source": src.get("name", "").strip() or src.get("id", "").strip(),
                    "published_utc": when.isoformat() if when else "",
                }
            )

    # Deduplicate by URL, then by normalized title.
    seen_url: set[str] = set()
    seen_title: set[str] = set()
    deduped: list[dict[str, Any]] = []
    for it in sorted(items, key=lambda x: x.get("published_utc", ""), reverse=True):
        url = it.get("url", "")
        tnorm = _norm(it.get("title", ""))
        if url in seen_url or tnorm in seen_title:
            continue
        seen_url.add(url)
        seen_title.add(tnorm)
        deduped.append(it)

    return deduped[: max_items or 200]


def _save_yaml(path: Path, payload: Any) -> None:
    path.write_text(yaml.safe_dump(payload, sort_keys=False, allow_unicode=True), encoding="utf-8")


def _load_digest_yaml(path: Path) -> dict[str, Any]:
    if not path.exists():
        _die(f"missing digest file: {path}")
    raw = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
    if not isinstance(raw, dict):
        _die(f"invalid digest yaml: {path}")
    return raw


def _group_by_category(items: list[dict[str, Any]]) -> list[tuple[str, list[dict[str, Any]]]]:
    groups: dict[str, list[dict[str, Any]]] = {}
    for it in items:
        cat = (it.get("category", "") or "Misc").strip()
        groups.setdefault(cat, []).append(it)
    # Preserve a sensible, stable ordering: CSS first, then AI, then others.
    order = [
        "CSS / AI & society",
        "AI frontier",
        "Digital health / health comm",
        "Education / learning sciences / edtech",
        "Misc",
    ]
    ordered: list[tuple[str, list[dict[str, Any]]]] = []
    for k in order:
        if k in groups:
            ordered.append((k, groups.pop(k)))
    for k in sorted(groups.keys()):
        ordered.append((k, groups[k]))
    return ordered


CAT_ZH = {
    "CSS / AI & society": "CSS / AI 与社会",
    "AI frontier": "AI 前沿",
    "Digital health / health comm": "数字健康 / 健康传播",
    "Education / learning sciences / edtech": "教育 / 学习科学 / 教育技术",
    "Misc": "其他",
}


def _pick_lang(it: dict[str, Any], key_en: str, key_zh: str, lang: str) -> str:
    if lang == "zh":
        v = (it.get(key_zh) or "").strip()
        if v:
            return v
        # Fallback to English if Chinese is missing.
    return (it.get(key_en) or "").strip()


def _font_paths(lang: str) -> dict[str, Path]:
    """
    Pick fonts that actually contain the glyphs we need.

    For zh, we must use a CJK font, otherwise Pillow will render tofu boxes.
    """
    if lang == "zh":
        candidates = [
            ("/System/Library/Fonts/Hiragino Sans GB.ttc", "regular"),
            ("/System/Library/Fonts/STHeiti Medium.ttc", "bold"),
            ("/System/Library/Fonts/STHeiti Light.ttc", "regular"),
            ("/System/Library/Fonts/Supplemental/Songti.ttc", "regular"),
        ]
    else:
        # Prefer widely available macOS fonts.
        candidates = [
            ("/System/Library/Fonts/Supplemental/Arial.ttf", "regular"),
            ("/System/Library/Fonts/Supplemental/Arial Bold.ttf", "bold"),
            ("/System/Library/Fonts/Supplemental/Arial Unicode.ttf", "regular"),
            ("/System/Library/Fonts/Supplemental/Helvetica.ttc", "regular"),
        ]
    out: dict[str, Path] = {}
    for fp, role in candidates:
        p = Path(fp)
        if p.exists() and role not in out:
            out[role] = p
    return out


def _wrap(draw: ImageDraw.ImageDraw, text: str, font: ImageFont.FreeTypeFont, max_width: int) -> list[str]:
    """
    Simple word-wrapping that preserves spaces.

    Important: do NOT strip whitespace aggressively, otherwise words will run together
    in the rendered card (a common issue when splitting on whitespace).
    """
    text = re.sub(r"\s+", " ", (text or "").strip())
    if not text:
        return []

    words = text.split(" ")
    lines: list[str] = []
    cur = ""

    def flush() -> None:
        nonlocal cur
        if cur:
            lines.append(cur)
            cur = ""

    for w in words:
        test = w if not cur else f"{cur} {w}"
        if draw.textlength(test, font=font) <= max_width:
            cur = test
            continue

        # If a single token is too long, hard-wrap it by characters.
        if not cur and draw.textlength(w, font=font) > max_width:
            chunk = ""
            for ch in w:
                t2 = chunk + ch
                if draw.textlength(t2, font=font) <= max_width:
                    chunk = t2
                else:
                    if chunk:
                        lines.append(chunk)
                    chunk = ch
            if chunk:
                cur = chunk
            continue

        flush()
        cur = w

    flush()
    return lines


_URL_RE = re.compile(r"https?://\S+")


def _x_char_len(s: str) -> int:
    """
    Approximate X/Twitter length.

    X counts any URL as ~23 chars regardless of actual length. We approximate
    with 23 to split threads safely.
    """
    if not s:
        return 0
    urls = list(_URL_RE.findall(s))
    # Replace each URL with 23 chars in the count.
    return len(s) - sum(len(u) for u in urls) + 23 * len(urls)


def _digest_issue_urls(date: dt.date, site_url: str) -> dict[str, str]:
    base = (site_url or DEFAULT_SITE_URL).rstrip("/")
    return {
        "en": f"{base}/digest/{date.isoformat()}/",
        "zh": f"{base}/zh/digest/{date.isoformat()}/",
        "en_card": f"/assets/img/digests/{date.isoformat()}-en.png",
        "zh_card": f"/assets/img/digests/{date.isoformat()}-zh.png",
    }


def _format_item_bilingual(it: dict[str, Any]) -> str:
    t_en = (it.get("title") or "").strip()
    w_en = (it.get("why") or "").strip()
    t_zh = (it.get("title_zh") or "").strip()
    w_zh = (it.get("why_zh") or "").strip()
    url = (it.get("url") or "").strip()

    en = t_en
    if w_en:
        en = f"{en} — {w_en}"
    zh = t_zh or ""
    if zh and w_zh:
        zh = f"{zh} — {w_zh}"
    elif not zh and w_zh:
        zh = w_zh

    lines: list[str] = []
    if en:
        lines.append(f"- {en}")
    if zh:
        lines.append(f"  {zh}")
    if url:
        lines.append(f"  {url}")
    return "\n".join(lines).strip()


def _format_linkedin_post(date: dt.date, digest: dict[str, Any], site_url: str) -> str:
    items = digest.get("items", [])
    groups = _group_by_category(items)
    urls = _digest_issue_urls(date, site_url)

    title_en = (digest.get("title") or f"Daily Digest — {date.isoformat()}").strip()
    title_zh = (digest.get("title_zh") or f"每日简报 — {date.isoformat()}").strip()

    lines: list[str] = [
        f"{title_en} / {title_zh}",
        "",
        "Links (EN):",
        urls["en"],
        "",
        "链接（中文）：",
        urls["zh"],
        "",
        "Top items / 今日要点：",
        "",
    ]

    for cat, cat_items in groups:
        if not cat_items:
            continue
        cat_zh = CAT_ZH.get(cat, cat)
        lines.append(f"{cat} / {cat_zh}")
        for it in cat_items[:10]:
            lines.append(_format_item_bilingual(it))
        lines.append("")

    # Hashtags are intentionally few; keep it academic.
    lines.extend(
        [
            "#ComputationalSocialScience #AIEvaluation #HealthCommunication #EdTech",
        ]
    )
    return "\n".join(lines).rstrip() + "\n"


def _format_x_thread(date: dt.date, digest: dict[str, Any], site_url: str) -> list[str]:
    items = digest.get("items", [])
    groups = _group_by_category(items)
    urls = _digest_issue_urls(date, site_url)

    # Tweet 1: short bilingual intro + canonical links.
    t0 = "\n".join(
        [
            f"Daily Digest / 每日简报 ({date.isoformat()})",
            f"EN: {urls['en']}",
            f"中文: {urls['zh']}",
            "",
            "Direct links in thread / 详细链接见串：",
        ]
    ).strip()

    tweets: list[str] = [t0]

    # Thread: compact per-item blocks (avoid orphan headings when splitting).
    blocks: list[str] = []
    for cat, cat_items in groups:
        if not cat_items:
            continue
        for it in cat_items[:10]:
            t_en = (it.get("title") or "").strip()
            w_en = (it.get("why") or "").strip()
            t_zh = (it.get("title_zh") or "").strip()
            w_zh = (it.get("why_zh") or "").strip()
            url = (it.get("url") or "").strip()

            cat_zh = CAT_ZH.get(cat, cat)
            cat_label = f"[{cat}]/{cat_zh}"

            line = t_en
            if w_en:
                line = f"{line} — {w_en}"
            if t_zh:
                zh_line = t_zh
                if w_zh:
                    zh_line = f"{zh_line} — {w_zh}"
                line = f"{cat_label} {line}\n{zh_line}"
            elif w_zh:
                line = f"{cat_label} {line}\n{w_zh}"
            else:
                line = f"{cat_label} {line}"
            if url:
                line = f"{line}\n{url}"
            blocks.append(line.strip())

    # Pack blocks into tweets with conservative length.
    cur: list[str] = []
    cur_len = 0
    max_len = 265  # keep headroom for platform differences
    for b in blocks:
        add = (b + "\n\n").strip() if b else ""
        if not add:
            continue
        add_len = _x_char_len(add) + (2 if cur else 0)
        if cur and cur_len + add_len > max_len:
            tweets.append("\n\n".join(cur).strip())
            cur = [b]
            cur_len = _x_char_len(b)
            continue
        if cur:
            cur_len += _x_char_len("\n\n" + b)
            cur.append(b)
        else:
            cur = [b]
            cur_len = _x_char_len(b)
    if cur:
        tweets.append("\n\n".join(cur).strip())

    return tweets


def render_card(date: dt.date, items: list[dict[str, Any]], out_path: Path, *, lang: str) -> None:
    # 4:5 aspect ratio is friendly for both LinkedIn and X.
    w, h = 1080, 1350
    bg = (248, 250, 252)  # very light gray-blue
    ink = (15, 23, 42)    # slate-900
    sub = (51, 65, 85)    # slate-700
    rule = (226, 232, 240)

    img = Image.new("RGB", (w, h), bg)
    draw = ImageDraw.Draw(img)

    fonts = _font_paths(lang)
    font_title = ImageFont.truetype(str(fonts.get("bold", fonts.get("regular"))), 52) if fonts else ImageFont.load_default()
    font_h2 = ImageFont.truetype(str(fonts.get("bold", fonts.get("regular"))), 30) if fonts else ImageFont.load_default()
    font_body = ImageFont.truetype(str(fonts.get("regular", fonts.get("bold"))), 26) if fonts else ImageFont.load_default()
    font_small = ImageFont.truetype(str(fonts.get("regular", fonts.get("bold"))), 22) if fonts else ImageFont.load_default()

    pad = 72
    x = pad
    y = 64

    title = "Daily Digest" if lang != "zh" else "每日简报"
    draw.text((x, y), title, fill=ink, font=font_title)
    y += 64

    date_str = date.isoformat()
    draw.text((x, y), date_str, fill=sub, font=font_small)
    y += 32

    # Top rule
    draw.line((x, y, w - pad, y), fill=rule, width=2)
    y += 26

    groups = _group_by_category(items)
    max_items_total = 10
    rendered = 0

    for cat, cat_items in groups:
        if rendered >= max_items_total:
            break
        if not cat_items:
            continue
        cat_label = cat if lang != "zh" else CAT_ZH.get(cat, cat)
        draw.text((x, y), cat_label, fill=ink, font=font_h2)
        y += 40

        for it in cat_items:
            if rendered >= max_items_total:
                break
            body = _pick_lang(it, "title", "title_zh", lang)
            why = _pick_lang(it, "why", "why_zh", lang)
            if why:
                body = f"{body} - {why}"
            # Keep bullets compact.
            bullet = "- "
            lines = _wrap(draw, body, font_body, max_width=w - pad * 2 - 20)
            if not lines:
                continue
            # 2-line cap per item to avoid clutter.
            lines = lines[:2]
            draw.text((x, y), bullet + lines[0], fill=sub, font=font_body)
            y += 34
            if len(lines) > 1:
                draw.text((x + 26, y), lines[1], fill=sub, font=font_body)
                y += 34
            y += 10
            rendered += 1

        y += 14
        if y > h - 220:
            break

    # Bottom rule + footer
    y = h - 160
    draw.line((x, y, w - pad, y), fill=rule, width=2)
    y += 22
    footer = "Zhilong George Zhao | @longlalaland | long19950304.github.io"
    draw.text((x, y), footer, fill=sub, font=font_small)

    out_path.parent.mkdir(parents=True, exist_ok=True)
    img.save(out_path, format="PNG", optimize=True)


def _md_for_lang(date: dt.date, digest: dict[str, Any], *, lang: str) -> tuple[Path, Path]:
    items = digest.get("items", [])
    if not isinstance(items, list):
        _die("digest.items must be a list")
    if not items:
        _die("digest has no items; skip build or add items first")

    # Image card
    suffix = "zh" if lang == "zh" else "en"
    card_path = CARD_DIR / f"{date.isoformat()}-{suffix}.png"
    render_card(date, items, card_path, lang=lang)

    # Markdown page (collection item)
    out_dir = ZH_DIGESTS_DIR if lang == "zh" else DIGESTS_DIR
    out_dir.mkdir(parents=True, exist_ok=True)
    md_path = out_dir / f"{date.isoformat()}.md"

    title = digest.get("title") or f"Daily Digest - {date.isoformat()}"
    if lang == "zh":
        title = digest.get("title_zh") or f"每日简报 - {date.isoformat()}"

    groups = _group_by_category(items)
    lines: list[str] = [
        "---",
        "layout: page",
        f'title: "{title}"',
        f"lang: {lang}",
        f"permalink: /{'zh/' if lang == 'zh' else ''}digest/{date.isoformat()}/",
        f"digest_date: {date.isoformat()}",
        f"card_image: /assets/img/digests/{date.isoformat()}-{suffix}.png",
        f"translation_key: digest-{date.isoformat()}",
        "---",
        "",
        f"![Daily digest card](/assets/img/digests/{date.isoformat()}-{suffix}.png)",
        "",
        "## Links" if lang != "zh" else "## 链接",
        "",
    ]

    for cat, cat_items in groups:
        if not cat_items:
            continue
        cat_label = cat if lang != "zh" else CAT_ZH.get(cat, cat)
        lines.append(f"### {cat_label}")
        lines.append("")
        for it in cat_items:
            t = _pick_lang(it, "title", "title_zh", lang)
            u = it.get("url", "").strip()
            why = _pick_lang(it, "why", "why_zh", lang)
            if not t or not u:
                continue
            if why:
                lines.append(f"- [{t}]({u}) - {why}")
            else:
                lines.append(f"- [{t}]({u})")
        lines.append("")

    md_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
    return md_path, card_path


def build_digest(date: dt.date, digest: dict[str, Any]) -> list[tuple[str, Path, Path]]:
    out: list[tuple[str, Path, Path]] = []
    out.append(("en",) + _md_for_lang(date, digest, lang="en"))
    out.append(("zh",) + _md_for_lang(date, digest, lang="zh"))
    return out


def cmd_draft(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    sources = _load_sources()
    # Since: last N hours from now (UTC). This keeps drafts useful for daily runs.
    since_utc = dt.datetime.now(dt.timezone.utc) - dt.timedelta(hours=args.hours)
    items = _draft_items(sources, since_utc=since_utc, max_items=args.max_items)

    out = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if out.exists() and not args.force:
        out = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"

    payload = {"date": date.isoformat(), "title": f"Daily Digest - {date.isoformat()}", "items": items}
    _save_yaml(out, payload)
    print(f"Wrote {out.relative_to(REPO_ROOT)} ({len(items)} items).")
    print("Next: edit `why` fields + delete low-signal items, then run:")
    print(f"  {REPO_ROOT}/bin/digest build --date {date.isoformat()}")


def cmd_build(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    digest_path = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if not digest_path.exists():
        # If only draft exists, use it.
        draft = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"
        if draft.exists():
            digest_path = draft
        else:
            _die(f"missing digest YAML for {date.isoformat()} (expected {digest_path})")
    digest = _load_digest_yaml(digest_path)
    results = build_digest(date, digest)
    for lang, md_path, card_path in results:
        print(f"Wrote {md_path.relative_to(REPO_ROOT)} ({lang})")
        print(f"Wrote {card_path.relative_to(REPO_ROOT)} ({lang})")


def cmd_social(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    digest_path = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if not digest_path.exists():
        draft = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"
        if draft.exists():
            digest_path = draft
        else:
            _die(f"missing digest YAML for {date.isoformat()} (expected {digest_path})")

    digest = _load_digest_yaml(digest_path)

    if args.platform in ("linkedin", "both"):
        print("==== LinkedIn (copy/paste) ====")
        print(_format_linkedin_post(date, digest, site_url=args.site_url))

    if args.platform in ("x", "both"):
        print("==== X thread (copy/paste) ====")
        tweets = _format_x_thread(date, digest, site_url=args.site_url)
        total = len(tweets)
        for i, t in enumerate(tweets, start=1):
            print(f"\n--- Tweet {i}/{total} ---\n{t}\n")


def main(argv: list[str]) -> int:
    parser = argparse.ArgumentParser(prog="digest")
    sub = parser.add_subparsers(dest="cmd", required=True)

    p_draft = sub.add_parser("draft", help="Draft a digest YAML from RSS/Atom sources")
    p_draft.add_argument("--date", required=True, help="YYYY-MM-DD (label for this issue)")
    p_draft.add_argument("--hours", type=int, default=36, help="Look back window (UTC), default: 36")
    p_draft.add_argument("--max-items", type=int, default=40, help="Max candidate items to write, default: 40")
    p_draft.add_argument("--force", action="store_true", help="Overwrite existing YYYY-MM-DD.yml")
    p_draft.set_defaults(func=cmd_draft)

    p_build = sub.add_parser("build", help="Build digest page + image card from YAML")
    p_build.add_argument("--date", required=True, help="YYYY-MM-DD")
    p_build.set_defaults(func=cmd_build)

    p_social = sub.add_parser("social", help="Generate bilingual social post text (LinkedIn + X)")
    p_social.add_argument("--date", required=True, help="YYYY-MM-DD")
    p_social.add_argument(
        "--platform",
        choices=["both", "linkedin", "x"],
        default="both",
        help="Which post text to emit, default: both",
    )
    p_social.add_argument(
        "--site-url",
        default=DEFAULT_SITE_URL,
        help=f"Canonical site URL for digest links, default: {DEFAULT_SITE_URL}",
    )
    p_social.set_defaults(func=cmd_social)

    args = parser.parse_args(argv)
    args.func(args)
    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
