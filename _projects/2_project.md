---
layout: page
title: "Reliable LLM-Assisted Qualitative Analysis"
description: "Benchmarks, calibration, and QA for human/LLM hybrid coding in communication research."
title_zh: "LLM 辅助质性分析的可靠性"
description_zh: "面向传播研究的人机协同编码：基准评测、校准与质量保障（QA），让复杂质性分析更可辩护。"
img: /assets/img/projects/llm-evaluation.svg
importance: 2
category: research
lang: en
permalink: /projects/2_project/
translation_key: project-llm-qual-qa
---

I work on reliability and evaluation for LLM-assisted qualitative analysis: how to design benchmarks, calibrate outputs, and run QA for complex coding tasks so results remain defensible in real research workflows.

Representative directions:
- Coding quality assessment and calibration (complex labels, multi-stage rubrics, error analysis)
- Benchmark design for domain-specific annotation tasks
- Reproducible pipelines for human/LLM hybrid coding (auditability and responsible use)

## Selected outputs & working papers (2025+)

{% include research_direction_list.liquid ids="long1-desk1-bds-confidence-diversity-framework,long1-desk1-scirep-confidence-diversity-framework,long1-desk1-hec-hierarchical-error-framework" %}

<p class="small">
See also: the full research portfolio with cite/export tools on <a href="/research/">Research</a>.
</p>
