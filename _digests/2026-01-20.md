---
layout: page
title: "Daily Digest — 2026-01-20"
lang: en
permalink: /digest/2026-01-20/
digest_date: 2026-01-20
card_image: /assets/img/digests/2026-01-20-en.png
translation_key: digest-2026-01-20
---

![Daily digest card](/assets/img/digests/2026-01-20-en.png)

I’m trying to build a small habit: skim widely, then pick 1–2 things to unpack properly.

## Links

### CSS / AI & society

- [AI Sycophancy: How Users Flag and Respond](https://arxiv.org/abs/2601.10467v1) - Empirical signals for detecting/handling overly agreeable LLM behavior.
  - Takeaway: Studies how users notice and react to sycophantic assistant behavior; offers observable signals and implications for trust and safety evaluation.
  - Open question: How should assistants balance politeness vs disagreement without drifting into sycophancy?

### AI frontier

- [Differential Transformer V2](https://huggingface.co/blog/microsoft/diff-attn-v2) - A new attention variant aimed at more efficient long-context modeling.
  - Takeaway: Proposes a differential attention mechanism (Diff Attn v2) to improve long-context efficiency; a useful reference point when benchmarking attention variants against standard Transformers.
- [OpenAI partners with Cerebras](https://openai.com/index/cerebras-partnership) - A notable infra/inference partnership; watch for speed/cost implications.
  - Takeaway: Signals continued investment in specialized inference hardware partnerships; this can reshape latency/cost trade-offs and who gets access to fast inference at scale.
- [Open Responses: What you need to know](https://huggingface.co/blog/open-responses) - A practical multi-model workflow for comparing and integrating LLM outputs.
  - Takeaway: Presents an 'open responses' pattern for comparing multiple models and synthesizing outputs; useful for building robust pipelines with diversity + verification.

### Education / learning sciences / edtech

- [The Conversational Exam: A Scalable Assessment Design for the AI Era](https://arxiv.org/abs/2601.10691v1) - An assessment format designed to stay meaningful when students have AI.
  - Takeaway: Proposes conversational exams that evaluate reasoning through interaction rather than final answers, aiming to stay meaningful when students have access to AI tools.
  - Open question: What guardrails would make conversational assessment fair, scalable, and less subjective?
- [Evaluating 21st-Century Competencies in Postsecondary Curricula with Large Language Models: Performance Benchmarking and Reasoning-Based Prompting Strategies](https://arxiv.org/abs/2601.10983v1) - Benchmarks LLMs for curricular competency evaluation and prompting strategies.
  - Takeaway: Benchmarks LLMs for evaluating 21st-century competencies in curricula and tests reasoning-based prompting; suggests how automated curriculum auditing can be made more reliable.
- [aiPlato: A Novel AI Tutoring and Step-wise Feedback System for Physics Homework](https://arxiv.org/abs/2601.09965v1) - A tutoring design emphasizing step-wise feedback (closer to how students learn).
  - Takeaway: An AI tutoring design that focuses on step-wise feedback rather than giving full solutions; a concrete pattern for scaffolding student learning.
- [Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations](https://arxiv.org/abs/2601.09953v1) - Uses simulated "LLM students" to estimate item difficulty for assessment design.
  - Takeaway: Uses simulated 'LLM students' to estimate item difficulty, offering a method idea for item analysis when real response data is limited.
