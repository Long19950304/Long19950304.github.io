date: "2026-01-20"
title: "Daily Digest — 2026-01-20"
title_zh: "每日简报 — 2026-01-20"
note: "I’m trying to build a small habit: skim widely, then pick 1–2 things to unpack properly."
note_zh: "我在练一个小习惯：先扫一圈，再挑 1–2 条认真读。"
items:
  - category: "AI frontier"
    title: "Differential Transformer V2"
    title_zh: "Differential Transformer V2"
    why: "A new attention variant aimed at more efficient long-context modeling."
    why_zh: "一种新的注意力变体，聚焦更高效的长上下文建模。"
    takeaway: "Proposes a differential attention mechanism (Diff Attn v2) to improve long-context efficiency; a useful reference point when benchmarking attention variants against standard Transformers."
    takeaway_zh: "Diff Attn v2 这类注意力改造主要是在追求更“省算力”的长上下文；适合拿来和标准 Transformer 对照看收益与代价。"
    prompt: ""
    prompt_zh: ""
    url: "https://huggingface.co/blog/microsoft/diff-attn-v2"
    source: "Hugging Face Blog"

  - category: "AI frontier"
    title: "OpenAI partners with Cerebras"
    title_zh: "OpenAI 与 Cerebras 合作"
    why: "A notable infra/inference partnership; watch for speed/cost implications."
    why_zh: "值得关注的算力/推理合作，可能影响速度与成本。"
    takeaway: "Signals continued investment in specialized inference hardware partnerships; this can reshape latency/cost trade-offs and who gets access to fast inference at scale."
    takeaway_zh: "一条基础设施新闻：模型方继续把推理压到更专用的硬件上，可能带来更低延迟/成本，也会改变“谁能用到高速推理”的格局。"
    prompt: ""
    prompt_zh: ""
    url: "https://openai.com/index/cerebras-partnership"
    source: "OpenAI Blog"

  - category: "AI frontier"
    title: "Open Responses: What you need to know"
    title_zh: "Open Responses：你需要了解什么"
    why: "A practical multi-model workflow for comparing and integrating LLM outputs."
    why_zh: "一个可操作的多模型工作流，用于对比与整合大模型输出。"
    takeaway: "Presents an 'open responses' pattern for comparing multiple models and synthesizing outputs; useful for building robust pipelines with diversity + verification."
    takeaway_zh: "一个多模型对照+整合的工作流范式：用多样性拿思路，用验证把结论收敛到更稳。"
    prompt: ""
    prompt_zh: ""
    url: "https://huggingface.co/blog/open-responses"
    source: "Hugging Face Blog"

  - category: "CSS / AI & society"
    title: "AI Sycophancy: How Users Flag and Respond"
    title_zh: "AI 迎合：用户如何标记与回应"
    why: "Empirical signals for detecting/handling overly agreeable LLM behavior."
    why_zh: "提供可观测的证据线索，帮助识别/处理过度迎合的 LLM 行为。"
    takeaway: "Studies how users notice and react to sycophantic assistant behavior; offers observable signals and implications for trust and safety evaluation."
    takeaway_zh: "这篇文章看用户在什么情况下会觉得模型在迎合，以及他们会怎么反馈；对做信任/安全评估挺有启发。"
    prompt: "How should assistants balance politeness vs disagreement without drifting into sycophancy?"
    prompt_zh: "助手应如何在“礼貌”与“必要的不同意”之间平衡，而不滑向迎合？"
    url: "https://arxiv.org/abs/2601.10467v1"
    source: "arXiv"

  - category: "Education / learning sciences / edtech"
    title: "The Conversational Exam: A Scalable Assessment Design for the AI Era"
    title_zh: "对话式考试：AI 时代可扩展的评估设计"
    why: "An assessment format designed to stay meaningful when students have AI."
    why_zh: "一种面向 AI 时代的评估形式，尽量保持测评的有效性。"
    takeaway: "Proposes conversational exams that evaluate reasoning through interaction rather than final answers, aiming to stay meaningful when students have access to AI tools."
    takeaway_zh: "把考试做成对话过程，重点评估推理与解释而不是答案本身；在学生能用 AI 工具的情况下更有意义。"
    prompt: "What guardrails would make conversational assessment fair, scalable, and less subjective?"
    prompt_zh: "哪些规则/机制能让对话式测评更公平、可扩展、并降低主观性？"
    url: "https://arxiv.org/abs/2601.10691v1"
    source: "arXiv"

  - category: "Education / learning sciences / edtech"
    title: "Evaluating 21st-Century Competencies in Postsecondary Curricula with Large Language Models: Performance Benchmarking and Reasoning-Based Prompting Strategies"
    title_zh: "用大语言模型评估高等教育课程的 21 世纪能力：性能基准与推理式提示策略"
    why: "Benchmarks LLMs for curricular competency evaluation and prompting strategies."
    why_zh: "针对课程能力评估任务，对模型表现与提示策略进行基准测试。"
    takeaway: "Benchmarks LLMs for evaluating 21st-century competencies in curricula and tests reasoning-based prompting; suggests how automated curriculum auditing can be made more reliable."
    takeaway_zh: "用 LLM 去读课程大纲、评估能力覆盖，并比较不同提示策略；很适合作为自动化课程审计的起点。"
    prompt: ""
    prompt_zh: ""
    url: "https://arxiv.org/abs/2601.10983v1"
    source: "arXiv"

  - category: "Education / learning sciences / edtech"
    title: "aiPlato: A Novel AI Tutoring and Step-wise Feedback System for Physics Homework"
    title_zh: "aiPlato：用于物理作业的 AI 辅导与分步反馈系统"
    why: "A tutoring design emphasizing step-wise feedback (closer to how students learn)."
    why_zh: "强调分步反馈的辅导设计，更贴近学生的学习过程。"
    takeaway: "An AI tutoring design that focuses on step-wise feedback rather than giving full solutions; a concrete pattern for scaffolding student learning."
    takeaway_zh: "强调分步提示/反馈，让学生自己走完推理链；比直接给答案更像辅导。"
    prompt: ""
    prompt_zh: ""
    url: "https://arxiv.org/abs/2601.09965v1"
    source: "arXiv"

  - category: "Education / learning sciences / edtech"
    title: "Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations"
    title_zh: "拿出你的计算器：用“LLM 学生模拟”估计题目真实难度"
    why: "Uses simulated \"LLM students\" to estimate item difficulty for assessment design."
    why_zh: "用模拟的“LLM 学生”来估计题目难度，服务于测评设计。"
    takeaway: "Uses simulated 'LLM students' to estimate item difficulty, offering a method idea for item analysis when real response data is limited."
    takeaway_zh: "用“模拟学生”来估题目难度：当真实作答数据不够时是个替代办法，但最好配套校准/验证。"
    prompt: ""
    prompt_zh: ""
    url: "https://arxiv.org/abs/2601.09953v1"
    source: "arXiv"
