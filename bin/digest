#!/usr/bin/env python3
"""
Daily digest generator for the personal site.

Workflow:
  1) Draft candidates from RSS/Atom feeds:
       personal-site/bin/digest draft --date 2026-01-20
  2) Edit the generated YAML (fill `why` fields, delete low-signal items).
  3) Build the digest page:
       personal-site/bin/digest build --date 2026-01-20
  4) (Optional) Generate social post text (X thread + LinkedIn copy):
       personal-site/bin/digest social --date 2026-01-20
"""

from __future__ import annotations

import argparse
import datetime as dt
import json
import re
import sys
import html
import socket
import shutil
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable

import feedparser
import yaml
from PIL import Image, ImageDraw, ImageFont


REPO_ROOT = Path(__file__).resolve().parents[1]
DATA_SOURCES = REPO_ROOT / "_data" / "digest_sources.yml"
DATA_DIGESTS_DIR = REPO_ROOT / "_data" / "digests"
DIGESTS_DIR = REPO_ROOT / "_digests"
ZH_DIGESTS_DIR = REPO_ROOT / "_digests_zh"
CARD_DIR = REPO_ROOT / "assets" / "img" / "digests"
DEFAULT_SITE_URL = "https://long19950304.github.io"
FETCH_TIMEOUT_SECONDS = 8
FETCH_WORKERS = 8
DEFAULT_MAX_ITEMS_ON_SITE = 100

# feedparser uses urllib under the hood and may otherwise block indefinitely on slow/blocked feeds.
socket.setdefaulttimeout(FETCH_TIMEOUT_SECONDS)


def _die(msg: str, code: int = 2) -> None:
    print(f"error: {msg}", file=sys.stderr)
    raise SystemExit(code)


def _ensure_dirs() -> None:
    DATA_DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    ZH_DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    CARD_DIR.mkdir(parents=True, exist_ok=True)

def _optimize_png(path: Path) -> None:
    """
    Make generated cards more web-friendly.

    - Keep it optional: if pngquant isn't installed, do nothing.
    - If optimization isn't smaller, keep the original (avoid quality regressions).
    - Write back in-place to preserve file owner/group on macOS volumes.
    """
    exe = shutil.which("pngquant")
    if not exe or not path.exists():
        return

    tmp = path.with_suffix(path.suffix + ".tmp")
    try:
        orig_size = path.stat().st_size
        subprocess.run(
            [exe, "--force", "--strip", "--speed", "1", "--quality", "80-95", "-o", str(tmp), str(path)],
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        if tmp.exists() and tmp.stat().st_size < orig_size:
            path.write_bytes(tmp.read_bytes())
    except Exception as e:
        print(f"warn: pngquant failed for {path.name}: {e}", file=sys.stderr)
    finally:
        try:
            tmp.unlink()
        except FileNotFoundError:
            pass


def _parse_date(s: str) -> dt.date:
    try:
        return dt.date.fromisoformat(s)
    except ValueError:
        _die(f"invalid --date '{s}', expected YYYY-MM-DD")


def _load_sources() -> list[dict[str, Any]]:
    if not DATA_SOURCES.exists():
        _die(f"missing {DATA_SOURCES}")
    raw = yaml.safe_load(DATA_SOURCES.read_text(encoding="utf-8")) or {}
    sources = raw.get("sources", [])
    if not isinstance(sources, list) or not sources:
        _die(f"no sources defined in {DATA_SOURCES}")
    return sources


def _norm(s: str) -> str:
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s


def _pick_entry_time(entry: Any) -> dt.datetime | None:
    # feedparser yields either .published_parsed or .updated_parsed
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    return dt.datetime(*t[:6], tzinfo=dt.timezone.utc)


def _clean_title(title: str) -> str:
    title = re.sub(r"\s+", " ", title or "").strip()
    # arXiv RSS titles sometimes include extra whitespace/line breaks.
    return title


def _strip_html(s: str) -> str:
    # RSS summaries often include HTML tags. Keep a plain-text snippet for drafting.
    s = html.unescape(s or "")
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _first_sentence(s: str, max_len: int = 160) -> str:
    s = re.sub(r"\s+", " ", (s or "").strip())
    if not s:
        return ""
    # Heuristic sentence split. Good enough for drafting.
    m = re.split(r"(?<=[.!?])\s+", s, maxsplit=1)
    out = (m[0] if m else s).strip()
    if len(out) > max_len:
        out = out[: max_len - 1].rstrip() + "…"
    return out


def _truncate(s: str, max_len: int) -> str:
    s = re.sub(r"\s+", " ", (s or "").strip())
    if len(s) <= max_len:
        return s
    return s[: max_len - 1].rstrip() + "…"


def _draft_items(
    sources: list[dict[str, Any]],
    since_utc: dt.datetime,
    max_items: int,
) -> list[dict[str, Any]]:
    items: list[dict[str, Any]] = []

    def fetch(src: dict[str, Any]) -> tuple[dict[str, Any], Any | None]:
        url = (src.get("url") or "").strip()
        if not url:
            return (src, None)
        try:
            return (src, feedparser.parse(url))
        except Exception as e:
            name = src.get("name", "").strip() or src.get("id", "").strip() or "unknown"
            print(f"warn: failed to fetch '{name}' ({url}): {e}", file=sys.stderr)
            return (src, None)

    # Fetch feeds concurrently; slow/blocked feeds shouldn't stall the whole daily run.
    with ThreadPoolExecutor(max_workers=FETCH_WORKERS) as ex:
        futs = [ex.submit(fetch, s) for s in sources]
        for fut in as_completed(futs):
            src, feed = fut.result()
            if not feed:
                continue
            for e in getattr(feed, "entries", []) or []:
                when = _pick_entry_time(e)
                if when and when < since_utc:
                    continue
                title = _clean_title(getattr(e, "title", "") or "")
                link = (getattr(e, "link", "") or "").strip()
                summary = _strip_html(getattr(e, "summary", "") or getattr(e, "description", "") or "")
                if not title or not link:
                    continue
                # Provide draft-level snippets (user should rewrite for quality).
                takeaway = _truncate(summary, 360) if summary else ""
                why = _first_sentence(summary, 140) if summary else ""
                items.append(
                    {
                        "category": src.get("category", "").strip() or "Misc",
                        "title": title,
                        "why": why,
                        "takeaway": takeaway,
                        "prompt": "",
                        "url": link,
                        "source": src.get("name", "").strip() or src.get("id", "").strip(),
                        "published_utc": when.isoformat() if when else "",
                        # Chinese fields are intentionally left blank for human curation.
                        "title_zh": "",
                        "why_zh": "",
                        "takeaway_zh": "",
                        "prompt_zh": "",
                    }
                )

    # Deduplicate by URL, then by normalized title.
    seen_url: set[str] = set()
    seen_title: set[str] = set()
    deduped: list[dict[str, Any]] = []
    for it in sorted(items, key=lambda x: x.get("published_utc", ""), reverse=True):
        url = it.get("url", "")
        tnorm = _norm(it.get("title", ""))
        if url in seen_url or tnorm in seen_title:
            continue
        seen_url.add(url)
        seen_title.add(tnorm)
        deduped.append(it)

    return deduped[: max_items or 200]


def _save_yaml(path: Path, payload: Any) -> None:
    path.write_text(yaml.safe_dump(payload, sort_keys=False, allow_unicode=True), encoding="utf-8")


def _load_digest_yaml(path: Path) -> dict[str, Any]:
    if not path.exists():
        _die(f"missing digest file: {path}")
    raw = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
    if not isinstance(raw, dict):
        _die(f"invalid digest yaml: {path}")
    return raw


def _group_by_category(items: list[dict[str, Any]]) -> list[tuple[str, list[dict[str, Any]]]]:
    groups: dict[str, list[dict[str, Any]]] = {}
    for it in items:
        cat = (it.get("category", "") or "Misc").strip()
        groups.setdefault(cat, []).append(it)
    # Preserve a sensible, stable ordering: CSS first, then AI, then others.
    order = [
        "Business & economy",
        "Tech industry",
        "Security",
        "CSS / AI & society",
        "AI frontier",
        "Digital health / health comm",
        "Education / learning sciences / edtech",
        "Misc",
    ]
    ordered: list[tuple[str, list[dict[str, Any]]]] = []
    for k in order:
        if k in groups:
            ordered.append((k, groups.pop(k)))
    for k in sorted(groups.keys()):
        ordered.append((k, groups[k]))
    return ordered


CAT_ZH = {
    "Business & economy": "商业 / 经济",
    "Tech industry": "科技 / 产业",
    "Security": "安全 / 风险",
    "CSS / AI & society": "CSS / AI 与社会",
    "AI frontier": "AI 前沿",
    "Digital health / health comm": "数字健康 / 健康传播",
    "Education / learning sciences / edtech": "教育 / 学习科学 / 教育技术",
    "Misc": "其他",
}


def _parse_published_utc(it: dict[str, Any]) -> dt.datetime | None:
    s = (it.get("published_utc") or "").strip()
    if not s:
        return None
    try:
        return dt.datetime.fromisoformat(s.replace("Z", "+00:00"))
    except Exception:
        return None


def _importance_score(it: dict[str, Any]) -> int:
    """
    Rank items for the website digest list.

    This is intentionally heuristic: source quality + policy/market/AI-infra keywords,
    with a small recency bump.
    """
    src = (it.get("source") or "").lower()
    title = (it.get("title") or "").lower()
    cat = (it.get("category") or "").lower()

    score = 0

    # Source quality / typical impact.
    if "reuters" in src:
        score += 90
    elif "financial times" in src or src.startswith("ft"):
        score += 85
    elif "economist" in src:
        score += 75
    elif "bbc" in src:
        score += 70
    elif "mit technology review" in src:
        score += 68
    elif "guardian" in src:
        score += 62
    elif "ars technica" in src:
        score += 60
    elif "techcrunch" in src:
        score += 55
    elif "the verge" in src:
        score += 52
    elif "hacker news" in src:
        score += 15
    elif "arxiv" in src:
        score += 20
    elif "hugging face" in src:
        score += 22
    else:
        score += 30

    # Category nudge.
    if "security" in cat:
        score += 18
    elif "business" in cat or "economy" in cat or "markets" in cat:
        score += 14
    elif "tech" in cat:
        score += 12
    elif "ai frontier" in cat:
        score += 10
    elif "ai & society" in cat or "css" in cat:
        score += 8

    # Keyword signals.
    kw = {
        # Macro / markets
        "inflation": 10,
        "interest rate": 10,
        "house prices": 7,
        "stocks": 6,
        "bond": 6,
        "tariff": 10,
        "trade": 8,
        "sanction": 8,
        # Regulation / governance
        "antitrust": 10,
        "regulator": 8,
        "privacy": 7,
        "data": 6,
        "water": 6,
        "energy": 6,
        # AI / platforms / infra
        "openai": 10,
        "chatgpt": 9,
        "anthropic": 8,
        "nvidia": 8,
        "meta": 7,
        "tiktok": 6,
        "youtube": 6,
        "netflix": 6,
        "semiconductor": 8,
        "asml": 8,
        "chip": 7,
        # Geopolitics / conflict
        "ukraine": 10,
        "russia": 9,
        "greenland": 8,
        "nato": 7,
        "drone": 7,
        # Security-ish
        "breach": 10,
        "hack": 10,
        "ransom": 10,
        "vulnerability": 10,
        "zero-day": 10,
    }
    for k, w in kw.items():
        if k in title:
            score += w

    # Recency: small bump for newer items.
    when = _parse_published_utc(it)
    if when:
        age_h = (dt.datetime.now(dt.timezone.utc) - when).total_seconds() / 3600.0
        if age_h <= 6:
            score += 6
        elif age_h <= 12:
            score += 4
        elif age_h <= 24:
            score += 2

    return score


def _pick_lang(it: dict[str, Any], key_en: str, key_zh: str, lang: str) -> str:
    if lang == "zh":
        v = (it.get(key_zh) or "").strip()
        if v:
            return v
        # Fallback to English if Chinese is missing.
    return (it.get(key_en) or "").strip()


def _font_paths(lang: str) -> dict[str, Path]:
    """
    Pick fonts that actually contain the glyphs we need.

    For zh, we must use a CJK font, otherwise Pillow will render tofu boxes.
    """
    if lang == "zh":
        candidates = [
            ("/System/Library/Fonts/Hiragino Sans GB.ttc", "regular"),
            ("/System/Library/Fonts/STHeiti Medium.ttc", "bold"),
            ("/System/Library/Fonts/STHeiti Light.ttc", "regular"),
            ("/System/Library/Fonts/Supplemental/Songti.ttc", "regular"),
        ]
    else:
        # Prefer widely available macOS fonts.
        candidates = [
            ("/System/Library/Fonts/Supplemental/Arial.ttf", "regular"),
            ("/System/Library/Fonts/Supplemental/Arial Bold.ttf", "bold"),
            ("/System/Library/Fonts/Supplemental/Arial Unicode.ttf", "regular"),
            ("/System/Library/Fonts/Supplemental/Helvetica.ttc", "regular"),
        ]
    out: dict[str, Path] = {}
    for fp, role in candidates:
        p = Path(fp)
        if p.exists() and role not in out:
            out[role] = p
    return out


def _wrap(draw: ImageDraw.ImageDraw, text: str, font: ImageFont.FreeTypeFont, max_width: int) -> list[str]:
    """
    Simple word-wrapping that preserves spaces.

    Important: do NOT strip whitespace aggressively, otherwise words will run together
    in the rendered card (a common issue when splitting on whitespace).
    """
    text = re.sub(r"\s+", " ", (text or "").strip())
    if not text:
        return []

    words = text.split(" ")
    lines: list[str] = []
    cur = ""

    def flush() -> None:
        nonlocal cur
        if cur:
            lines.append(cur)
            cur = ""

    for w in words:
        test = w if not cur else f"{cur} {w}"
        if draw.textlength(test, font=font) <= max_width:
            cur = test
            continue

        # If a single token is too long, hard-wrap it by characters.
        if not cur and draw.textlength(w, font=font) > max_width:
            chunk = ""
            for ch in w:
                t2 = chunk + ch
                if draw.textlength(t2, font=font) <= max_width:
                    chunk = t2
                else:
                    if chunk:
                        lines.append(chunk)
                    chunk = ch
            if chunk:
                cur = chunk
            continue

        flush()
        cur = w

    flush()
    return lines


_URL_RE = re.compile(r"https?://\S+")


def _x_char_len(s: str) -> int:
    """
    Approximate X/Twitter length.

    X counts any URL as ~23 chars regardless of actual length. We approximate
    with 23 to split threads safely.
    """
    if not s:
        return 0
    urls = list(_URL_RE.findall(s))
    # Replace each URL with 23 chars in the count.
    return len(s) - sum(len(u) for u in urls) + 23 * len(urls)


def _digest_issue_urls(date: dt.date, site_url: str) -> dict[str, str]:
    base = (site_url or DEFAULT_SITE_URL).rstrip("/")
    return {
        "en": f"{base}/digest/{date.isoformat()}/",
        "zh": f"{base}/zh/digest/{date.isoformat()}/",
        "en_card": f"/assets/img/digests/{date.isoformat()}-en.png",
        "zh_card": f"/assets/img/digests/{date.isoformat()}-zh.png",
    }


def _format_item_bilingual(it: dict[str, Any]) -> str:
    t_en = (it.get("title") or "").strip()
    w_en = (it.get("why") or "").strip()
    take_en = (it.get("takeaway") or "").strip()
    p_en = (it.get("prompt") or "").strip()
    t_zh = (it.get("title_zh") or "").strip()
    w_zh = (it.get("why_zh") or "").strip()
    take_zh = (it.get("takeaway_zh") or "").strip()
    p_zh = (it.get("prompt_zh") or "").strip()
    url = (it.get("url") or "").strip()

    en = t_en
    if w_en:
        en = f"{en} — {w_en}"

    zh = t_zh or ""
    if zh and w_zh:
        zh = f"{zh} — {w_zh}"
    elif not zh and w_zh:
        zh = w_zh

    # Professional bulletin: one concise takeaway line (no "my take"/first-person labels).
    # LinkedIn will auto-link raw URLs, so we keep the URL on its own line.
    tl_en = _first_sentence(take_en, 200) if take_en else ""
    tl_zh = _first_sentence(take_zh, 140) if take_zh else ""

    lines: list[str] = []
    if en:
        lines.append(f"- {en} — {tl_en}" if tl_en else f"- {en}")
    if zh:
        lines.append(f"  {zh} — {tl_zh}" if tl_zh else f"  {zh}")
    if url:
        lines.append(f"  {url}")
    return "\n".join(lines).strip()


def _format_linkedin_post(
    date: dt.date,
    digest: dict[str, Any],
    site_url: str,
    *,
    text_mode: str = "bilingual",
    style: str = "bulletin",
) -> str:
    items = digest.get("items", [])
    groups = _group_by_category(items)
    urls = _digest_issue_urls(date, site_url)

    title_en = (digest.get("title") or f"Daily Digest — {date.isoformat()}").strip()
    title_zh = (digest.get("title_zh") or f"每日简报 — {date.isoformat()}").strip()
    note_en = (digest.get("note") or "").strip()
    note_zh = (digest.get("note_zh") or "").strip()

    def blurb(it: dict[str, Any], lang: str) -> str:
        # For social, prefer the short "why" line (less templated, avoids mid-sentence truncation).
        why = _pick_lang(it, "why", "why_zh", lang)
        if why:
            return why
        take = _pick_lang(it, "takeaway", "takeaway_zh", lang)
        return _first_sentence(take, 110 if lang != "zh" else 70)

    if style == "theme":
        # Theme-first format: a single short paragraph + a clean list of links.
        lines: list[str] = []
        if text_mode == "en":
            lines.extend(
                [
                    f"{title_en}",
                    "",
                    "Cards attached (EN + Chinese card).",
                    "",
                ]
            )
            if note_en:
                lines.extend([_truncate(note_en, 520), ""])
            lines.extend([f"Links (full list): {urls['en']}", "", "Links:", ""])
        else:
            lines.extend(
                [
                    f"{title_en} / {title_zh}",
                    "",
                    "EN + 中文 cards attached. Full lists:",
                    "附英文/中文两张卡片；完整列表：",
                    "",
                    f"EN list: {urls['en']}",
                    f"中文列表: {urls['zh']}",
                    "",
                ]
            )
            if note_en or note_zh:
                if note_en:
                    lines.append(_truncate(note_en, 520))
                if note_zh:
                    lines.append(_truncate(note_zh, 240))
                lines.append("")
            lines.extend(["Links / 链接：", ""])

        # Flatten items; keep it scannable.
        picks: list[tuple[str, str]] = []
        for _, cat_items in groups:
            for it in cat_items:
                t = (it.get("title") or "").strip()
                u = (it.get("url") or "").strip()
                if t and u:
                    picks.append((t, u))

        max_links = 10 if text_mode == "en" else 10
        for t, u in picks[:max_links]:
            lines.append(f"- {_truncate(t, 120)}")
            lines.append(f"  {u}")
        lines.append("")
        return "\n".join(lines).rstrip() + "\n"

    if text_mode == "en":
        lines: list[str] = [
            f"{title_en}",
            "",
            "Cards attached (EN + Chinese card).",
            "",
            f"Full list: {urls['en']}",
            "",
            "Highlights (clickable links):",
            "",
        ]
    else:
        lines = [
            f"{title_en} / {title_zh}",
            "",
            "EN + 中文 cards attached. Full lists:",
            "附英文/中文两张卡片；完整列表：",
        ]

        lines.extend(
            [
                "",
                f"EN list: {urls['en']}",
                f"中文列表: {urls['zh']}",
                "",
                "Bulletin (clickable links) / 快报（可点击链接）：",
                "",
            ]
        )

    if text_mode == "en":
        # LinkedIn EN-only: keep it scannable (no category blocks); rely on the attached card + full list.
        picks: list[dict[str, Any]] = []
        for _, cat_items in groups:
            for it in cat_items:
                t_en = (it.get("title") or "").strip()
                u = (it.get("url") or "").strip()
                if t_en and u:
                    picks.append(it)
        picks = picks[:3]

        for it in picks:
            t_en = (it.get("title") or "").strip()
            u = (it.get("url") or "").strip()
            b_en = _truncate(blurb(it, "en"), 140)
            head = f"- {_truncate(t_en, 120)}"
            if b_en:
                head += f" — {b_en}"
            lines.append(head)
            lines.append(f"  {u}")
            lines.append("")
    else:
        max_items_total = 10
        rendered = 0
        for cat, cat_items in groups:
            if not cat_items:
                continue
            lines.append(f"{cat} / {CAT_ZH.get(cat, cat)}")
            for it in cat_items:
                if rendered >= max_items_total:
                    break
                t_en = (it.get("title") or "").strip()
                t_zh = (it.get("title_zh") or "").strip()
                u = (it.get("url") or "").strip()
                if not t_en or not u:
                    continue
                b_en = blurb(it, "en")
                b_zh = blurb(it, "zh")
                head = f"- {t_en}"
                if t_zh and t_zh != t_en:
                    head += f" / {t_zh}"
                if b_en:
                    head += f" — {b_en}"
                if b_zh and b_zh != b_en:
                    head += f" / {b_zh}"
                lines.append(head)
                lines.append(f"  {u}")
                rendered += 1
            lines.append("")
            if rendered >= max_items_total:
                break

    if text_mode != "en":
        # Discussion prompts (max 2) at the end.
        prompts: list[tuple[str, str]] = []
        for it in items:
            p_en = (it.get("prompt") or "").strip()
            p_zh = (it.get("prompt_zh") or "").strip()
            if p_en or p_zh:
                prompts.append((p_en, p_zh))
        if prompts:
            lines.append("Open questions / 开放问题：")
            for p_en, p_zh in prompts[:2]:
                if p_en and p_zh:
                    lines.append(f"- {p_en} / {p_zh}")
                elif p_en:
                    lines.append(f"- {p_en}")
                elif p_zh:
                    lines.append(f"- {p_zh}")
            lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def _format_x_thread(
    date: dt.date,
    digest: dict[str, Any],
    site_url: str,
    *,
    text_mode: str = "bilingual",
    style: str = "bulletin",
) -> list[str]:
    items = digest.get("items", [])
    groups = _group_by_category(items)
    urls = _digest_issue_urls(date, site_url)
    note_en = (digest.get("note") or "").strip()
    note_zh = (digest.get("note_zh") or "").strip()

    if text_mode == "en":
        if style == "theme":
            # Theme tweet + link list in replies (so the first post stays readable).
            head: list[str] = [f"Daily Digest ({date.isoformat()}). Cards: EN + Chinese."]
            if note_en:
                head.append(_truncate(note_en, 240))
            head.append(f"Full list: {urls['en']}")
            head.append("Links below.")
            t0 = "\n".join(head).strip()

            lines: list[str] = []
            for _, cat_items in groups:
                for it in cat_items:
                    t = (it.get("title") or "").strip()
                    u = (it.get("url") or "").strip()
                    if t and u:
                        lines.append(f"{_truncate(t, 72)}: {u}")

            tweets: list[str] = [t0]
            cur: list[str] = []
            cur_len = 0
            max_len = 270
            for ln in lines[:12]:  # cap to avoid spammy threads
                # Use X-style length approximation (URLs ~23 chars) to pack more per tweet.
                add_len = _x_char_len(ln) + (1 if cur else 0)
                if cur and cur_len + add_len > max_len:
                    tweets.append("\n".join(cur).strip())
                    cur = [ln]
                    cur_len = _x_char_len(ln)
                else:
                    cur.append(ln)
                    cur_len += add_len
            if cur:
                tweets.append("\n".join(cur).strip())
            return tweets

        # Single post for X: keep it lightweight; attach BOTH cards here.
        picks: list[tuple[str, str]] = []
        for _, cat_items in groups:
            for it in cat_items:
                t = (it.get("title") or "").strip()
                u = (it.get("url") or "").strip()
                if t and u:
                    picks.append((t, u))
        picks = picks[:2]  # keep within X length limits

        lines = [
            f"Daily Digest ({date.isoformat()}). Cards: EN + Chinese.",
            f"Full list: {urls['en']}",
            "",
        ]
        for t, u in picks:
            # One-line items: readable, within length limits; details live on the digest page.
            lines.append(f"{_truncate(t, 72)}: {u}")
        return ["\n".join([ln for ln in lines if ln is not None]).strip()]

    if style == "theme":
        # Bilingual theme post + link list in replies.
        head: list[str] = [f"Daily Digest / 每日简报 ({date.isoformat()})"]
        head.append("Cards attached (EN + 中文).")
        if note_en:
            head.append(_truncate(note_en, 260))
        if note_zh:
            head.append(_truncate(note_zh, 200))
        head.append(f"EN list: {urls['en']}")
        head.append(f"中文列表: {urls['zh']}")
        head.append("Links below / 链接见串。")
        t0 = "\n".join(head).strip()

        lines: list[str] = []
        for _, cat_items in groups:
            for it in cat_items:
                t = (it.get("title") or "").strip()
                u = (it.get("url") or "").strip()
                if t and u:
                    lines.append(f"{_truncate(t, 72)}: {u}")

        tweets: list[str] = [t0]
        cur: list[str] = []
        cur_len = 0
        max_len = 270
        for ln in lines[:12]:
            add_len = _x_char_len(ln) + (1 if cur else 0)
            if cur and cur_len + add_len > max_len:
                tweets.append("\n".join(cur).strip())
                cur = [ln]
                cur_len = _x_char_len(ln)
            else:
                cur.append(ln)
                cur_len += add_len
        if cur:
            tweets.append("\n".join(cur).strip())
        return tweets

    # Tweet 1: short bilingual intro + canonical links (attach BOTH cards here).
    t0 = "\n".join(
        [
            f"Daily Digest / 每日简报 ({date.isoformat()})",
            "EN + 中文 cards attached.",
            f"EN: {urls['en']}",
            f"中文: {urls['zh']}",
            "",
            "Direct links in thread / 详细链接见串：",
        ]
    ).strip()

    tweets: list[str] = [t0]

    # Thread: compact per-item blocks (avoid orphan headings when splitting).
    blocks: list[str] = []
    for cat, cat_items in groups:
        if not cat_items:
            continue
        for it in cat_items[:10]:
            t_en = (it.get("title") or "").strip()
            w_en = (it.get("why") or "").strip()
            take_en = (it.get("takeaway") or "").strip()
            t_zh = (it.get("title_zh") or "").strip()
            w_zh = (it.get("why_zh") or "").strip()
            take_zh = (it.get("takeaway_zh") or "").strip()
            url = (it.get("url") or "").strip()

            cat_zh = CAT_ZH.get(cat, cat)
            cat_label = f"[{cat}]/{cat_zh}"

            tl_en = w_en or _first_sentence(take_en, 200)
            tl_zh = w_zh or _first_sentence(take_zh, 140)

            # Keep X readable: title + short notes + link.
            line = f"{cat_label} {t_en}".strip()
            if tl_en:
                line = f"{line}\nNote: {tl_en}"
            if t_zh:
                line = f"{line}\n{t_zh}"
            if tl_zh:
                line = f"{line}\n要点：{tl_zh}"
            if url:
                line = f"{line}\n{url}"
            blocks.append(line.strip())

    # Pack blocks into tweets with conservative length.
    cur: list[str] = []
    cur_len = 0
    max_len = 265  # keep headroom for platform differences
    for b in blocks:
        add = (b + "\n\n").strip() if b else ""
        if not add:
            continue
        add_len = _x_char_len(add) + (2 if cur else 0)
        if cur and cur_len + add_len > max_len:
            tweets.append("\n\n".join(cur).strip())
            cur = [b]
            cur_len = _x_char_len(b)
            continue
        if cur:
            cur_len += _x_char_len("\n\n" + b)
            cur.append(b)
        else:
            cur = [b]
            cur_len = _x_char_len(b)
    if cur:
        tweets.append("\n\n".join(cur).strip())

    return tweets


def render_card(date: dt.date, items: list[dict[str, Any]], out_path: Path, *, lang: str) -> None:
    # 4:5 aspect ratio is friendly for both LinkedIn and X.
    w, h = 1080, 1350
    # Professional / restrained: clean, bright background; no decorative textures.
    bg = (252, 252, 253)  # near-white
    ink = (15, 23, 42)    # slate-900
    sub = (51, 65, 85)    # slate-700
    faint = (226, 232, 240)

    img = Image.new("RGB", (w, h), bg)
    draw = ImageDraw.Draw(img)

    fonts = _font_paths(lang)
    font_title = ImageFont.truetype(str(fonts.get("bold", fonts.get("regular"))), 50) if fonts else ImageFont.load_default()
    font_h2 = ImageFont.truetype(str(fonts.get("bold", fonts.get("regular"))), 26) if fonts else ImageFont.load_default()
    font_small = ImageFont.truetype(str(fonts.get("regular", fonts.get("bold"))), 21) if fonts else ImageFont.load_default()
    font_item_title = ImageFont.truetype(str(fonts.get("bold", fonts.get("regular"))), 26) if fonts else ImageFont.load_default()
    font_item_body = ImageFont.truetype(str(fonts.get("regular", fonts.get("bold"))), 22) if fonts else ImageFont.load_default()

    pad = 64
    x = pad
    y = 56

    def round_rect_outline(x0: int, y0: int, x1: int, y1: int, r: int) -> None:
        # Monochrome pill with a thin outline (more "academic", less "poster-like").
        draw.rounded_rectangle((x0, y0, x1, y1), radius=r, outline=faint, width=2, fill=bg)

    title = "Daily Digest" if lang != "zh" else "每日简报"
    draw.text((x, y), title, fill=ink, font=font_title)
    y += 54

    date_str = date.isoformat()
    subtitle = "A quick bulletin of what I'm reading today." if lang != "zh" else "今天在读的内容速览。"
    draw.text((x, y), date_str + "  ·  " + subtitle, fill=sub, font=font_small)
    y += 32

    # Top rule
    draw.line((x, y, w - pad, y), fill=faint, width=2)
    y += 20

    groups = _group_by_category(items)
    max_items_total = 10
    rendered = 0

    for cat, cat_items in groups:
        if rendered >= max_items_total:
            break
        if not cat_items:
            continue
        cat_label = cat if lang != "zh" else CAT_ZH.get(cat, cat)
        # Category chip (outlined, monochrome)
        chip_h = 30
        chip_w = int(draw.textlength(cat_label, font=font_small)) + 28
        round_rect_outline(x, y, x + chip_w, y + chip_h, 14)
        draw.text((x + 14, y + 5), cat_label, fill=sub, font=font_small)
        y += chip_h + 12

        for it in cat_items:
            if rendered >= max_items_total:
                break
            t = _pick_lang(it, "title", "title_zh", lang)
            why = _pick_lang(it, "why", "why_zh", lang)
            take = _pick_lang(it, "takeaway", "takeaway_zh", lang)
            # Card should stay very scannable: 1 title line + 1 TL;DR line.
            tl = why or _first_sentence(take, 140)

            if not t:
                continue

            # Title (allow 2 lines so long titles stay readable).
            t_lines = _wrap(draw, t, font_item_title, max_width=w - pad * 2)[:2]
            for ln in t_lines:
                draw.text((x, y), ln, fill=ink, font=font_item_title)
                y += 32
            y += 1

            if tl:
                # Keep the blurb compact, but allow up to 2 lines.
                tl_lines = _wrap(draw, tl, font_item_body, max_width=w - pad * 2)[:2]
                for ln in tl_lines:
                    draw.text((x, y), ln, fill=sub, font=font_item_body)
                    y += 26

            y += 10
            rendered += 1

        y += 2
        # Reserve space for the footer; don't let the last lines collide.
        if y > h - 210:
            break

    # Bottom rule + footer
    y = h - 160
    draw.line((x, y, w - pad, y), fill=faint, width=2)
    y += 22
    footer = "Zhilong George Zhao | @longlalaland | long19950304.github.io"
    draw.text((x, y), footer, fill=sub, font=font_small)

    out_path.parent.mkdir(parents=True, exist_ok=True)
    img.save(out_path, format="PNG", optimize=True, compress_level=9)
    _optimize_png(out_path)


def _md_for_lang(date: dt.date, digest: dict[str, Any], *, lang: str) -> Path:
    items = digest.get("items", [])
    if not isinstance(items, list):
        _die("digest.items must be a list")
    if not items:
        _die("digest has no items; skip build or add items first")

    # Markdown page (collection item)
    out_dir = ZH_DIGESTS_DIR if lang == "zh" else DIGESTS_DIR
    out_dir.mkdir(parents=True, exist_ok=True)
    md_path = out_dir / f"{date.isoformat()}.md"

    title = digest.get("title") or f"Daily Digest - {date.isoformat()}"
    if lang == "zh":
        title = digest.get("title_zh") or f"每日简报 - {date.isoformat()}"

    # Sort by importance (global list, not grouped) and cap to a readable max.
    items_sorted = sorted(
        items,
        key=lambda it: (
            _importance_score(it),
            _parse_published_utc(it) or dt.datetime.min.replace(tzinfo=dt.timezone.utc),
        ),
        reverse=True,
    )
    max_items = int(digest.get("max_items", 0) or DEFAULT_MAX_ITEMS_ON_SITE)
    items_sorted = items_sorted[:max_items]
    top = items_sorted[:10]
    rest = items_sorted[10:]

    note = (digest.get("note_zh") if lang == "zh" else digest.get("note")) or ""
    note = note.strip()

    lines: list[str] = [
        "---",
        "layout: page",
        f'title: "{title}"',
        f"lang: {lang}",
        f"permalink: /{'zh/' if lang == 'zh' else ''}digest/{date.isoformat()}/",
        f"digest_date: {date.isoformat()}",
        f"translation_key: digest-{date.isoformat()}",
        "---",
        "",
    ]
    if note:
        lines.extend([note, ""])

    def _cat_label(v: str) -> str:
        v = (v or "Misc").strip() or "Misc"
        return v if lang != "zh" else CAT_ZH.get(v, v)

    def _short(it: dict[str, Any]) -> str:
        why = _pick_lang(it, "why", "why_zh", lang)
        if why:
            return why
        take = _pick_lang(it, "takeaway", "takeaway_zh", lang)
        return _first_sentence(take, 160 if lang != "zh" else 120)

    def _pub(it: dict[str, Any]) -> str:
        when = _parse_published_utc(it)
        if not when:
            return ""
        return when.strftime("%Y-%m-%d %H:%M UTC")

    lines.extend(["## Top 10" if lang != "zh" else "## 最重要的 10 条", ""])

    for idx, it in enumerate(top, start=1):
        t = _pick_lang(it, "title", "title_zh", lang)
        u = (it.get("url") or "").strip()
        if not t or not u:
            continue
        meta = " | ".join(
            x
            for x in [
                _cat_label(it.get("category", "")),
                (it.get("source") or "").strip(),
                _pub(it),
            ]
            if x
        )
        lines.append(f"{idx}. [{t}]({u})" + (f" — {meta}" if meta else ""))
        blurb = _short(it)
        if blurb:
            lines.append(f"   - {blurb}")
        lines.append("")

    lines.extend(["## Full list" if lang != "zh" else "## 全部列表", ""])
    lines.append("| # | Title | Category | Source | Published (UTC) |")
    lines.append("|---:|---|---|---|---|")
    for idx, it in enumerate(items_sorted, start=1):
        t = _pick_lang(it, "title", "title_zh", lang)
        u = (it.get("url") or "").strip()
        if not t or not u:
            continue
        cat = _cat_label(it.get("category", ""))
        src = (it.get("source") or "").strip()
        pub = _pub(it)
        lines.append(f"| {idx} | [{t}]({u}) | {cat} | {src} | {pub} |")

    md_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
    return md_path


def build_digest(date: dt.date, digest: dict[str, Any]) -> list[tuple[str, Path]]:
    out: list[tuple[str, Path]] = []
    out.append(("en", _md_for_lang(date, digest, lang="en")))
    out.append(("zh", _md_for_lang(date, digest, lang="zh")))
    return out


def cmd_draft(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    sources = _load_sources()
    # Since: last N hours from now (UTC). This keeps drafts useful for daily runs.
    since_utc = dt.datetime.now(dt.timezone.utc) - dt.timedelta(hours=args.hours)
    items = _draft_items(sources, since_utc=since_utc, max_items=args.max_items)

    out = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if out.exists() and not args.force:
        out = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"

    payload = {
        "date": date.isoformat(),
        "title": f"Daily Digest - {date.isoformat()}",
        "title_zh": f"每日简报 - {date.isoformat()}",
        "note": "",
        "note_zh": "",
        "items": items,
    }
    _save_yaml(out, payload)
    print(f"Wrote {out.relative_to(REPO_ROOT)} ({len(items)} items).")
    print("Next: edit `why` (short), `takeaway` (1–2 sentences), `prompt` (1 question) + fill *_zh, then run:")
    print(f"  {REPO_ROOT}/bin/digest build --date {date.isoformat()}")


def cmd_build(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    digest_path = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if not digest_path.exists():
        # If only draft exists, use it.
        draft = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"
        if draft.exists():
            digest_path = draft
        else:
            _die(f"missing digest YAML for {date.isoformat()} (expected {digest_path})")
    digest = _load_digest_yaml(digest_path)
    results = build_digest(date, digest)
    for lang, md_path in results:
        print(f"Wrote {md_path.relative_to(REPO_ROOT)} ({lang})")


def cmd_social(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    digest_path = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if not digest_path.exists():
        draft = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"
        if draft.exists():
            digest_path = draft
        else:
            _die(f"missing digest YAML for {date.isoformat()} (expected {digest_path})")

    digest = _load_digest_yaml(digest_path)
    # Social requirement: each post should include BOTH EN + ZH card images.
    card_en = CARD_DIR / f"{date.isoformat()}-en.png"
    card_zh = CARD_DIR / f"{date.isoformat()}-zh.png"
    print("==== Attach images (EN + ZH) ====")
    print(f"EN card: {card_en}")
    print(f"ZH card: {card_zh}")
    if not card_en.exists() or not card_zh.exists():
        print("(Tip) Run `./bin/digest build --date YYYY-MM-DD` first to generate the cards.")
    print("")

    if args.platform in ("linkedin", "both"):
        print("==== LinkedIn (copy/paste) ====")
        print(
            _format_linkedin_post(
                date,
                digest,
                site_url=args.site_url,
                text_mode=args.text,
                style=args.style,
            )
        )

    if args.platform in ("x", "both"):
        print("==== X thread (copy/paste) ====")
        tweets = _format_x_thread(
            date,
            digest,
            site_url=args.site_url,
            text_mode=args.text,
            style=args.style,
        )
        total = len(tweets)
        for i, t in enumerate(tweets, start=1):
            print(f"\n--- Tweet {i}/{total} ---\n{t}\n")


def main(argv: list[str]) -> int:
    parser = argparse.ArgumentParser(prog="digest")
    sub = parser.add_subparsers(dest="cmd", required=True)

    p_draft = sub.add_parser("draft", help="Draft a digest YAML from RSS/Atom sources")
    p_draft.add_argument("--date", required=True, help="YYYY-MM-DD (label for this issue)")
    p_draft.add_argument("--hours", type=int, default=36, help="Look back window (UTC), default: 36")
    p_draft.add_argument("--max-items", type=int, default=40, help="Max candidate items to write, default: 40")
    p_draft.add_argument("--force", action="store_true", help="Overwrite existing YYYY-MM-DD.yml")
    p_draft.set_defaults(func=cmd_draft)

    p_build = sub.add_parser("build", help="Build digest page from YAML")
    p_build.add_argument("--date", required=True, help="YYYY-MM-DD")
    p_build.set_defaults(func=cmd_build)

    p_social = sub.add_parser("social", help="Generate social post text (LinkedIn + X)")
    p_social.add_argument("--date", required=True, help="YYYY-MM-DD")
    p_social.add_argument(
        "--platform",
        choices=["both", "linkedin", "x"],
        default="both",
        help="Which post text to emit, default: both",
    )
    p_social.add_argument(
        "--text",
        choices=["en", "bilingual"],
        default="bilingual",
        help="Text mode for posts; note: cards are always EN+ZH. Default: bilingual",
    )
    p_social.add_argument(
        "--style",
        choices=["bulletin", "theme"],
        default="bulletin",
        help="Post layout: bulletin (default) or theme-first (theme + links).",
    )
    p_social.add_argument(
        "--site-url",
        default=DEFAULT_SITE_URL,
        help=f"Canonical site URL for digest links, default: {DEFAULT_SITE_URL}",
    )
    p_social.set_defaults(func=cmd_social)

    args = parser.parse_args(argv)
    args.func(args)
    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
