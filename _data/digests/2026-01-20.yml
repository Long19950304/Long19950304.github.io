date: '2026-01-20'
title: Daily Digest — 2026-01-20
title_zh: 每日简报 — 2026-01-20
note: This issue is a reading log and short takeaways; 1–2 items may be expanded later.
note_zh: 本期为阅读记录与要点摘录；后续会选 1–2 条进一步展开。
items:
- category: AI frontier
  title: Differential Transformer V2
  title_zh: 差分变压器V2
  why: An attention variant for long-context efficiency; a clear comparison point
    vs standard Transformers.
  why_zh: 面向长上下文效率改进的注意力变体，可与标准 Transformer 做对照。
  takeaway: Diff Attn v2 introduces a differential attention formulation for more
    efficient long-context modeling; a useful comparison point against standard Transformers
    and other attention variants.
  takeaway_zh: Diff Attn v2 以“差分注意力”形式改进长上下文效率；适合作为与标准 Transformer 及其他注意力变体对照的参考点。
  prompt: ''
  prompt_zh: ''
  url: https://huggingface.co/blog/microsoft/diff-attn-v2
  source: Hugging Face Blog
- category: AI frontier
  title: OpenAI partners with Cerebras
  title_zh: OpenAI 与 Cerebras 合作
  why: A notable infra/inference partnership; watch for speed/cost implications.
  why_zh: 推理/硬件合作动向，可能影响延迟、成本与高速推理的可获得性。
  takeaway: Signals continued investment in specialized inference hardware partnerships,
    with implications for inference latency, cost, and who can access high-speed inference
    at scale.
  takeaway_zh: 显示推理侧继续推进与专用硬件的合作，可能影响推理延迟、成本，以及规模化“高速推理”的可获得性。
  prompt: ''
  prompt_zh: ''
  url: https://openai.com/index/cerebras-partnership
  source: OpenAI Blog
- category: AI frontier
  title: 'Open Responses: What you need to know'
  title_zh: Open Responses：你需要了解什么
  why: A practical multi-model workflow for comparing and integrating LLM outputs.
  why_zh: 多模型对照与整合的工作流范式：用多样性拓展思路，用验证收敛结论。
  takeaway: Outlines an 'open responses' workflow for comparing multiple models and
    synthesizing outputs, emphasizing diversity for exploration and verification for
    consolidation.
  takeaway_zh: 给出“多模型对照→整合→验证”的工作流框架：用多样性获取思路，用验证将结论收敛得更稳。
  prompt: ''
  prompt_zh: ''
  url: https://huggingface.co/blog/open-responses
  source: Hugging Face Blog
- category: CSS / AI & society
  title: 'AI Sycophancy: How Users Flag and Respond'
  title_zh: AI 迎合：用户如何标记与回应
  why: Empirical signals for detecting/handling overly agreeable LLM behavior.
  why_zh: 整理用户识别/反馈“迎合型”行为的可观测信号，可用于信任与安全评估。
  takeaway: Examines when users perceive sycophantic behavior and how they respond,
    and consolidates observable cues with direct relevance to trust and safety evaluation.
  takeaway_zh: 研究用户何时认为助手在迎合以及如何反馈，并整理可观测信号；对信任与安全评估具有直接参考价值。
  prompt: How should assistants balance politeness vs disagreement without drifting
    into sycophancy?
  prompt_zh: 助手应如何在“礼貌”与“必要的不同意”之间平衡，而不滑向迎合？
  url: https://arxiv.org/abs/2601.10467v1
  source: arXiv
- category: Education / learning sciences / edtech
  title: 'The Conversational Exam: A Scalable Assessment Design for the AI Era'
  title_zh: 对话式考试：AI 时代可扩展的评估设计
  why: An assessment format designed to stay meaningful when students have AI.
  why_zh: 一种面向 AI 时代的评估形式，尽量保持测评的有效性。
  takeaway: Proposes conversational exams that assess reasoning and explanation through
    interaction (not only final answers), designed to remain meaningful when AI tools
    are available.
  takeaway_zh: 提出“对话式考试”，通过交互过程评估推理与解释能力，而不只看最终答案，以提升 AI 可用环境下的测评有效性。
  prompt: What guardrails would make conversational assessment fair, scalable, and
    less subjective?
  prompt_zh: 哪些规则/机制能让对话式测评更公平、可扩展、并降低主观性？
  url: https://arxiv.org/abs/2601.10691v1
  source: arXiv
- category: Education / learning sciences / edtech
  title: 'Evaluating 21st-Century Competencies in Postsecondary Curricula with Large
    Language Models: Performance Benchmarking and Reasoning-Based Prompting Strategies'
  title_zh: 用大语言模型评估高等教育课程的 21 世纪能力：性能基准与推理式提示策略
  why: Benchmarks LLMs for curricular competency evaluation and prompting strategies.
  why_zh: 用 LLM 读课程材料评估能力覆盖，并提供基准与提示策略对照。
  takeaway: Benchmarks LLMs for evaluating 21st-century competencies in curricula
    and compares prompting strategies, informing more reliable automated curriculum
    auditing.
  takeaway_zh: 对“课程能力覆盖评估”任务进行基准测试并比较提示策略，为自动化课程审计提供方法线索与性能参考。
  prompt: ''
  prompt_zh: ''
  url: https://arxiv.org/abs/2601.10983v1
  source: arXiv
- category: Education / learning sciences / edtech
  title: 'aiPlato: A Novel AI Tutoring and Step-wise Feedback System for Physics Homework'
  title_zh: aiPlato：用于物理作业的 AI 辅导与分步反馈系统
  why: A tutoring design emphasizing step-wise feedback (closer to how students learn).
  why_zh: 物理作业辅导系统：强调分步反馈/支架，而非直接给完整答案。
  takeaway: An AI tutoring design centered on step-wise feedback (rather than full
    solutions), aligning with scaffolding-oriented learning support.
  takeaway_zh: 以分步提示与反馈支持学生完成推理链，更接近支架式学习；对教学型 AI 的反馈设计有启发。
  prompt: ''
  prompt_zh: ''
  url: https://arxiv.org/abs/2601.09965v1
  source: arXiv
- category: Education / learning sciences / edtech
  title: 'Take Out Your Calculators: Estimating the Real Difficulty of Question Items
    with LLM Student Simulations'
  title_zh: 拿出你的计算器：用“LLM 学生模拟”估计题目真实难度
  why: Uses simulated "LLM students" to estimate item difficulty for assessment design.
  why_zh: 用“LLM 学生模拟”估计题目难度：适用于数据不足场景，但需要校准。
  takeaway: Uses simulated 'LLM students' to estimate item difficulty, as a method
    option when real response data is limited (with calibration needs).
  takeaway_zh: 提出用“模拟学生”估计题目难度的方法路径；适用于数据不足场景，但需要与真实数据/人工标定进行校准。
  prompt: ''
  prompt_zh: ''
  url: https://arxiv.org/abs/2601.09953v1
  source: arXiv
