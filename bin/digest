#!/usr/bin/env python3
"""
Daily digest generator for the personal site.

Workflow:
  1) Draft candidates from RSS/Atom feeds:
       personal-site/bin/digest draft --date 2026-01-20
  2) Edit the generated YAML (fill `why` fields, delete low-signal items).
  3) Build the digest page:
       personal-site/bin/digest build --date 2026-01-20
  4) (Optional) Generate social post text (X thread + LinkedIn copy):
       personal-site/bin/digest social --date 2026-01-20
"""

from __future__ import annotations

import argparse
import datetime as dt
import base64
import hashlib
import json
import os
import random
import re
import sys
import html
import socket
import shutil
import subprocess
import urllib.request
import gzip
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable
from types import SimpleNamespace

import feedparser
import yaml
from PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter


REPO_ROOT = Path(__file__).resolve().parents[1]
DATA_SOURCES = REPO_ROOT / "_data" / "digest_sources.yml"
DATA_DIGESTS_DIR = REPO_ROOT / "_data" / "digests"
DATA_WEEKLY_PATH = REPO_ROOT / "_data" / "digest_weekly.yml"
DIGESTS_DIR = REPO_ROOT / "_digests"
ZH_DIGESTS_DIR = REPO_ROOT / "_digests_zh"
CARD_DIR = REPO_ROOT / "assets" / "img" / "digests"
DEFAULT_SITE_URL = "https://www.zhaozhilong.com"
FETCH_TIMEOUT_SECONDS = 15
FETCH_WORKERS = 8
DEFAULT_MAX_ITEMS_ON_SITE = 100
# AI tool/model updates are a separate block; keep a separate cap so they don't get squeezed out.
DEFAULT_MAX_AI_ITEMS_ON_SITE = 80
# Default to a model that is broadly available from more regions.
DEFAULT_TRANSLATE_MODEL = "deepseek/deepseek-chat"
DEFAULT_TRANSLATE_FALLBACK_MODELS = [
    "qwen/qwen-2.5-7b-instruct",
    "mistralai/mistral-7b-instruct",
]

# feedparser uses urllib under the hood and may otherwise block indefinitely on slow/blocked feeds.
socket.setdefaulttimeout(FETCH_TIMEOUT_SECONDS)


def _die(msg: str, code: int = 2) -> None:
    print(f"error: {msg}", file=sys.stderr)
    raise SystemExit(code)

def _site_asset_version() -> str:
    """
    Best-effort cache buster for GitHub Pages assets.

    GitHub Pages serves long-lived assets with caching; when we regenerate a card for the same
    filename, browsers may keep showing the old image for a while. We append `?v=<sha>` so each
    deployment can bust caches deterministically.
    """
    sha = (os.environ.get("GITHUB_SHA") or "").strip()
    if sha:
        return sha[:7]
    try:
        out = subprocess.check_output(
            ["git", "rev-parse", "--short", "HEAD"],
            cwd=str(REPO_ROOT),
            stderr=subprocess.DEVNULL,
            text=True,
        ).strip()
        return out
    except Exception:
        return ""


def _ensure_dirs() -> None:
    DATA_DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    ZH_DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    CARD_DIR.mkdir(parents=True, exist_ok=True)

def _optimize_png(path: Path) -> None:
    """
    Make generated cards more web-friendly.

    - Keep it optional: if pngquant isn't installed, do nothing.
    - If optimization isn't smaller, keep the original (avoid quality regressions).
    - Write back in-place to preserve file owner/group on macOS volumes.
    """
    exe = shutil.which("pngquant")
    if not exe or not path.exists():
        return

    tmp = path.with_suffix(path.suffix + ".tmp")
    try:
        orig_size = path.stat().st_size
        subprocess.run(
            [exe, "--force", "--strip", "--speed", "1", "--quality", "80-95", "-o", str(tmp), str(path)],
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        if tmp.exists() and tmp.stat().st_size < orig_size:
            path.write_bytes(tmp.read_bytes())
    except Exception as e:
        print(f"warn: pngquant failed for {path.name}: {e}", file=sys.stderr)
    finally:
        try:
            tmp.unlink()
        except FileNotFoundError:
            pass


def _write_webp_from_png(png_path: Path) -> None:
    """
    Create a .webp sibling for a generated PNG (for faster web delivery).

    Keep PNG as a fallback; templates should prefer WebP via <picture>.
    """
    if not png_path.exists():
        return
    webp = png_path.with_suffix(".webp")
    try:
        im = Image.open(png_path).convert("RGB")
        webp.parent.mkdir(parents=True, exist_ok=True)
        # Cards are mostly text; a conservative quality keeps edges crisp.
        im.save(webp, format="WEBP", quality=82, method=6)
    except Exception:
        return


def _parse_date(s: str) -> dt.date:
    try:
        return dt.date.fromisoformat(s)
    except ValueError:
        _die(f"invalid --date '{s}', expected YYYY-MM-DD")


def _load_sources() -> list[dict[str, Any]]:
    if not DATA_SOURCES.exists():
        _die(f"missing {DATA_SOURCES}")
    raw = yaml.safe_load(DATA_SOURCES.read_text(encoding="utf-8")) or {}
    sources = raw.get("sources", [])
    if not isinstance(sources, list) or not sources:
        _die(f"no sources defined in {DATA_SOURCES}")
    return sources


def _norm(s: str) -> str:
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s


def _pick_entry_time(entry: Any) -> dt.datetime | None:
    # feedparser yields either .published_parsed or .updated_parsed
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    return dt.datetime(*t[:6], tzinfo=dt.timezone.utc)


def _openrouter_key() -> str | None:
    """
    Resolve OpenRouter API key without ever printing it.

    Resolution order:
      1) Env: OPENROUTER_API_KEY
      2) Optional env override: XIANXIAN_ENV_PATH points to a .env file
      3) Auto-discovery: /Volumes/long1/desk1/先贤对谈/ep*/.env and archive .env files
         (prefers the most recently modified .env)
    """

    key = (os.environ.get("OPENROUTER_API_KEY") or "").strip()
    if key and not key.lower().startswith("your_") and not key.startswith("<"):
        return key

    def _parse_dotenv(path: Path) -> dict[str, str]:
        env: dict[str, str] = {}
        try:
            txt = path.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            return env
        for raw in txt.splitlines():
            line = raw.strip()
            if not line or line.startswith("#") or "=" not in line:
                continue
            k, v = line.split("=", 1)
            k = k.strip()
            v = v.strip().strip('"').strip("'")
            if k:
                env[k] = v
        return env

    candidates: list[Path] = []
    override = (os.environ.get("XIANXIAN_ENV_PATH") or "").strip()
    if override:
        candidates.append(Path(override))

    base = Path("/Volumes/long1/desk1/先贤对谈")
    if base.exists():
        found: list[Path] = []
        found.extend(base.glob("ep*/.env"))
        archive = base / "_archive_历史文档"
        if archive.exists():
            found.extend(archive.glob("**/.env"))

        uniq: list[Path] = []
        seen: set[str] = set()
        for p in found:
            ps = str(p)
            if ps in seen:
                continue
            seen.add(ps)
            uniq.append(p)
        uniq.sort(key=lambda p: p.stat().st_mtime if p.exists() else 0.0, reverse=True)
        candidates.extend(uniq)

    for p in candidates:
        if not p.exists():
            continue
        env = _parse_dotenv(p)
        key2 = (env.get("OPENROUTER_API_KEY") or "").strip()
        if key2 and not key2.lower().startswith("your_") and not key2.startswith("<"):
            os.environ.setdefault("OPENROUTER_API_KEY", key2)
            return key2

    return None


_CJK_RE = re.compile(r"[\u4e00-\u9fff]")


def _has_cjk(s: str) -> bool:
    return bool(_CJK_RE.search(s or ""))


def _translate_en_to_zh(texts: list[str]) -> list[str]:
    """
    Translate a batch of short English snippets into natural Simplified Chinese.

    - Uses OpenRouter if `OPENROUTER_API_KEY` is available.
    - Returns a list of the same length/order.
    - If no API key, returns the original texts (so builds still work), but zh pages will fall back to English.
    """
    texts = [t.strip() for t in texts]
    if not texts:
        return []

    key = _openrouter_key()
    if not key:
        return texts

    model = (os.environ.get("DIGEST_TRANSLATE_MODEL") or DEFAULT_TRANSLATE_MODEL).strip()
    fallback_models_env = (os.environ.get("DIGEST_TRANSLATE_MODEL_FALLBACKS") or "").strip()
    fallback_models = (
        [m.strip() for m in fallback_models_env.split(",") if m.strip()]
        if fallback_models_env
        else list(DEFAULT_TRANSLATE_FALLBACK_MODELS)
    )
    models_to_try = [model] + [m for m in fallback_models if m != model]

    url = "https://openrouter.ai/api/v1/chat/completions"

    sys_msg = (
        "You are a professional bilingual editor for a news digest. "
        "Translate English to natural, concise Simplified Chinese suitable for an academic/professional briefing. "
        "Avoid literal calques; prioritize fluent, idiomatic Chinese. "
        "Keep proper nouns/brand names as-is if commonly used; keep technical terms accurate. "
        "No hype, no emojis, no explanations."
    )

    # Single-item requests are much more reliable if we don't force JSON.
    messages: list[dict[str, str]]
    if len(texts) == 1:
        messages = [
            {"role": "system", "content": sys_msg},
            {
                "role": "user",
                "content": (
                    "Translate this to Simplified Chinese. Return ONLY the translated text.\n\n"
                    f"{texts[0]}"
                ),
            },
        ]
    else:
        messages = [
            {"role": "system", "content": sys_msg},
            {
                "role": "user",
                "content": (
                    "Translate the following snippets to Simplified Chinese. "
                    "Return ONLY a JSON array of strings with the same order/length.\n\n"
                    + "\n".join(f"{i+1}. {t}" for i, t in enumerate(texts))
                ),
            },
        ]

    last_err: str | None = None
    for model_to_try in models_to_try:
        payload = {
            "model": model_to_try,
            "temperature": 0,
            "messages": messages,
        }

        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode("utf-8"),
            headers={
                "Authorization": f"Bearer {key}",
                "Content-Type": "application/json",
                # Recommended by OpenRouter, optional but helps attribution.
                "HTTP-Referer": DEFAULT_SITE_URL,
                "X-Title": "personal-site digest translator",
            },
            method="POST",
        )

        try:
            with urllib.request.urlopen(req, timeout=30) as resp:
                raw = resp.read().decode("utf-8", errors="replace")
        except Exception as e:
            # Try the next model (some are region-locked).
            last_err = str(e)
            print(f"warn: translate failed for {model_to_try} ({e}); trying fallback", file=sys.stderr)
            continue

        try:
            data = json.loads(raw)
            content = data["choices"][0]["message"]["content"]
        except Exception as e:
            last_err = str(e)
            print(
                f"warn: translate parse failed for {model_to_try} ({e}); trying fallback",
                file=sys.stderr,
            )
            continue

        # Success.
        break
    else:
        # Exhausted all models.
        print(f"warn: translate failed ({last_err}); leaving zh as EN for this run", file=sys.stderr)
        return texts

    content = content.strip()
    content = re.sub(r"^```(?:json)?\s*", "", content)
    content = re.sub(r"\s*```$", "", content)

    if len(texts) == 1:
        return [content.strip().strip("\"'")]

    # Extract the first JSON array. If the model doesn't comply, recursively split the batch.
    try:
        start = content.find("[")
        end = content.rfind("]")
        if start == -1 or end == -1 or end <= start:
            raise ValueError("no JSON array found")
        arr = json.loads(content[start : end + 1])
        if not isinstance(arr, list) or len(arr) != len(texts):
            raise ValueError("invalid JSON array length/type")
        return [str(s).strip() for s in arr]
    except Exception:
        # Retry with smaller batches to improve reliability.
        if len(texts) <= 5:
            return [_translate_en_to_zh([t])[0] for t in texts]
        mid = len(texts) // 2
        left = _translate_en_to_zh(texts[:mid])
        right = _translate_en_to_zh(texts[mid:])
        return left + right


def _autofill_digest_zh_fields(digest: dict[str, Any], *, now: dt.datetime) -> bool:
    """
    Fill missing *_zh fields in digest items in-place.
    Returns True if anything changed.

    Only fills what is needed for the on-site list (capped to max_items), so we don't translate
    large drafts unnecessarily.
    """
    items = digest.get("items", [])
    if not isinstance(items, list) or not items:
        return False

    # Match the site output: rank first, then cap per section (news vs AI tools).
    ranked_all = sorted(
        items,
        key=lambda it: (
            _importance_score(it, now=now),
            _parse_published_utc(it) or dt.datetime.min.replace(tzinfo=dt.timezone.utc),
        ),
        reverse=True,
    )
    ai_cat = "AI tools & model releases"
    max_news = int(digest.get("max_items", 0) or DEFAULT_MAX_ITEMS_ON_SITE)
    max_ai = int(digest.get("max_items_ai", 0) or DEFAULT_MAX_AI_ITEMS_ON_SITE)

    news = [it for it in ranked_all if (it.get("category") or "").strip() != ai_cat][:max_news]
    ai = [it for it in ranked_all if (it.get("category") or "").strip() == ai_cat][:max_ai]

    # Only translate what we actually show:
    # - Titles for the full lists (news + AI) so zh pages are fully Chinese.
    # - Short blurbs for Top items (<= 20 total) so zh pages read naturally.
    top_news = _select_top_diverse(
        news,
        n=10,
        max_per_source=int(os.environ.get("DIGEST_TOP_MAX_PER_SOURCE", "3") or "3"),
        max_per_domain=int(os.environ.get("DIGEST_TOP_MAX_PER_DOMAIN", "1") or "1"),
        max_per_category=int(os.environ.get("DIGEST_TOP_MAX_PER_CATEGORY", "5") or "5"),
    )
    ai_top_pool = _ai_item_practicality_filter(ai)
    ai_unique_sources = {(it.get("source") or "").strip() for it in ai_top_pool if (it.get("source") or "").strip()}
    top_ai = _select_top_diverse(
        ai_top_pool,
        n=min(10, len(ai_unique_sources)),
        max_per_source=int(os.environ.get("DIGEST_AI_TOP_MAX_PER_SOURCE", "1") or "1"),
        max_per_domain=int(os.environ.get("DIGEST_AI_TOP_MAX_PER_DOMAIN", "20") or "20"),
        max_per_category=int(os.environ.get("DIGEST_AI_TOP_MAX_PER_CATEGORY", "10") or "10"),
    )

    items_sorted: list[dict[str, Any]] = []
    seen_ids: set[int] = set()
    for it in news + ai:
        iid = id(it)
        if iid in seen_ids:
            continue
        seen_ids.add(iid)
        items_sorted.append(it)

    # Items for which we need a zh blurb (top picks only).
    blurb_sorted: list[dict[str, Any]] = []
    seen2: set[int] = set()
    for it in top_news + top_ai:
        iid = id(it)
        if iid in seen2:
            continue
        seen2.add(iid)
        blurb_sorted.append(it)

    # If a previous run wrote back an English "translation", clear it so we can retry properly.
    for it in items_sorted:
        for k_zh, k_en in [
            ("title_zh", "title"),
            ("why_zh", "why"),
            ("takeaway_zh", "takeaway"),
        ]:
            v = (it.get(k_zh) or "").strip()
            src = (it.get(k_en) or "").strip()
            if v and (not _has_cjk(v)) and src and (not _has_cjk(src)):
                it[k_zh] = ""

    # Collect translation jobs (dedupe by source text to reduce API calls).
    jobs_by_text: dict[str, list[tuple[dict[str, Any], str]]] = {}
    # Titles for all visible items (news + AI caps)
    for it in items_sorted:
        if (it.get("title_zh") or "").strip():
            continue
        src = (it.get("title") or "").strip()
        if not src:
            continue
        jobs_by_text.setdefault(src, []).append((it, "title_zh"))

    # Short blurbs only for top items (<= 20).
    for it in blurb_sorted:
        if (it.get("why_zh") or "").strip():
            continue
        src = (it.get("why") or "").strip()
        if not src:
            src = _first_sentence((it.get("takeaway") or "").strip(), 160)
        if not src:
            continue
        jobs_by_text.setdefault(src, []).append((it, "why_zh"))

    # Takeaways (optional) for top items: helps Chinese pages feel less empty when `why` is short.
    for it in blurb_sorted:
        if (it.get("takeaway_zh") or "").strip():
            continue
        src = (it.get("takeaway") or "").strip()
        if not src:
            continue
        jobs_by_text.setdefault(src, []).append((it, "takeaway_zh"))

    if not jobs_by_text:
        return False

    # Batch translate with stable ordering.
    changed = False
    batch_size = int(os.environ.get("DIGEST_TRANSLATE_BATCH", "20") or "20")
    src_unique = list(jobs_by_text.keys())
    for i in range(0, len(src_unique), batch_size):
        chunk_src = src_unique[i : i + batch_size]
        zh_texts = _translate_en_to_zh(chunk_src)
        for src, zh in zip(chunk_src, zh_texts, strict=True):
            zh = (zh or "").strip()
            # Don't write back non-Chinese "translations" (e.g., when a batch fails and echoes EN).
            if not zh or (not _has_cjk(zh) and not _has_cjk(src)):
                continue
            for it, k_zh in jobs_by_text.get(src, []):
                if zh != (it.get(k_zh) or "").strip():
                    it[k_zh] = zh
                    changed = True

    return changed


def _autofill_digest_notes(digest: dict[str, Any], *, now: dt.datetime) -> bool:
    """
    Auto-fill `note` / `note_zh` (1-line highlight) if missing.

    Used by:
      - digest index card tagline
      - "Today in one line / 今日一句话" on the issue page
    """
    note = (digest.get("note") or "").strip()
    note_zh = (digest.get("note_zh") or "").strip()
    if note and note_zh:
        return False

    items = digest.get("items", [])
    if not isinstance(items, list) or not items:
        return False

    ranked = sorted(
        items,
        key=lambda it: (
            _importance_score(it, now=now),
            _parse_published_utc(it) or dt.datetime.min.replace(tzinfo=dt.timezone.utc),
        ),
        reverse=True,
    )
    ai_cat = "AI tools & model releases"
    news = [it for it in ranked if (it.get("category") or "").strip() != ai_cat]
    ai = [it for it in ranked if (it.get("category") or "").strip() == ai_cat]

    top_news = _select_top_diverse(
        news,
        n=min(3, len(news)),
        max_per_source=2,
        max_per_domain=1,
        max_per_category=2,
    )
    ai_top_pool = _ai_item_practicality_filter(ai)
    top_ai = _select_top_diverse(
        ai_top_pool,
        n=min(2, len(ai_top_pool)),
        max_per_source=1,
        max_per_domain=20,
        max_per_category=10,
    )

    def _clean(s: str) -> str:
        s = (s or "").strip()
        s = re.sub(r"\s+", " ", s)
        return s.strip(" .;:，。；：-—")

    def _shorten(s: str, max_len: int) -> str:
        s = _clean(s)
        if len(s) <= max_len:
            return s
        return s[: max_len - 1].rstrip() + "…"

    if not note:
        picks: list[str] = []
        for it in top_news[:2]:
            t = (it.get("title") or "").strip()
            if t:
                picks.append(_shorten(t, 64))
        if top_ai:
            t = (top_ai[0].get("title") or "").strip()
            if t:
                picks.append(_shorten(t, 58))
        if picks:
            if len(picks) == 1:
                note = f"Today: {picks[0]}."
            elif len(picks) == 2:
                note = f"Today: {picks[0]} · {picks[1]}."
            else:
                note = f"Today: {picks[0]} · {picks[1]} · AI: {picks[2]}."

    if not note_zh:
        picks_zh: list[str] = []
        for it in top_news[:2]:
            t = (it.get("title_zh") or "").strip()
            if t and _has_cjk(t):
                picks_zh.append(_shorten(t, 38))
            else:
                t2 = (it.get("title") or "").strip()
                if t2:
                    picks_zh.append(_shorten(t2, 38))
        if top_ai:
            t = (top_ai[0].get("title_zh") or "").strip()
            if t and _has_cjk(t):
                picks_zh.append(_shorten(t, 34))
            else:
                t2 = (top_ai[0].get("title") or "").strip()
                if t2:
                    picks_zh.append(_shorten(t2, 34))
        if picks_zh:
            if len(picks_zh) == 1:
                note_zh = f"今日关注：{picks_zh[0]}。"
            elif len(picks_zh) == 2:
                note_zh = f"今日关注：{picks_zh[0]}；{picks_zh[1]}。"
            else:
                note_zh = f"今日关注：{picks_zh[0]}；{picks_zh[1]}；AI：{picks_zh[2]}。"

    changed = False
    if note and note != (digest.get("note") or "").strip():
        digest["note"] = note
        changed = True
    if note_zh and note_zh != (digest.get("note_zh") or "").strip():
        digest["note_zh"] = note_zh
        changed = True
    return changed


def _clean_title(title: str) -> str:
    title = re.sub(r"\s+", " ", title or "").strip()
    # arXiv RSS titles sometimes include extra whitespace/line breaks.
    return title


def _strip_html(s: str) -> str:
    # RSS summaries often include HTML tags. Keep a plain-text snippet for drafting.
    s = html.unescape(s or "")
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _http_get_bytes(url: str, *, timeout: int = FETCH_TIMEOUT_SECONDS) -> bytes:
    """
    Fetch bytes with a stable User-Agent and gzip support.

    Some feeds return empty or hang without a UA. We fetch bytes ourselves (instead of letting
    feedparser fetch) so we can set headers and enforce timeouts consistently.
    """
    req = urllib.request.Request(
        url,
        headers={
            "User-Agent": f"personal-site-digest/1.0 (+{DEFAULT_SITE_URL})",
            "Accept": "application/rss+xml, application/atom+xml, application/xml;q=0.9, text/xml;q=0.8, */*;q=0.1",
            "Accept-Encoding": "gzip",
        },
        method="GET",
    )
    with urllib.request.urlopen(req, timeout=timeout) as resp:
        raw = resp.read()
        enc = (resp.headers.get("Content-Encoding") or "").lower().strip()
        if enc == "gzip":
            try:
                raw = gzip.decompress(raw)
            except Exception:
                pass
        return raw


def _fetch_feed(url: str) -> Any | None:
    """
    Fetch + parse RSS/Atom from bytes.
    Returns a feedparser feed-like object (with `.entries`) or None on failure.
    """
    try:
        raw = _http_get_bytes(url, timeout=FETCH_TIMEOUT_SECONDS)
        feed = feedparser.parse(raw)
        # bozo means parsing issues; if there are entries we still keep it.
        if getattr(feed, "bozo", False) and not (getattr(feed, "entries", None) or []):
            return None
        return feed
    except Exception:
        return None


def _fetch_github_releases(repo: str, *, per_page: int = 10) -> Any | None:
    """
    Fetch GitHub releases via API and expose them as feed-like entries.

    This avoids github.com Atom timeouts (common from this environment) while keeping the rest
    of the pipeline unchanged.
    """
    repo = (repo or "").strip()
    if not repo or "/" not in repo:
        return None
    url = f"https://api.github.com/repos/{repo}/releases?per_page={per_page}"
    req = urllib.request.Request(
        url,
        headers={
            "User-Agent": f"personal-site-digest/1.0 (+{DEFAULT_SITE_URL})",
            "Accept": "application/vnd.github+json",
            "X-GitHub-Api-Version": "2022-11-28",
        },
        method="GET",
    )
    try:
        with urllib.request.urlopen(req, timeout=FETCH_TIMEOUT_SECONDS) as resp:
            data = json.loads(resp.read().decode("utf-8", errors="replace"))
    except Exception:
        return None

    if not isinstance(data, list):
        return None

    entries: list[Any] = []
    for rel in data:
        if not isinstance(rel, dict):
            continue
        if rel.get("draft") or rel.get("prerelease"):
            continue
        link = (rel.get("html_url") or "").strip()
        if not link:
            continue
        tag = (rel.get("tag_name") or "").strip()
        name = (rel.get("name") or "").strip()
        title = name or tag
        if not title:
            continue
        # Prefix repo for readability and better zh translation.
        title = f"{repo}: {title}"
        summary = (rel.get("body") or "").strip()
        when_iso = (rel.get("published_at") or rel.get("created_at") or rel.get("updated_at") or "").strip()
        published_parsed = None
        if when_iso:
            try:
                when = dt.datetime.fromisoformat(when_iso.replace("Z", "+00:00")).astimezone(dt.timezone.utc)
                published_parsed = when.utctimetuple()
            except Exception:
                published_parsed = None
        entries.append(SimpleNamespace(title=title, link=link, summary=summary, published_parsed=published_parsed))

    return SimpleNamespace(entries=entries)


def _first_sentence(s: str, max_len: int = 160) -> str:
    s = re.sub(r"\s+", " ", (s or "").strip())
    if not s:
        return ""
    # Heuristic sentence split. Good enough for drafting.
    m = re.split(r"(?<=[.!?])\s+", s, maxsplit=1)
    out = (m[0] if m else s).strip()
    if len(out) > max_len:
        out = out[: max_len - 1].rstrip() + "…"
    return out


def _truncate(s: str, max_len: int) -> str:
    s = re.sub(r"\s+", " ", (s or "").strip())
    if len(s) <= max_len:
        return s
    return s[: max_len - 1].rstrip() + "…"


def _draft_items(
    sources: list[dict[str, Any]],
    since_utc: dt.datetime,
    max_items: int,
) -> list[dict[str, Any]]:
    items: list[dict[str, Any]] = []
    now_utc = dt.datetime.now(dt.timezone.utc)
    # Tool/model releases (MCP servers, SDKs, etc.) are often weekly/biweekly, so keep a longer window.
    ai_hours = int(os.environ.get("DIGEST_AI_DRAFT_HOURS", "336") or "336")  # 14 days by default

    # Keep the AI tools section practical: we want tool/model release updates.
    # For AI-news feeds that also publish commentary, reclassify non-release items into AI frontier.
    ai_tools_news_ids = {"decoder-ai", "wired-ai-tag", "venturebeat-ai"}
    ai_tools_misc_ids = {"simon-willison"}
    # Stricter heuristic: require at least one "action" and one "object" keyword
    # to keep an item in the AI tools/model releases section.
    ai_action_kw = [
        "release",
        "launch",
        "announc",
        "introduc",
        "roll out",
        "rollout",
        "unveil",
        "ship",
        "update",
        "new",
        "open-source",
        "open source",
        "beta",
        "preview",
        "available",
    ]
    ai_object_kw = [
        "model",
        "api",
        "sdk",
        "agent",
        "tool",
        "workflow",
        "plugin",
        "mcp",
        "server",
        "checkpoint",
        "diffusion",
        "image",
        "video",
        "inference",
        "fine-tune",
        "finetune",
    ]
    ai_relevance_kw = [
        "llm",
        "gpt",
        "openai",
        "anthropic",
        "claude",
        "gemini",
        "mcp",
        "prompt",
        "embedding",
        "transformer",
        "diffusion",
        "agent",
        "model",
        "inference",
        "tool",
        "sdk",
        "api",
    ]

    def fetch(src: dict[str, Any]) -> tuple[dict[str, Any], Any | None]:
        url = (src.get("url") or "").strip()
        try:
            gh_repo = (src.get("github_repo") or "").strip()
            if gh_repo:
                return (src, _fetch_github_releases(gh_repo))
            if not url:
                return (src, None)
            return (src, _fetch_feed(url))
        except Exception as e:
            name = src.get("name", "").strip() or src.get("id", "").strip() or "unknown"
            print(f"warn: failed to fetch '{name}' ({url}): {e}", file=sys.stderr)
            return (src, None)

    # Fetch feeds concurrently; slow/blocked feeds shouldn't stall the whole daily run.
    with ThreadPoolExecutor(max_workers=FETCH_WORKERS) as ex:
        futs = [ex.submit(fetch, s) for s in sources]
        for fut in as_completed(futs):
            src, feed = fut.result()
            if not feed:
                continue
            for e in getattr(feed, "entries", []) or []:
                when = _pick_entry_time(e)
                # Separate lookback window for AI tool/model releases: releases are bursty and may
                # not happen daily, so we keep a longer window there to increase variety.
                cat_for_cutoff = (src.get("category") or "").strip()
                cutoff = (
                    now_utc - dt.timedelta(hours=ai_hours)
                    if cat_for_cutoff == "AI tools & model releases"
                    else since_utc
                )
                if when and when < cutoff:
                    continue
                title = _clean_title(getattr(e, "title", "") or "")
                link = (getattr(e, "link", "") or "").strip()
                summary = _strip_html(getattr(e, "summary", "") or getattr(e, "description", "") or "")
                if not title or not link:
                    continue
                # Optional per-source keyword filter (useful for broad announcement feeds).
                inc = src.get("include_keywords")
                if isinstance(inc, list) and inc:
                    blob = f"{title} {summary}".lower()
                    if not any(str(k).lower() in blob for k in inc if str(k).strip()):
                        continue

                # GitHub releases feeds often use tag-only titles (e.g., "v0.7.1" or "b7836"),
                # which are unreadable in the Chinese digest and fail our CJK presence check.
                # Prefix with repo name so translation produces a Chinese cue like "发布/更新".
                try:
                    u0 = urlparse(link)
                    dom0 = (u0.netloc or "").lower()
                    p0 = (u0.path or "").strip("/")
                    parts0 = [p for p in p0.split("/") if p]
                    if "github.com" in dom0 and len(parts0) >= 3 and parts0[2].lower().startswith("releases"):
                        repo0 = f"{parts0[0]}/{parts0[1]}"
                        if repo0 and repo0.lower() not in title.lower():
                            title = f"{repo0}: {title}"
                except Exception:
                    pass
                # Category override heuristics (some sources have broad feeds).
                cat = src.get("category", "").strip() or "Misc"
                sid = (src.get("id") or "").strip()
                if cat == "AI tools & model releases" and sid in ai_tools_news_ids:
                    blob = f"{title} {summary}".lower()
                    keep = any(k in blob for k in ai_action_kw) and any(k in blob for k in ai_object_kw)
                    if not keep:
                        cat = "AI frontier"
                elif cat == "AI tools & model releases" and sid in ai_tools_misc_ids:
                    # Filter out non-AI posts from general blogs that sometimes cover AI tooling.
                    # Use title-only: some blogs include sitewide navigation in summaries, causing false hits.
                    blob = f"{title}".lower()
                    if not any(k in blob for k in ai_relevance_kw):
                        # Keep the section high-signal: don't leak unrelated posts into the main news list.
                        continue
                try:
                    u = urlparse(link)
                    dom = (u.netloc or "").lower()
                    path = (u.path or "").lower()
                    if "arstechnica.com" in dom and path.startswith("/security/"):
                        cat = "Security"
                except Exception:
                    pass
                # Provide draft-level snippets (user should rewrite for quality).
                # For GitHub releases, summaries are too verbose/noisy; keep it readable.
                is_gh_release = False
                try:
                    u1 = urlparse(link)
                    dom1 = (u1.netloc or "").lower()
                    p1 = (u1.path or "").lower()
                    is_gh_release = ("github.com" in dom1) and ("/releases/" in p1)
                except Exception:
                    is_gh_release = False

                if is_gh_release:
                    takeaway = ""
                    why = "Release update (see link for changelog)."
                else:
                    takeaway = _truncate(summary, 360) if summary else ""
                    why = _first_sentence(summary, 140) if summary else ""
                items.append(
                    {
                        "category": cat,
                        "title": title,
                        "why": why,
                        "takeaway": takeaway,
                        "prompt": "",
                        "url": link,
                        "source": src.get("name", "").strip() or src.get("id", "").strip(),
                        "published_utc": when.isoformat() if when else "",
                        # Chinese fields are intentionally left blank for human curation.
                        "title_zh": "",
                        "why_zh": "",
                        "takeaway_zh": "",
                        "prompt_zh": "",
                    }
                )

    # Deduplicate by URL, then by normalized title.
    seen_url: set[str] = set()
    seen_title: set[str] = set()
    deduped: list[dict[str, Any]] = []
    for it in sorted(items, key=lambda x: x.get("published_utc", ""), reverse=True):
        url = it.get("url", "")
        tnorm = _norm(it.get("title", ""))
        if url in seen_url or tnorm in seen_title:
            continue
        seen_url.add(url)
        seen_title.add(tnorm)
        deduped.append(it)

    # High-volume sources (e.g., Hacker News, FT) can otherwise dominate the most-recent slice,
    # leading to a Top 10 that feels single-source. Apply simple caps here to keep candidates diverse.
    total = int(max_items or 200)
    cap_per_source = int(os.environ.get("DIGEST_DRAFT_MAX_PER_SOURCE", "20") or "20")
    cap_per_domain = int(os.environ.get("DIGEST_DRAFT_MAX_PER_DOMAIN", "20") or "20")
    cap_github_domain = int(os.environ.get("DIGEST_DRAFT_MAX_GITHUB_PER_DOMAIN", "80") or "80")

    by_src: dict[str, int] = {}
    by_dom: dict[str, int] = {}
    out: list[dict[str, Any]] = []
    for it in deduped:
        if len(out) >= total:
            break
        src = (it.get("source") or "").strip()
        dom = (urlparse((it.get("url") or "").strip()).netloc or "").lower()
        if src and by_src.get(src, 0) >= cap_per_source:
            continue
        cap_dom = cap_github_domain if dom == "github.com" else cap_per_domain
        if dom and by_dom.get(dom, 0) >= cap_dom:
            continue
        out.append(it)
        if src:
            by_src[src] = by_src.get(src, 0) + 1
        if dom:
            by_dom[dom] = by_dom.get(dom, 0) + 1

    return out


def _save_yaml(path: Path, payload: Any) -> None:
    path.write_text(yaml.safe_dump(payload, sort_keys=False, allow_unicode=True), encoding="utf-8")


def _load_digest_yaml(path: Path) -> dict[str, Any]:
    if not path.exists():
        _die(f"missing digest file: {path}")
    raw = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
    if not isinstance(raw, dict):
        _die(f"invalid digest yaml: {path}")
    return raw


def _compute_top_lists(date: dt.date, digest: dict[str, Any], *, lang: str = "en") -> tuple[list[dict[str, Any]], list[dict[str, Any]]]:
    items = digest.get("items", [])
    if not isinstance(items, list) or not items:
        return ([], [])

    now = _score_now_for_date(date)
    ranked = sorted(
        items,
        key=lambda it: (
            _importance_score(it, now=now),
            _parse_published_utc(it) or dt.datetime.min.replace(tzinfo=dt.timezone.utc),
        ),
        reverse=True,
    )

    ai_cat = "AI tools & model releases"
    max_news = int(digest.get("max_items", 0) or DEFAULT_MAX_ITEMS_ON_SITE)
    max_ai = int(digest.get("max_items_ai", 0) or DEFAULT_MAX_AI_ITEMS_ON_SITE)

    if lang == "zh":
        ranked_zh: list[dict[str, Any]] = []
        for it in ranked:
            t_zh = (it.get("title_zh") or "").strip()
            if not t_zh or not _has_cjk(t_zh):
                continue
            ranked_zh.append(it)
        items_news = [it for it in ranked_zh if (it.get("category") or "").strip() != ai_cat][:max_news]
        items_ai = [it for it in ranked_zh if (it.get("category") or "").strip() == ai_cat][:max_ai]
    else:
        items_news = [it for it in ranked if (it.get("category") or "").strip() != ai_cat][:max_news]
        items_ai = [it for it in ranked if (it.get("category") or "").strip() == ai_cat][:max_ai]

    top_news = _select_top_diverse(
        items_news,
        n=10,
        max_per_source=int(os.environ.get("DIGEST_TOP_MAX_PER_SOURCE", "3") or "3"),
        max_per_domain=int(os.environ.get("DIGEST_TOP_MAX_PER_DOMAIN", "1") or "1"),
        max_per_category=int(os.environ.get("DIGEST_TOP_MAX_PER_CATEGORY", "5") or "5"),
    )

    ai_top_pool = _ai_item_practicality_filter(items_ai)
    top_ai = _select_top_diverse(
        ai_top_pool,
        n=min(10, len({(it.get("source") or "").strip() for it in ai_top_pool if (it.get("source") or "").strip()})),
        max_per_source=int(os.environ.get("DIGEST_AI_TOP_MAX_PER_SOURCE", "1") or "1"),
        max_per_domain=int(os.environ.get("DIGEST_AI_TOP_MAX_PER_DOMAIN", "20") or "20"),
        max_per_category=int(os.environ.get("DIGEST_AI_TOP_MAX_PER_CATEGORY", "10") or "10"),
    )
    return (top_news, top_ai)


def _update_weekly_data(date: dt.date, digest: dict[str, Any]) -> None:
    """
    Maintain a weekly aggregation file for the site.

    Output: `personal-site/_data/digest_weekly.yml`
    """
    start = date - dt.timedelta(days=date.weekday())
    end = start + dt.timedelta(days=6)
    iso = start.isocalendar()
    week_key = f"{iso.year}-W{iso.week:02d}"

    payload = {"weeks": []}
    if DATA_WEEKLY_PATH.exists():
        try:
            loaded = yaml.safe_load(DATA_WEEKLY_PATH.read_text(encoding="utf-8")) or {}
            if isinstance(loaded, dict) and isinstance(loaded.get("weeks"), list):
                payload = loaded
        except Exception:
            payload = {"weeks": []}

    weeks: list[dict[str, Any]] = payload.get("weeks") or []
    if not isinstance(weeks, list):
        weeks = []

    wk: dict[str, Any] | None = None
    for w in weeks:
        if isinstance(w, dict) and (w.get("week") == week_key):
            wk = w
            break
    if wk is None:
        wk = {
            "week": week_key,
            "start": start.isoformat(),
            "end": end.isoformat(),
            "issues": [],
            "highlights_news": [],
            "highlights_ai": [],
        }
        weeks.append(wk)

    issues = wk.get("issues") if isinstance(wk.get("issues"), list) else []
    if date.isoformat() not in issues:
        issues.append(date.isoformat())
    issues = sorted(set(issues))
    wk["issues"] = issues

    top_news, top_ai = _compute_top_lists(date, digest, lang="en")

    def add_highlights(key: str, items: list[dict[str, Any]], cap: int) -> None:
        cur = wk.get(key) if isinstance(wk.get(key), list) else []
        if not isinstance(cur, list):
            cur = []
        seen = {str(x.get("url")) for x in cur if isinstance(x, dict) and x.get("url")}
        for it in items[:5]:
            u = (it.get("url") or "").strip()
            if not u or u in seen:
                continue
            cur.append(
                {
                    "date": date.isoformat(),
                    "url": u,
                    "title": (it.get("title") or "").strip(),
                    "title_zh": (it.get("title_zh") or "").strip(),
                    "category": (it.get("category") or "").strip(),
                    "source": (it.get("source") or "").strip(),
                    "published_utc": (it.get("published_utc") or "").strip(),
                }
            )
            seen.add(u)
        wk[key] = cur[:cap]

    add_highlights("highlights_news", top_news, cap=25)
    add_highlights("highlights_ai", top_ai, cap=25)

    # Keep a stable order.
    weeks_sorted = sorted(weeks, key=lambda w: (w.get("week") or ""), reverse=False)
    payload["weeks"] = weeks_sorted
    _save_yaml(DATA_WEEKLY_PATH, payload)


def _group_by_category(items: list[dict[str, Any]]) -> list[tuple[str, list[dict[str, Any]]]]:
    groups: dict[str, list[dict[str, Any]]] = {}
    for it in items:
        cat = (it.get("category", "") or "Misc").strip()
        groups.setdefault(cat, []).append(it)
    # Preserve a sensible, stable ordering: CSS first, then AI, then others.
    order = [
        "Business & economy",
        "Tech industry",
        "Security",
        "CSS / AI & society",
        "AI frontier",
        "AI tools & model releases",
        "Digital health / health comm",
        "Education / learning sciences / edtech",
        "Misc",
    ]
    ordered: list[tuple[str, list[dict[str, Any]]]] = []
    for k in order:
        if k in groups:
            ordered.append((k, groups.pop(k)))
    for k in sorted(groups.keys()):
        ordered.append((k, groups[k]))
    return ordered


CAT_ZH = {
    "Business & economy": "商业 / 经济",
    "Tech industry": "科技 / 产业",
    "Security": "安全 / 风险",
    "CSS / AI & society": "CSS / AI 与社会",
    "AI frontier": "AI 前沿",
    "AI tools & model releases": "AI 工具 / 模型发布",
    "Digital health / health comm": "数字健康 / 健康传播",
    "Education / learning sciences / edtech": "教育 / 学习科学 / 教育技术",
    "Misc": "其他",
}


def _parse_published_utc(it: dict[str, Any]) -> dt.datetime | None:
    s = (it.get("published_utc") or "").strip()
    if not s:
        return None
    try:
        return dt.datetime.fromisoformat(s.replace("Z", "+00:00"))
    except Exception:
        return None


def _score_now_for_date(date: dt.date) -> dt.datetime:
    # Use a stable "now" anchored to the digest date so rebuilds are deterministic.
    return dt.datetime.combine(date, dt.time(23, 59, 59), tzinfo=dt.timezone.utc)


def _importance_score(it: dict[str, Any], *, now: dt.datetime | None = None) -> int:
    """
    Rank items for the website digest list.

    This is intentionally heuristic: source quality + policy/market/AI-infra keywords,
    with a small recency bump.
    """
    src = (it.get("source") or "").lower()
    title = (it.get("title") or "").lower()
    cat = (it.get("category") or "").lower()

    score = 0

    # Source quality / typical impact.
    if "reuters" in src:
        score += 90
    elif "financial times" in src or src.startswith("ft"):
        score += 85
    elif "economist" in src:
        score += 75
    elif "bbc" in src:
        score += 70
    elif "mit technology review" in src:
        score += 68
    elif "guardian" in src:
        score += 62
    elif "ars technica" in src:
        score += 60
    elif "techcrunch" in src:
        score += 55
    elif "the verge" in src:
        score += 52
    elif "hacker news" in src:
        score += 15
    elif "arxiv" in src:
        score += 20
    elif "hugging face" in src:
        score += 22
    else:
        score += 30

    # Category nudge.
    if "security" in cat:
        score += 18
    elif "business" in cat or "economy" in cat or "markets" in cat:
        score += 14
    elif "tech" in cat:
        score += 12
    elif "ai frontier" in cat:
        score += 10
    elif "ai tools" in cat or "model releases" in cat:
        score += 11
    elif "ai & society" in cat or "css" in cat:
        score += 8

    # Keyword signals.
    kw = {
        # Macro / markets
        "inflation": 10,
        "interest rate": 10,
        "house prices": 7,
        "stocks": 6,
        "bond": 6,
        "tariff": 10,
        "trade": 8,
        "sanction": 8,
        # Regulation / governance
        "antitrust": 10,
        "regulator": 8,
        "privacy": 7,
        "data": 6,
        "water": 6,
        "energy": 6,
        # AI / platforms / infra
        "openai": 10,
        "chatgpt": 9,
        "anthropic": 8,
        "nvidia": 8,
        "meta": 7,
        "tiktok": 6,
        "youtube": 6,
        "netflix": 6,
        "semiconductor": 8,
        "asml": 8,
        "chip": 7,
        # Geopolitics / conflict
        "ukraine": 10,
        "russia": 9,
        "greenland": 8,
        "nato": 7,
        "drone": 7,
        # Security-ish
        "breach": 10,
        "hack": 10,
        "ransom": 10,
        "vulnerability": 10,
        "zero-day": 10,

        # Practical AI tooling / agents / visuals
        "mcp": 12,
        "model context protocol": 12,
        "modelcontextprotocol": 12,
        "comfyui": 10,
        "stable diffusion": 9,
        "sdxl": 9,
        "image model": 8,
        "video model": 8,
    }
    for k, w in kw.items():
        if k in title:
            score += w

    # AI tools: de-noise commit-tag style releases (too frequent to be useful in a daily brief).
    if "ai tools" in cat or "model releases" in cat:
        if "llama.cpp" in title and re.search(r"\brelease:\s*b\d{4,}\b", title):
            score -= 18

    # Recency: small bump for newer items.
    when = _parse_published_utc(it)
    if when:
        ref = now or dt.datetime.now(dt.timezone.utc)
        age_h = (ref - when).total_seconds() / 3600.0
        if age_h <= 6:
            score += 6
        elif age_h <= 12:
            score += 4
        elif age_h <= 24:
            score += 2

    return score


def _ai_item_practicality_filter(items: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """
    Filter low-signal AI tool items for the *Top* section.

    Goal: keep "new tool / model / MCP / visual release" style items, avoid noisy prereleases
    and endless commit-tag updates dominating the top list.
    """

    def _is_noisy(it: dict[str, Any]) -> bool:
        t = (it.get("title") or "").lower()
        u = (it.get("url") or "").lower()
        # GitHub releases are, by definition, "tool updates"—keep them unless they are noisy pre-releases.
        if "github.com" in u and "/releases/" in u:
            # Pre-releases/nightlies are too frequent for a daily brief.
            if any(x in t for x in ["nightly", "rc", "beta", "alpha", "dev preview", "preview", "snapshot"]):
                return True
            # llama.cpp commit-tag releases are extremely frequent; keep them out of Top unless needed.
            if "llama.cpp" in t and re.search(r"\brelease:\s*b\d{4,}\b", t):
                return True
            return False
        # Keep Top list practical: require at least one tooling/release cue.
        practical_kw = [
            "release",
            "launch",
            "introduc",
            "rollout",
            "roll out",
            "unveil",
            "ship",
            "update",
            "adds",
            "add support",
            "support",
            "sdk",
            "api",
            "mcp",
            "server",
            "agent",
            "tool",
            "workflow",
            "plugin",
            "model",
            "inference",
            "open-source",
            "open source",
        ]
        if not any(k in t for k in practical_kw):
            return True
        # Pre-releases/nightlies are too frequent for a daily brief.
        if any(x in t for x in ["nightly", "rc", "beta", "alpha", "dev preview", "preview", "snapshot"]):
            return True
        # llama.cpp commit-tag releases are extremely frequent; keep them out of Top unless needed.
        if "llama.cpp" in t and re.search(r"\brelease:\s*b\d{4,}\b", t):
            return True
        return False

    filtered = [it for it in items if not _is_noisy(it)]
    # If we filtered too aggressively, fall back to original list.
    return filtered if len(filtered) >= 3 else items


def _select_top_diverse(
    ranked_items: list[dict[str, Any]],
    *,
    n: int,
    max_per_source: int,
    max_per_domain: int,
    max_per_category: int,
) -> list[dict[str, Any]]:
    """
    Pick a diversified Top N from an already-ranked list.

    Motivation: a single high-volume/high-weight outlet (e.g., FT) can dominate a Top 10 or card.
    We cap items by:
      - per `source` label
      - per URL domain (e.g., ft.com)
      - per category (soft cap; can be relaxed during filling)
    """

    def _domain(u: str) -> str:
        try:
            return (urlparse(u).netloc or "").lower()
        except Exception:
            return ""

    picked: list[dict[str, Any]] = []
    used: set[str] = set()
    by_src: dict[str, int] = {}
    by_dom: dict[str, int] = {}
    by_cat: dict[str, int] = {}

    # Phase 1: strict caps (source + domain + category).
    for it in ranked_items:
        if len(picked) >= n:
            break
        key = it.get("url", "") or it.get("title", "")
        if not key or key in used:
            continue
        src = (it.get("source") or "").strip()
        dom = _domain((it.get("url") or "").strip())
        cat = (it.get("category") or "Misc").strip() or "Misc"

        if src and by_src.get(src, 0) >= max_per_source:
            continue
        if dom and by_dom.get(dom, 0) >= max_per_domain:
            continue
        if cat and by_cat.get(cat, 0) >= max_per_category:
            continue

        picked.append(it)
        used.add(key)
        if src:
            by_src[src] = by_src.get(src, 0) + 1
        if dom:
            by_dom[dom] = by_dom.get(dom, 0) + 1
        if cat:
            by_cat[cat] = by_cat.get(cat, 0) + 1

    # Phase 2: preserve *source diversity* (domain/source caps), relax category cap.
    if len(picked) < n:
        for it in ranked_items:
            if len(picked) >= n:
                break
            key = it.get("url", "") or it.get("title", "")
            if not key or key in used:
                continue
            src = (it.get("source") or "").strip()
            dom = _domain((it.get("url") or "").strip())
            if src and by_src.get(src, 0) >= max_per_source:
                continue
            if dom and by_dom.get(dom, 0) >= max_per_domain:
                continue

            picked.append(it)
            used.add(key)
            if src:
                by_src[src] = by_src.get(src, 0) + 1
            if dom:
                by_dom[dom] = by_dom.get(dom, 0) + 1

    # Phase 3: final fallback — fill ignoring caps (better a complete list than empty).
    if len(picked) < n:
        for it in ranked_items:
            if len(picked) >= n:
                break
            key = it.get("url", "") or it.get("title", "")
            if not key or key in used:
                continue
            picked.append(it)
            used.add(key)

    return picked


def _pick_lang(it: dict[str, Any], key_en: str, key_zh: str, lang: str) -> str:
    if lang == "zh":
        v = (it.get(key_zh) or "").strip()
        if v:
            return v
        # Fallback to English if Chinese is missing.
    return (it.get(key_en) or "").strip()


def _font_paths(lang: str) -> dict[str, Path]:
    """
    Pick fonts that actually contain the glyphs we need.

    For zh, we must use a CJK font, otherwise Pillow will render tofu boxes.
    """
    if lang == "zh":
        candidates = [
            # Prefer Heiti (黑体) per site style guide.
            ("/System/Library/Fonts/STHeiti Light.ttc", "regular"),
            ("/System/Library/Fonts/STHeiti Medium.ttc", "bold"),
            ("/System/Library/Fonts/STHeiti Light.ttc", "regular"),
            ("/System/Library/Fonts/Hiragino Sans GB.ttc", "regular"),
            ("/System/Library/Fonts/Supplemental/Songti.ttc", "regular"),
        ]
    else:
        # Prefer Times New Roman per site style guide.
        candidates = [
            ("/System/Library/Fonts/Supplemental/Times New Roman.ttf", "regular"),
            ("/System/Library/Fonts/Supplemental/Times New Roman Bold.ttf", "bold"),
            ("/System/Library/Fonts/SFNSMono.ttf", "mono"),
            ("/System/Library/Fonts/Supplemental/Arial.ttf", "regular"),
            ("/System/Library/Fonts/Supplemental/Arial Bold.ttf", "bold"),
            ("/System/Library/Fonts/Supplemental/Arial Unicode.ttf", "regular"),
            ("/System/Library/Fonts/Supplemental/Helvetica.ttc", "regular"),
        ]
    out: dict[str, Path] = {}
    for fp, role in candidates:
        p = Path(fp)
        if p.exists() and role not in out:
            out[role] = p
    return out


def _as_bool(v: str | None) -> bool:
    return (v or "").strip().lower() in {"1", "true", "yes", "y", "on"}

def _as_bool_default(v: str | None, default: bool) -> bool:
    if v is None:
        return default
    s = v.strip().lower()
    if s in {"0", "false", "no", "n", "off"}:
        return False
    if s in {"1", "true", "yes", "y", "on"}:
        return True
    return default


def _openrouter_headers(*, title: str) -> dict[str, str]:
    key = _openrouter_key()
    if not key:
        return {}
    return {
        "Authorization": f"Bearer {key}",
        # These are recommended by OpenRouter; keep them stable and non-sensitive.
        "HTTP-Referer": DEFAULT_SITE_URL,
        "X-Title": title,
        "Content-Type": "application/json",
    }


def _openrouter_generate_image_png(*, model: str, prompt: str) -> bytes:
    """
    Generate a single PNG via an image-capable OpenRouter chat model.

    Many Gemini image models return base64 in `choices[0].message.images[0].image_url.url`.
    """
    key = _openrouter_key()
    if not key:
        raise RuntimeError("OPENROUTER_API_KEY missing")

    url = "https://openrouter.ai/api/v1/chat/completions"
    payload = {
        "model": model,
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            "Generate ONE single image. OUTPUT MUST BE IMAGE ONLY.\n"
                            "ABSOLUTELY NO TEXT: no letters, no words, no numbers, no symbols, no watermark, no logo.\n"
                            + prompt
                        ),
                    }
                ],
            }
        ],
        "modalities": ["image", "text"],
        "temperature": 0,
    }

    req = urllib.request.Request(
        url,
        data=json.dumps(payload).encode("utf-8"),
        headers=_openrouter_headers(title="Personal Site Digest Card BG") or {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
    )

    try:
        with urllib.request.urlopen(req, timeout=90) as resp:
            raw = resp.read()
    except Exception as e:
        raise RuntimeError(f"OpenRouter image request failed: {e}") from e

    result = json.loads(raw.decode("utf-8"))
    message = result["choices"][0]["message"]
    images = message.get("images") or []
    if not images:
        raise RuntimeError("OpenRouter response missing images payload")

    image_data = images[0]
    image_url = None
    if isinstance(image_data, dict):
        if isinstance(image_data.get("image_url"), dict):
            image_url = image_data["image_url"].get("url")
        image_url = image_url or image_data.get("url") or image_data.get("data")
    else:
        image_url = image_data

    if not image_url or not str(image_url).startswith("data:image"):
        raise RuntimeError("Unexpected image payload format")

    b64 = str(image_url).split(",", 1)[1]
    return base64.b64decode(b64)


def _resize_to_cover(im: Image.Image, w: int, h: int) -> Image.Image:
    """
    Resize without distortion: scale to cover target, then center-crop.
    This avoids "stretched" backgrounds when the source aspect ratio differs.
    """
    im = im.convert("RGB")
    scale = max(w / im.width, h / im.height)
    new_size = (max(1, int(round(im.width * scale))), max(1, int(round(im.height * scale))))
    im = im.resize(new_size, Image.Resampling.LANCZOS)
    left = max(0, (im.width - w) // 2)
    top = max(0, (im.height - h) // 2)
    return im.crop((left, top, left + w, top + h))


def _wrap(draw: ImageDraw.ImageDraw, text: str, font: ImageFont.FreeTypeFont, max_width: int) -> list[str]:
    """
    Simple word-wrapping that preserves spaces.

    Important: do NOT strip whitespace aggressively, otherwise words will run together
    in the rendered card (a common issue when splitting on whitespace).
    """
    text = re.sub(r"\s+", " ", (text or "").strip())
    if not text:
        return []

    words = text.split(" ")
    lines: list[str] = []
    cur = ""

    def flush() -> None:
        nonlocal cur
        if cur:
            lines.append(cur)
            cur = ""

    for w in words:
        test = w if not cur else f"{cur} {w}"
        if draw.textlength(test, font=font) <= max_width:
            cur = test
            continue

        # If a single token is too long, hard-wrap it by characters.
        if not cur and draw.textlength(w, font=font) > max_width:
            chunk = ""
            for ch in w:
                t2 = chunk + ch
                if draw.textlength(t2, font=font) <= max_width:
                    chunk = t2
                else:
                    if chunk:
                        lines.append(chunk)
                    chunk = ch
            if chunk:
                cur = chunk
            continue

        flush()
        cur = w

    flush()
    return lines


def _wrap_ellipsize(
    draw: ImageDraw.ImageDraw,
    text: str,
    font: ImageFont.FreeTypeFont,
    max_width: int,
    max_lines: int,
) -> list[str]:
    lines = _wrap(draw, text, font, max_width=max_width)
    if len(lines) <= max_lines:
        return lines
    lines = lines[:max_lines]
    last = lines[-1].rstrip()
    # Add ellipsis without overflowing too much.
    while last and draw.textlength(last + "…", font=font) > max_width:
        last = last[:-1].rstrip()
    lines[-1] = (last + "…") if last else "…"
    return lines


_URL_RE = re.compile(r"https?://\S+")


def _x_char_len(s: str) -> int:
    """
    Approximate X/Twitter length.

    X counts any URL as ~23 chars regardless of actual length. We approximate
    with 23 to split threads safely.
    """
    if not s:
        return 0
    urls = list(_URL_RE.findall(s))
    # Replace each URL with 23 chars in the count.
    return len(s) - sum(len(u) for u in urls) + 23 * len(urls)


def _digest_issue_urls(date: dt.date, site_url: str) -> dict[str, str]:
    base = (site_url or DEFAULT_SITE_URL).rstrip("/")
    return {
        "en": f"{base}/digest/{date.isoformat()}/",
        "zh": f"{base}/zh/digest/{date.isoformat()}/",
        # Cards: 2 for the main news + 2 for AI tools/model releases.
        "news_en_card": f"/assets/img/digests/{date.isoformat()}-en.png",
        "news_zh_card": f"/assets/img/digests/{date.isoformat()}-zh.png",
        "ai_en_card": f"/assets/img/digests/{date.isoformat()}-ai-en.png",
        "ai_zh_card": f"/assets/img/digests/{date.isoformat()}-ai-zh.png",
    }


def _format_item_bilingual(it: dict[str, Any]) -> str:
    t_en = (it.get("title") or "").strip()
    w_en = (it.get("why") or "").strip()
    take_en = (it.get("takeaway") or "").strip()
    p_en = (it.get("prompt") or "").strip()
    t_zh = (it.get("title_zh") or "").strip()
    w_zh = (it.get("why_zh") or "").strip()
    take_zh = (it.get("takeaway_zh") or "").strip()
    p_zh = (it.get("prompt_zh") or "").strip()
    url = (it.get("url") or "").strip()

    en = t_en
    if w_en:
        en = f"{en} — {w_en}"

    zh = t_zh or ""
    if zh and w_zh:
        zh = f"{zh} — {w_zh}"
    elif not zh and w_zh:
        zh = w_zh

    # Professional bulletin: one concise takeaway line (no "my take"/first-person labels).
    # LinkedIn will auto-link raw URLs, so we keep the URL on its own line.
    tl_en = _first_sentence(take_en, 200) if take_en else ""
    tl_zh = _first_sentence(take_zh, 140) if take_zh else ""

    lines: list[str] = []
    if en:
        lines.append(f"- {en} — {tl_en}" if tl_en else f"- {en}")
    if zh:
        lines.append(f"  {zh} — {tl_zh}" if tl_zh else f"  {zh}")
    if url:
        lines.append(f"  {url}")
    return "\n".join(lines).strip()


def _format_linkedin_post(
    date: dt.date,
    digest: dict[str, Any],
    site_url: str,
    *,
    text_mode: str = "bilingual",
    style: str = "bulletin",
) -> str:
    items = digest.get("items", [])
    groups = _group_by_category(items)
    urls = _digest_issue_urls(date, site_url)

    title_en = (digest.get("title") or f"Daily Digest — {date.isoformat()}").strip()
    title_zh = (digest.get("title_zh") or f"每日简报 — {date.isoformat()}").strip()
    note_en = (digest.get("note") or "").strip()
    note_zh = (digest.get("note_zh") or "").strip()

    def blurb(it: dict[str, Any], lang: str) -> str:
        # For social, prefer the short "why" line (less templated, avoids mid-sentence truncation).
        why = _pick_lang(it, "why", "why_zh", lang)
        if why:
            return why
        take = _pick_lang(it, "takeaway", "takeaway_zh", lang)
        return _first_sentence(take, 110 if lang != "zh" else 70)

    now = _score_now_for_date(date)
    ranked_all = sorted(
        items,
        key=lambda it: (
            _importance_score(it, now=now),
            _parse_published_utc(it) or dt.datetime.min.replace(tzinfo=dt.timezone.utc),
        ),
        reverse=True,
    )

    if style == "theme":
        # Theme-first format: a single short paragraph + a clean list of links.
        lines: list[str] = []
        if text_mode == "en":
            lines.extend(
                [
                    f"{title_en}",
                    "",
                    "Cards attached (EN + Chinese card).",
                    "",
                ]
            )
            if note_en:
                lines.extend([_truncate(note_en, 520), ""])
            lines.extend([f"Links (full list): {urls['en']}", "", "Links:", ""])
        else:
            lines.extend(
                [
                    f"{title_en} / {title_zh}",
                    "",
                    "EN + 中文 cards attached. Full lists:",
                    "附英文/中文两张卡片；完整列表：",
                    "",
                    f"EN list: {urls['en']}",
                    f"中文列表: {urls['zh']}",
                    "",
                ]
            )
            if note_en or note_zh:
                if note_en:
                    lines.append(_truncate(note_en, 520))
                if note_zh:
                    lines.append(_truncate(note_zh, 240))
                lines.append("")
            lines.extend(["Links / 链接：", ""])

        # Top items by importance; keep it scannable.
        picks: list[tuple[str, str]] = []
        for it in ranked_all:
            t = (it.get("title") or "").strip()
            u = (it.get("url") or "").strip()
            if t and u:
                picks.append((t, u))

        max_links = 10 if text_mode == "en" else 10
        for t, u in picks[:max_links]:
            lines.append(f"- {_truncate(t, 120)}")
            lines.append(f"  {u}")
        lines.append("")
        return "\n".join(lines).rstrip() + "\n"

    if text_mode == "en":
        lines: list[str] = [
            f"{title_en}",
            "",
            "Cards attached (EN + Chinese card).",
            "",
            f"Full list: {urls['en']}",
            "",
            "Highlights (clickable links):",
            "",
        ]
    else:
        lines = [
            f"{title_en} / {title_zh}",
            "",
            "EN + 中文 cards attached. Full lists:",
            "附英文/中文两张卡片；完整列表：",
        ]

        lines.extend(
            [
                "",
                f"EN list: {urls['en']}",
                f"中文列表: {urls['zh']}",
                "",
                "Bulletin (clickable links) / 快报（可点击链接）：",
                "",
            ]
        )

    if text_mode == "en":
        # LinkedIn EN-only: keep it scannable (no category blocks); rely on the attached card + full list.
        picks: list[dict[str, Any]] = []
        for it in ranked_all:
            t_en = (it.get("title") or "").strip()
            u = (it.get("url") or "").strip()
            if t_en and u:
                picks.append(it)
            if len(picks) >= 3:
                break

        for it in picks:
            t_en = (it.get("title") or "").strip()
            u = (it.get("url") or "").strip()
            src = (it.get("source") or "").strip()
            cat = (it.get("category") or "").strip()
            meta = " · ".join([m for m in [cat, src] if m])
            head = f"- {_truncate(t_en, 120)}"
            if meta:
                head += f" ({meta})"
            lines.append(head)
            lines.append(f"  {u}")
            lines.append("")
    else:
        max_items_total = 10
        rendered = 0
        for cat, cat_items in groups:
            if not cat_items:
                continue
            lines.append(f"{cat} / {CAT_ZH.get(cat, cat)}")
            for it in cat_items:
                if rendered >= max_items_total:
                    break
                t_en = (it.get("title") or "").strip()
                t_zh = (it.get("title_zh") or "").strip()
                u = (it.get("url") or "").strip()
                if not t_en or not u:
                    continue
                b_en = blurb(it, "en")
                b_zh = blurb(it, "zh")
                head = f"- {t_en}"
                if t_zh and t_zh != t_en:
                    head += f" / {t_zh}"
                if b_en:
                    head += f" — {b_en}"
                if b_zh and b_zh != b_en:
                    head += f" / {b_zh}"
                lines.append(head)
                lines.append(f"  {u}")
                rendered += 1
            lines.append("")
            if rendered >= max_items_total:
                break

    if text_mode != "en":
        # Discussion prompts (max 2) at the end.
        prompts: list[tuple[str, str]] = []
        for it in items:
            p_en = (it.get("prompt") or "").strip()
            p_zh = (it.get("prompt_zh") or "").strip()
            if p_en or p_zh:
                prompts.append((p_en, p_zh))
        if prompts:
            lines.append("Open questions / 开放问题：")
            for p_en, p_zh in prompts[:2]:
                if p_en and p_zh:
                    lines.append(f"- {p_en} / {p_zh}")
                elif p_en:
                    lines.append(f"- {p_en}")
                elif p_zh:
                    lines.append(f"- {p_zh}")
            lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def _format_x_thread(
    date: dt.date,
    digest: dict[str, Any],
    site_url: str,
    *,
    text_mode: str = "bilingual",
    style: str = "bulletin",
) -> list[str]:
    items = digest.get("items", [])
    groups = _group_by_category(items)
    urls = _digest_issue_urls(date, site_url)
    note_en = (digest.get("note") or "").strip()
    note_zh = (digest.get("note_zh") or "").strip()

    if text_mode == "en":
        if style == "theme":
            # Theme tweet + link list in replies (so the first post stays readable).
            head: list[str] = [f"Daily Digest ({date.isoformat()}). Cards: EN + Chinese."]
            if note_en:
                head.append(_truncate(note_en, 240))
            head.append(f"Full list: {urls['en']}")
            head.append("Links below.")
            t0 = "\n".join(head).strip()

            lines: list[str] = []
            for _, cat_items in groups:
                for it in cat_items:
                    t = (it.get("title") or "").strip()
                    u = (it.get("url") or "").strip()
                    if t and u:
                        lines.append(f"{_truncate(t, 72)}: {u}")

            tweets: list[str] = [t0]
            cur: list[str] = []
            cur_len = 0
            max_len = 270
            for ln in lines[:12]:  # cap to avoid spammy threads
                # Use X-style length approximation (URLs ~23 chars) to pack more per tweet.
                add_len = _x_char_len(ln) + (1 if cur else 0)
                if cur and cur_len + add_len > max_len:
                    tweets.append("\n".join(cur).strip())
                    cur = [ln]
                    cur_len = _x_char_len(ln)
                else:
                    cur.append(ln)
                    cur_len += add_len
            if cur:
                tweets.append("\n".join(cur).strip())
            return tweets

        # Single post for X: keep it lightweight; attach BOTH cards here.
        picks: list[tuple[str, str]] = []
        for _, cat_items in groups:
            for it in cat_items:
                t = (it.get("title") or "").strip()
                u = (it.get("url") or "").strip()
                if t and u:
                    picks.append((t, u))
        picks = picks[:2]  # keep within X length limits

        lines = [
            f"Daily Digest ({date.isoformat()}). Cards: EN + Chinese.",
            f"Full list: {urls['en']}",
            "",
        ]
        for t, u in picks:
            # One-line items: readable, within length limits; details live on the digest page.
            lines.append(f"{_truncate(t, 72)}: {u}")
        return ["\n".join([ln for ln in lines if ln is not None]).strip()]

    if style == "theme":
        # Bilingual theme post + link list in replies.
        head: list[str] = [f"Daily Digest / 每日简报 ({date.isoformat()})"]
        head.append("Cards attached (EN + 中文).")
        if note_en:
            head.append(_truncate(note_en, 260))
        if note_zh:
            head.append(_truncate(note_zh, 200))
        head.append(f"EN list: {urls['en']}")
        head.append(f"中文列表: {urls['zh']}")
        head.append("Links below / 链接见串。")
        t0 = "\n".join(head).strip()

        lines: list[str] = []
        for _, cat_items in groups:
            for it in cat_items:
                t = (it.get("title") or "").strip()
                u = (it.get("url") or "").strip()
                if t and u:
                    lines.append(f"{_truncate(t, 72)}: {u}")

        tweets: list[str] = [t0]
        cur: list[str] = []
        cur_len = 0
        max_len = 270
        for ln in lines[:12]:
            add_len = _x_char_len(ln) + (1 if cur else 0)
            if cur and cur_len + add_len > max_len:
                tweets.append("\n".join(cur).strip())
                cur = [ln]
                cur_len = _x_char_len(ln)
            else:
                cur.append(ln)
                cur_len += add_len
        if cur:
            tweets.append("\n".join(cur).strip())
        return tweets

    # Tweet 1: short bilingual intro + canonical links (attach BOTH cards here).
    t0 = "\n".join(
        [
            f"Daily Digest / 每日简报 ({date.isoformat()})",
            "EN + 中文 cards attached.",
            f"EN: {urls['en']}",
            f"中文: {urls['zh']}",
            "",
            "Direct links in thread / 详细链接见串：",
        ]
    ).strip()

    tweets: list[str] = [t0]

    # Thread: compact per-item blocks (avoid orphan headings when splitting).
    blocks: list[str] = []
    for cat, cat_items in groups:
        if not cat_items:
            continue
        for it in cat_items[:10]:
            t_en = (it.get("title") or "").strip()
            w_en = (it.get("why") or "").strip()
            take_en = (it.get("takeaway") or "").strip()
            t_zh = (it.get("title_zh") or "").strip()
            w_zh = (it.get("why_zh") or "").strip()
            take_zh = (it.get("takeaway_zh") or "").strip()
            url = (it.get("url") or "").strip()

            cat_zh = CAT_ZH.get(cat, cat)
            cat_label = f"[{cat}]/{cat_zh}"

            tl_en = w_en or _first_sentence(take_en, 200)
            tl_zh = w_zh or _first_sentence(take_zh, 140)

            # Keep X readable: title + short notes + link.
            line = f"{cat_label} {t_en}".strip()
            if tl_en:
                line = f"{line}\nNote: {tl_en}"
            if t_zh:
                line = f"{line}\n{t_zh}"
            if tl_zh:
                line = f"{line}\n要点：{tl_zh}"
            if url:
                line = f"{line}\n{url}"
            blocks.append(line.strip())

    # Pack blocks into tweets with conservative length.
    cur: list[str] = []
    cur_len = 0
    max_len = 265  # keep headroom for platform differences
    for b in blocks:
        add = (b + "\n\n").strip() if b else ""
        if not add:
            continue
        add_len = _x_char_len(add) + (2 if cur else 0)
        if cur and cur_len + add_len > max_len:
            tweets.append("\n\n".join(cur).strip())
            cur = [b]
            cur_len = _x_char_len(b)
            continue
        if cur:
            cur_len += _x_char_len("\n\n" + b)
            cur.append(b)
        else:
            cur = [b]
            cur_len = _x_char_len(b)
    if cur:
        tweets.append("\n\n".join(cur).strip())

    return tweets


def render_card(
    date: dt.date,
    items: list[dict[str, Any]],
    out_path: Path,
    *,
    lang: str,
    title_override: str | None = None,
    subtitle_override: str | None = None,
    footer_url_override: str | None = None,
    max_per_source: int | None = None,
    max_per_domain: int | None = None,
    max_per_category: int | None = None,
) -> None:
    # Always render near-square cards. The user explicitly prefers a square-ish digest image.
    w, h = 1080, 1080

    # Palette: text sits directly on the colorful background (NO translucent white panel).
    # We make this legible via a dark gradient overlay + subtle stroke/shadow.
    bg = (252, 252, 253)              # used only for fallback background generation
    ink = (255, 255, 255, 255)        # main text
    sub = (226, 232, 240, 255)        # secondary text
    faint = (255, 255, 255, 120)      # divider lines
    accent_rgb = (59, 130, 246)       # blue-500 (works on dark backgrounds)
    accent = (*accent_rgb, 255)

    def _seed() -> int:
        # Stable per-date seed so the card "look" is consistent for a given issue.
        hexd = hashlib.sha256(date.isoformat().encode("utf-8")).hexdigest()[:8]
        return int(hexd, 16)

    def _make_background() -> Image.Image:
        # 1) Allow a user-provided background image (e.g., generated via an API) but soften it.
        bg_img = (os.environ.get("DIGEST_CARD_BG_IMAGE") or "").strip()
        if bg_img and Path(bg_img).exists():
            im = _resize_to_cover(Image.open(bg_img), w, h)
            # Soften and desaturate so text remains dominant.
            im = im.filter(ImageFilter.GaussianBlur(radius=10))
            im = ImageEnhance.Color(im).enhance(0.35)
            im = ImageEnhance.Brightness(im).enhance(1.15)
            im = ImageEnhance.Contrast(im).enhance(0.95)
            return im

        # 1b) If a pinned background exists, always reuse it (avoid regenerating a new background every day).
        # Users can still override via DIGEST_CARD_BG_IMAGE.
        pinned = CARD_DIR / "_bg" / "global-bg.png"
        if pinned.exists():
            im = _resize_to_cover(Image.open(pinned), w, h)
            im = im.filter(ImageFilter.GaussianBlur(radius=10))
            im = ImageEnhance.Color(im).enhance(0.35)
            im = ImageEnhance.Brightness(im).enhance(1.15)
            im = ImageEnhance.Contrast(im).enhance(0.95)
            return im

        # 2) Optionally auto-generate a vivid background using an image-capable Gemini model
        # (then blur it so it behaves like a background, not a poster).
        # Default: if OPENROUTER_API_KEY is available, we generate (unless explicitly disabled).
        do_generate = _as_bool_default(os.environ.get("DIGEST_CARD_BG_GENERATE"), default=bool(_openrouter_key()))
        if do_generate and _openrouter_key():
            bg_cache_dir = CARD_DIR / "_bg"
            bg_cache_dir.mkdir(parents=True, exist_ok=True)
            cached = bg_cache_dir / f"{date.isoformat()}-bg.png"
            if cached.exists():
                im = _resize_to_cover(Image.open(cached), w, h)
            else:
                model = (os.environ.get("DIGEST_CARD_BG_MODEL") or "google/gemini-2.5-flash-image").strip()
                prompt = (os.environ.get("DIGEST_CARD_BG_PROMPT") or "").strip() or (
                    "Create an abstract, colorful background suitable for a professional news digest card. "
                    "Style: modern academic, rich color, soft gradients + subtle bokeh, faint data-network vibe. "
                    "No people, no objects, no logos. Square composition."
                )
                png = _openrouter_generate_image_png(model=model, prompt=prompt)
                cached.write_bytes(png)
                im = _resize_to_cover(Image.open(cached), w, h)

            # Make it behave like a background: blur + slight brighten; keep color but avoid overpowering text.
            im = im.filter(ImageFilter.GaussianBlur(radius=18))
            im = ImageEnhance.Color(im).enhance(0.80)
            im = ImageEnhance.Brightness(im).enhance(1.08)
            im = ImageEnhance.Contrast(im).enhance(0.95)
            return im

        # Default: subtle gradient + pattern.
        base = Image.new("RGB", (w, h), bg)
        # Light blue top → near-white bottom.
        top = Image.new("RGB", (w, h), (244, 247, 255))
        grad = Image.linear_gradient("L").resize((w, h))
        base = Image.composite(top, base, grad)

        style = (os.environ.get("DIGEST_CARD_STYLE") or "grid").strip().lower()
        rnd = random.Random(_seed())

        overlay = Image.new("RGBA", (w, h), (0, 0, 0, 0))
        od = ImageDraw.Draw(overlay)

        if style == "network":
            # Minimal "data network" in the top-right quadrant.
            nodes: list[tuple[int, int]] = []
            for _ in range(16):
                nx = rnd.randint(int(w * 0.52), w - 80)
                ny = rnd.randint(60, int(h * 0.45))
                nodes.append((nx, ny))
            for _ in range(20):
                a = rnd.choice(nodes)
                b = rnd.choice(nodes)
                if a == b:
                    continue
                od.line([a, b], fill=(*accent_rgb, 14), width=2)
            for (nx, ny) in nodes:
                od.ellipse((nx - 3, ny - 3, nx + 3, ny + 3), fill=(*accent_rgb, 30))
        else:
            # Subtle grid + dots (default).
            step = 60
            for gx in range(0, w + 1, step):
                od.line((gx, 0, gx, h), fill=(*accent_rgb, 8), width=2)
            for gy in range(0, h + 1, step):
                od.line((0, gy, w, gy), fill=(*accent_rgb, 6), width=2)
            for gx in range(step, w, step * 2):
                for gy in range(step, h, step * 2):
                    od.ellipse((gx - 2, gy - 2, gx + 2, gy + 2), fill=(*accent_rgb, 14))

        return Image.alpha_composite(base.convert("RGBA"), overlay).convert("RGB")

    def _apply_dark_overlay(im: Image.Image) -> Image.Image:
        """
        Darken the background (subtle gradient + vignette) so text can be placed directly on it.
        This replaces the old translucent white content panel.
        """
        rgba = im.convert("RGBA")

        # Vertical gradient: slightly dark at top, darker at bottom.
        g = Image.linear_gradient("L").resize((1, h)).resize((w, h))
        alpha = g.point(lambda p: int(55 + (p / 255.0) * 155))  # 55..210
        overlay = Image.new("RGBA", (w, h), (0, 0, 0, 0))
        overlay.putalpha(alpha)
        rgba = Image.alpha_composite(rgba, overlay)

        # Subtle vignette to keep focus near center.
        vignette = Image.new("L", (w, h), 0)
        vd = ImageDraw.Draw(vignette)
        steps = 8
        for i in range(steps):
            inset = int((i / (steps - 1)) * 120)
            a = int((i / (steps - 1)) * 75)  # edge darkening strength
            vd.rectangle((inset, inset, w - inset, h - inset), outline=a, width=34)
        v_rgba = Image.new("RGBA", (w, h), (0, 0, 0, 0))
        v_rgba.putalpha(vignette.filter(ImageFilter.GaussianBlur(radius=18)))
        rgba = Image.alpha_composite(rgba, v_rgba)
        return rgba

    def _draw_text(
        draw: ImageDraw.ImageDraw,
        xy: tuple[int, int],
        text: str,
        *,
        font: ImageFont.FreeTypeFont,
        fill: tuple[int, int, int, int],
        stroke_width: int = 2,
        stroke_fill: tuple[int, int, int, int] = (0, 0, 0, 210),
        shadow: bool = True,
    ) -> None:
        x0, y0 = xy
        if shadow:
            draw.text((x0 + 2, y0 + 2), text, fill=(0, 0, 0, 150), font=font)
        draw.text(
            (x0, y0),
            text,
            fill=fill,
            font=font,
            stroke_width=stroke_width,
            stroke_fill=stroke_fill,
        )

    img = _apply_dark_overlay(_make_background())
    draw = ImageDraw.Draw(img)

    fonts = _font_paths(lang)
    font_title = ImageFont.truetype(str(fonts.get("bold", fonts.get("regular"))), 54) if fonts else ImageFont.load_default()
    font_h2 = ImageFont.truetype(str(fonts.get("bold", fonts.get("regular"))), 28) if fonts else ImageFont.load_default()
    font_small = ImageFont.truetype(str(fonts.get("regular", fonts.get("bold"))), 21) if fonts else ImageFont.load_default()
    font_item_title = ImageFont.truetype(str(fonts.get("bold", fonts.get("regular"))), 28) if fonts else ImageFont.load_default()
    font_item_body = ImageFont.truetype(str(fonts.get("regular", fonts.get("bold"))), 22) if fonts else ImageFont.load_default()
    font_mono = (
        ImageFont.truetype(str(fonts["mono"]), 20)
        if fonts and "mono" in fonts
        else font_small
    )

    pad = 56
    x = pad
    y = 52

    # Header accent mark (brand cue).
    draw.rounded_rectangle((x - 10, y + 10, x - 4, y + 44), radius=4, fill=accent)

    title = title_override or ("Daily Digest" if lang != "zh" else "每日简报")
    _draw_text(draw, (x, y), title, font=font_title, fill=ink, stroke_width=2)
    y += 54

    date_str = date.isoformat()
    subtitle = subtitle_override or (
        "Top stories + sources (see full list on site)." if lang != "zh" else "今日要点与来源（完整版见网站）。"
    )
    _draw_text(draw, (x, y), date_str + "  ·  " + subtitle, font=font_small, fill=sub, stroke_width=1)
    y += 34

    # Top rule
    draw.line((x, y, w - pad, y), fill=faint, width=2)
    y += 20

    # Keep cards low-cognitive-load: show only top N items (titles + source), no long blurbs.
    default_max = 10
    max_items_total = int((os.environ.get("DIGEST_CARD_MAX_ITEMS") or "").strip() or default_max)
    now = _score_now_for_date(date)
    ranked_all = sorted(
        items,
        key=lambda it: (
            _importance_score(it, now=now),
            _parse_published_utc(it) or dt.datetime.min.replace(tzinfo=dt.timezone.utc),
        ),
        reverse=True,
    )

    # Match the website "Top 10" logic: diversify sources/domains so the card isn't all from one outlet.
    mps = max_per_source if max_per_source is not None else int(os.environ.get("DIGEST_TOP_MAX_PER_SOURCE", "3") or "3")
    mpd = max_per_domain if max_per_domain is not None else int(os.environ.get("DIGEST_TOP_MAX_PER_DOMAIN", "1") or "1")
    mpc = max_per_category if max_per_category is not None else int(os.environ.get("DIGEST_TOP_MAX_PER_CATEGORY", "5") or "5")
    ranked = _select_top_diverse(
        ranked_all,
        n=max_items_total,
        max_per_source=mps,
        max_per_domain=mpd,
        max_per_category=mpc,
    )

    # Footer anchored at the bottom to avoid a large empty block below the content.
    footer1 = "Zhilong George Zhao · @longlalaland"
    digest_path = f"/digest/{date.isoformat()}/" if lang != "zh" else f"/zh/digest/{date.isoformat()}/"
    footer2 = footer_url_override or (DEFAULT_SITE_URL + digest_path)
    bottom_pad = 52
    fy = h - bottom_pad - 52
    footer_top_y = fy - 28  # keep content above divider + footer lines

    for idx, it in enumerate(ranked, start=1):
        t = _pick_lang(it, "title", "title_zh", lang)
        if not t:
            continue
        src = (it.get("source") or "").strip()
        cat_raw = (it.get("category") or "").strip()
        cat = (CAT_ZH.get(cat_raw, cat_raw) if lang == "zh" else cat_raw)

        num = f"{idx}."
        _draw_text(draw, (x, y + 1), num, font=font_h2, fill=sub, stroke_width=1)
        tx = x + 46
        t_lines = _wrap_ellipsize(draw, t, font_item_title, max_width=w - pad - tx, max_lines=2)
        for ln in t_lines:
            _draw_text(draw, (tx, y), ln, font=font_item_title, fill=ink, stroke_width=2)
            y += 34

        meta = " · ".join([m for m in [cat, src] if m]) if (cat or src) else ""
        if meta:
            _draw_text(draw, (tx, y - 4), meta, font=font_small, fill=sub, stroke_width=1)
            y += 28

        y += 10
        if y > footer_top_y:
            break

    # Footer (no panel). URL in mono reduces OCR/link detection issues.
    draw.line((x, fy - 14, w - pad, fy - 14), fill=faint, width=2)
    _draw_text(draw, (x, fy - 2), footer1, font=font_small, fill=sub, stroke_width=1)
    _draw_text(draw, (x, fy + 20), footer2, font=font_mono, fill=sub, stroke_width=1)

    out_path.parent.mkdir(parents=True, exist_ok=True)
    img.convert("RGB").save(out_path, format="PNG", optimize=True, compress_level=9)
    _optimize_png(out_path)
    _write_webp_from_png(out_path)


def _md_for_lang(date: dt.date, digest: dict[str, Any], *, lang: str) -> Path:
    items = digest.get("items", [])
    if not isinstance(items, list):
        _die("digest.items must be a list")
    if not items:
        _die("digest has no items; skip build or add items first")

    # Markdown page (collection item)
    out_dir = ZH_DIGESTS_DIR if lang == "zh" else DIGESTS_DIR
    out_dir.mkdir(parents=True, exist_ok=True)
    md_path = out_dir / f"{date.isoformat()}.md"

    title = digest.get("title") or f"Daily Digest - {date.isoformat()}"
    if lang == "zh":
        title = digest.get("title_zh") or f"每日简报 - {date.isoformat()}"

    # Cards for this issue (rendered during build). We now create 4 images per day:
    #  - main news (EN + ZH)
    #  - AI tools/model releases (EN + ZH)
    ver = _site_asset_version()
    card_news_rel = f"/assets/img/digests/{date.isoformat()}-{'zh' if lang == 'zh' else 'en'}.png"
    card_news_webp_rel = card_news_rel[:-4] + ".webp"
    card_ai_rel = f"/assets/img/digests/{date.isoformat()}-ai-{'zh' if lang == 'zh' else 'en'}.png"
    card_ai_webp_rel = card_ai_rel[:-4] + ".webp"
    if ver:
        card_news_rel += f"?v={ver}"
        card_news_webp_rel += f"?v={ver}"
        card_ai_rel += f"?v={ver}"
        card_ai_webp_rel += f"?v={ver}"

    # Sort by importance (global list, not grouped).
    now = _score_now_for_date(date)
    ranked = sorted(
        items,
        key=lambda it: (
            _importance_score(it, now=now),
            _parse_published_utc(it) or dt.datetime.min.replace(tzinfo=dt.timezone.utc),
        ),
        reverse=True,
    )
    # Split content: main news vs AI tools/model releases, with separate caps.
    ai_cat = "AI tools & model releases"
    max_news = int(digest.get("max_items", 0) or DEFAULT_MAX_ITEMS_ON_SITE)
    max_ai = int(digest.get("max_items_ai", 0) or DEFAULT_MAX_AI_ITEMS_ON_SITE)

    if lang == "zh":
        # For zh pages, ensure visible text is Chinese (don't fall back to EN).
        ranked_zh: list[dict[str, Any]] = []
        for it in ranked:
            t_zh = (it.get("title_zh") or "").strip()
            if not t_zh or not _has_cjk(t_zh):
                continue
            ranked_zh.append(it)
        items_news = [it for it in ranked_zh if (it.get("category") or "").strip() != ai_cat][:max_news]
        items_ai = [it for it in ranked_zh if (it.get("category") or "").strip() == ai_cat][:max_ai]
    else:
        items_news = [it for it in ranked if (it.get("category") or "").strip() != ai_cat][:max_news]
        items_ai = [it for it in ranked if (it.get("category") or "").strip() == ai_cat][:max_ai]

    top_news = _select_top_diverse(
        items_news,
        n=10,
        max_per_source=int(os.environ.get("DIGEST_TOP_MAX_PER_SOURCE", "3") or "3"),
        max_per_domain=int(os.environ.get("DIGEST_TOP_MAX_PER_DOMAIN", "1") or "1"),
        max_per_category=int(os.environ.get("DIGEST_TOP_MAX_PER_CATEGORY", "5") or "5"),
    )

    ai_top_pool = _ai_item_practicality_filter(items_ai)
    top_ai = _select_top_diverse(
        ai_top_pool,
        # Prefer diversity over padding: if we only have a few distinct sources today,
        # show fewer items rather than repeating the same repo/tool many times.
        n=min(10, len({(it.get("source") or "").strip() for it in ai_top_pool if (it.get("source") or "").strip()})),
        # For AI tools, most high-signal items come from GitHub Releases (same domain),
        # so keep a looser domain cap but tighter per-source cap to avoid 1 repo dominating.
        max_per_source=int(os.environ.get("DIGEST_AI_TOP_MAX_PER_SOURCE", "1") or "1"),
        max_per_domain=int(os.environ.get("DIGEST_AI_TOP_MAX_PER_DOMAIN", "20") or "20"),
        max_per_category=int(os.environ.get("DIGEST_AI_TOP_MAX_PER_CATEGORY", "10") or "10"),
    )

    note = (digest.get("note_zh") if lang == "zh" else digest.get("note")) or ""
    note = note.strip()

    lines: list[str] = [
        "---",
        "layout: page",
        f'title: "{title}"',
        f"lang: {lang}",
        f"permalink: /{'zh/' if lang == 'zh' else ''}digest/{date.isoformat()}/",
        f"digest_date: {date.isoformat()}",
        f"translation_key: digest-{date.isoformat()}",
        "page_class: digest-page",
        "---",
        "",
    ]

    # Hero cards (two on-site: (1) main news, (2) AI tools/model updates).
    hero_news = "News" if lang != "zh" else "新闻"
    hero_ai = "AI tools" if lang != "zh" else "AI 工具"
    hero_sections_label = "Sections" if lang != "zh" else "目录"
    lines.extend(
        [
            '<div class="digest-hero">',
            '  <div class="digest-hero-grid">',
            '    <figure class="digest-hero-card">',
            f'      <a href="{card_news_rel}"><picture><source type="image/webp" srcset="{card_news_webp_rel}"><img src="{card_news_rel}" alt="{html.escape(title)} — {hero_news}" loading="lazy"></picture></a>',
            f'      <figcaption>{hero_news}</figcaption>',
            "    </figure>",
            '    <figure class="digest-hero-card">',
            f'      <a href="{card_ai_rel}"><picture><source type="image/webp" srcset="{card_ai_webp_rel}"><img src="{card_ai_rel}" alt="{html.escape(title)} — {hero_ai}" loading="lazy"></picture></a>',
            f'      <figcaption>{hero_ai}</figcaption>',
            "    </figure>",
            "  </div>",
            '  <details class="digest-sections" open>',
            f"    <summary>{hero_sections_label}</summary>",
            '    <div class="digest-hero-nav">',
            f'      <a href="#top-news">{("Top 10 — News" if lang != "zh" else "最重要的 10 条（新闻）")}</a>',
            f'      <a href="#top-ai">{("AI tools & model updates" if lang != "zh" else "AI 工具 / 模型更新")}</a>',
            f'      <a href="#full-news">{("Full list — News" if lang != "zh" else "全部列表（新闻）")}</a>',
            f'      <a href="#full-ai">{("Full list — AI tools" if lang != "zh" else "全部列表（AI 工具 / 模型更新）")}</a>',
            "    </div>",
            "  </details>",
            '  <nav class="digest-float-toc" aria-label="Digest sections">',
            f'    <a href="#top-news">{("Top News" if lang != "zh" else "Top 新闻")}</a>',
            f'    <a href="#top-ai">{("Top AI" if lang != "zh" else "Top AI")}</a>',
            f'    <a href="#full-news">{("All News" if lang != "zh" else "全部新闻")}</a>',
            f'    <a href="#full-ai">{("All AI" if lang != "zh" else "全部 AI")}</a>',
            "  </nav>",
            "</div>",
            "",
        ]
    )
    if note:
        # Keep this one short; it shows at the top of the page.
        label = "Today in one line" if lang != "zh" else "今日一句话"
        lines.extend(
            [
                f'<div class="digest-highlight"><div class="digest-highlight__label">{label}</div><div class="digest-highlight__text">{html.escape(note)}</div></div>',
                "",
            ]
        )

    def _cat_label(v: str) -> str:
        v = (v or "Misc").strip() or "Misc"
        return v if lang != "zh" else CAT_ZH.get(v, v)

    def _short(it: dict[str, Any]) -> str:
        if lang == "zh":
            why = (it.get("why_zh") or "").strip()
            if why and _has_cjk(why):
                return why
            take = (it.get("takeaway_zh") or "").strip()
            return _first_sentence(take, 120) if take and _has_cjk(take) else ""
        why = _pick_lang(it, "why", "why_zh", lang)
        if why:
            return why
        take = _pick_lang(it, "takeaway", "takeaway_zh", lang)
        return _first_sentence(take, 160 if lang != "zh" else 120)

    def _pub(it: dict[str, Any]) -> str:
        when = _parse_published_utc(it)
        if not when:
            return ""
        return when.strftime("%Y-%m-%d %H:%M UTC")

    def _render_top_section(title_md: str, top_items: list[dict[str, Any]]) -> None:
        lines.extend([title_md, ""])
        if not top_items:
            lines.append("- (No items in this window.)" if lang != "zh" else "-（本窗口暂无条目。）")
            lines.append("")
            return
        for idx, it in enumerate(top_items, start=1):
            t = (it.get("title_zh") or "").strip() if lang == "zh" else _pick_lang(it, "title", "title_zh", lang)
            u = (it.get("url") or "").strip()
            if not t or not u:
                continue
            meta = " | ".join(
                x
                for x in [
                    _cat_label(it.get("category", "")),
                    (it.get("source") or "").strip(),
                    _pub(it),
                ]
                if x
            )
            # Keep the list scannable: title line + a muted meta line.
            lines.append(f"{idx}. [{t}]({u})")
            if meta:
                lines.append(f'   <div class="digest-item-meta">{html.escape(meta)}</div>')
            blurb = _short(it)
            if blurb:
                lines.append(f'   <div class="digest-item-why">{html.escape(blurb)}</div>')
            lines.append("")

    def _render_full_list(title_md: str, table_items: list[dict[str, Any]], *, section_id: str) -> None:
        # Wrap the full list in a collapsible block to keep the page scannable.
        n_items = len(
            [
                it
                for it in table_items
                if (it.get("title_zh") if lang == "zh" else it.get("title")) and it.get("url")
            ]
        )
        summary = f"{title_md.lstrip('#').strip()} ({n_items})" if lang != "zh" else f"{title_md.lstrip('#').strip()}（{n_items} 条）"
        search_ph = "Search in this list…" if lang != "zh" else "在本列表中搜索…"
        sort_label = "Sort" if lang != "zh" else "排序"
        sort_time = "Newest first" if lang != "zh" else "最新优先"
        sort_src = "Source" if lang != "zh" else "来源"
        sort_cat = "Category" if lang != "zh" else "类别"
        cat_label = "Category" if lang != "zh" else "类别"
        src_label = "Source" if lang != "zh" else "来源"

        lines.extend(
            [
                f'<details class="digest-details" id="{section_id}" data-digest-full>',
                f"<summary>{summary}</summary>",
                "",
                '<div class="digest-full-controls">',
                f'  <input class="form-control form-control-sm digest-full-search" type="search" placeholder="{search_ph}" data-digest-full-search>',
                f'  <select class="custom-select custom-select-sm digest-full-select" data-digest-full-sort aria-label="{sort_label}">',
                f'    <option value="time">{sort_time}</option>',
                f'    <option value="source">{sort_src}</option>',
                f'    <option value="category">{sort_cat}</option>',
                "  </select>",
                f'  <select class="custom-select custom-select-sm digest-full-select" data-digest-full-filter=\"category\" aria-label=\"{cat_label}\"></select>',
                f'  <select class="custom-select custom-select-sm digest-full-select" data-digest-full-filter=\"source\" aria-label=\"{src_label}\"></select>',
                "</div>",
                "",
                '<div class="digest-full-list" data-digest-full-list>',
            ]
        )

        for idx, it in enumerate(table_items, start=1):
            t = (it.get("title_zh") or "").strip() if lang == "zh" else _pick_lang(it, "title", "title_zh", lang)
            u = (it.get("url") or "").strip()
            if not t or not u:
                continue
            cat = _cat_label(it.get("category", ""))
            src = (it.get("source") or "").strip()
            pub = _pub(it)
            blurb = _short(it)

            # Data attrs power front-end filtering/sorting.
            data_title = html.escape(t)
            data_cat = html.escape(cat)
            data_src = html.escape(src)
            data_pub = html.escape(pub)
            data_url = html.escape(u)

            lines.extend(
                [
                    f'<div class="digest-full-item" data-title="{data_title}" data-category="{data_cat}" data-source="{data_src}" data-published="{data_pub}" data-url="{data_url}">',
                    '  <div class="digest-full-item__head">',
                    f'    <div class="digest-full-item__idx">{idx}</div>',
                    f'    <div class="digest-full-item__title"><a href="{u}" target="_blank" rel="noopener noreferrer">{html.escape(t)}</a></div>',
                    '    <button type="button" class="btn btn-sm btn-outline-secondary digest-copy-row" data-copy-url="' + data_url + '" data-copy-title="' + data_title + '">' + ("Copy" if lang != "zh" else "复制") + "</button>",
                    "  </div>",
                    f'  <div class="digest-full-item__meta">{html.escape(cat)} · {html.escape(src)}' + (f" · {html.escape(pub)}" if pub else "") + "</div>",
                ]
            )
            if blurb:
                lines.append(f'  <div class="digest-full-item__blurb">{html.escape(blurb)}</div>')
            lines.append("</div>")

        lines.extend(["</div>", "</details>", ""])

    # Top lists (split into two parts per user preference).
    _render_top_section(
        ("## Top 10 — News {#top-news}" if lang != "zh" else "## 最重要的 10 条（新闻） {#top-news}"),
        top_news,
    )
    _render_top_section(
        ("## AI tools & model updates {#top-ai}" if lang != "zh" else "## AI 工具 / 模型更新 {#top-ai}"),
        top_ai,
    )

    # Full lists (split).
    _render_full_list(
        "## Full list — News" if lang != "zh" else "## 全部列表（新闻）",
        items_news,
        section_id="full-news",
    )
    _render_full_list(
        "## Full list — AI tools & model updates" if lang != "zh" else "## 全部列表（AI 工具 / 模型更新）",
        items_ai,
        section_id="full-ai",
    )

    md_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
    return md_path


def build_digest(date: dt.date, digest: dict[str, Any]) -> list[tuple[str, Path]]:
    out: list[tuple[str, Path]] = []
    out.append(("en", _md_for_lang(date, digest, lang="en")))
    out.append(("zh", _md_for_lang(date, digest, lang="zh")))

    # Social cards (4 per day):
    #  - main news (EN + ZH)
    #  - AI tools/model releases (EN + ZH)
    items = digest.get("items", [])
    if isinstance(items, list) and items:
        def _looks_ai_related(it: dict[str, Any]) -> bool:
            # Fall back when the curated list has no dedicated "AI tools & model releases" items.
            # This keeps the two AI cards from rendering as empty backgrounds.
            text = " ".join(
                [
                    (it.get("title") or ""),
                    (it.get("why") or ""),
                    (it.get("takeaway") or ""),
                    (it.get("source") or ""),
                ]
            ).lower()
            keys = [
                " ai ",
                "ai-",
                "openai",
                "chatgpt",
                "llm",
                "model",
                "claude",
                "codex",
                "agentic",
                "mcp",
                "nvidia",
                "gpu",
                "chips",
                "vector",
            ]
            return any(k in text for k in keys)

        ai_cat = "AI tools & model releases"
        items_news = [it for it in items if (it.get("category") or "").strip() != ai_cat]
        items_ai = [it for it in items if (it.get("category") or "").strip() == ai_cat]

        # For AI cards, prefer a short diversified set (avoid 10 near-identical release tags).
        now = _score_now_for_date(date)
        ai_ranked = sorted(
            _ai_item_practicality_filter(items_ai),
            key=lambda it: (
                _importance_score(it, now=now),
                _parse_published_utc(it) or dt.datetime.min.replace(tzinfo=dt.timezone.utc),
            ),
            reverse=True,
        )
        ai_unique_sources = {(it.get("source") or "").strip() for it in ai_ranked if (it.get("source") or "").strip()}
        ai_n = min(10, len(ai_unique_sources))
        if ai_n > 0:
            ai_ranked = _select_top_diverse(
                ai_ranked,
                n=ai_n,
                max_per_source=int(os.environ.get("DIGEST_AI_CARD_MAX_PER_SOURCE", "1") or "1"),
                max_per_domain=int(os.environ.get("DIGEST_AI_CARD_MAX_PER_DOMAIN", "20") or "20"),
                max_per_category=int(os.environ.get("DIGEST_AI_CARD_MAX_PER_CATEGORY", "10") or "10"),
            )
        else:
            ai_ranked = []

        render_card(date, items_news, CARD_DIR / f"{date.isoformat()}-en.png", lang="en")
        render_card(date, items_news, CARD_DIR / f"{date.isoformat()}-zh.png", lang="zh")

        # If there are no dedicated AI tool/model items in the curated list, fall back to
        # AI-related highlights from the main items so the AI cards aren't blank.
        if not ai_ranked:
            ai_ranked = [it for it in items_news if _looks_ai_related(it)]

        render_card(
            date,
            ai_ranked,
            CARD_DIR / f"{date.isoformat()}-ai-en.png",
            lang="en",
            title_override="AI Tools & Model Updates",
            subtitle_override=(
                "New tools/models/releases (see site for the full list)."
                if any((it.get("category") or "").strip() == ai_cat for it in items)
                else "AI highlights (no dedicated tool/model releases in today's top list)."
            ),
            max_per_source=int(os.environ.get("DIGEST_AI_CARD_MAX_PER_SOURCE", "1") or "1"),
            max_per_domain=int(os.environ.get("DIGEST_AI_CARD_MAX_PER_DOMAIN", "20") or "20"),
            max_per_category=int(os.environ.get("DIGEST_AI_CARD_MAX_PER_CATEGORY", "10") or "10"),
        )
        render_card(
            date,
            ai_ranked,
            CARD_DIR / f"{date.isoformat()}-ai-zh.png",
            lang="zh",
            title_override="AI 工具 / 模型更新",
            subtitle_override=(
                "今日工具/模型发布（完整版见网站）。"
                if any((it.get("category") or "").strip() == ai_cat for it in items)
                else "AI 要闻（今日未选入工具/模型发布条目；详见网站）。"
            ),
            max_per_source=int(os.environ.get("DIGEST_AI_CARD_MAX_PER_SOURCE", "1") or "1"),
            max_per_domain=int(os.environ.get("DIGEST_AI_CARD_MAX_PER_DOMAIN", "20") or "20"),
            max_per_category=int(os.environ.get("DIGEST_AI_CARD_MAX_PER_CATEGORY", "10") or "10"),
        )

    return out


def cmd_draft(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    sources = _load_sources()
    # Since: last N hours from now (UTC). This keeps drafts useful for daily runs.
    since_utc = dt.datetime.now(dt.timezone.utc) - dt.timedelta(hours=args.hours)
    items = _draft_items(sources, since_utc=since_utc, max_items=args.max_items)

    out = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if out.exists() and not args.force:
        out = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"

    payload = {
        "date": date.isoformat(),
        "title": f"Daily Digest - {date.isoformat()}",
        "title_zh": f"每日简报 - {date.isoformat()}",
        "note": "",
        "note_zh": "",
        "items": items,
    }
    _save_yaml(out, payload)
    print(f"Wrote {out.relative_to(REPO_ROOT)} ({len(items)} items).")
    print("Next: edit `why` (short), `takeaway` (1–2 sentences), `prompt` (1 question), then run:")
    print(f"  {REPO_ROOT}/bin/digest build --date {date.isoformat()}")


def cmd_build(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    digest_path = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if not digest_path.exists():
        # If only draft exists, use it.
        draft = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"
        if draft.exists():
            digest_path = draft
        else:
            _die(f"missing digest YAML for {date.isoformat()} (expected {digest_path})")
    digest = _load_digest_yaml(digest_path)
    # Auto-fill missing zh fields so the Chinese site is fully Chinese by default,
    # then auto-generate a short one-line highlight if missing.
    changed = False
    now = _score_now_for_date(date)
    if _autofill_digest_zh_fields(digest, now=now):
        changed = True
    if _autofill_digest_notes(digest, now=now):
        changed = True
    if changed:
        _save_yaml(digest_path, digest)
    results = build_digest(date, digest)
    for lang, md_path in results:
        print(f"Wrote {md_path.relative_to(REPO_ROOT)} ({lang})")
    # Maintain weekly aggregates for the weekly digest view.
    try:
        _update_weekly_data(date, digest)
    except Exception as e:
        print(f"warn: weekly aggregate update failed ({e})", file=sys.stderr)


def cmd_social(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    digest_path = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if not digest_path.exists():
        draft = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"
        if draft.exists():
            digest_path = draft
        else:
            _die(f"missing digest YAML for {date.isoformat()} (expected {digest_path})")

    digest = _load_digest_yaml(digest_path)
    # Social requirement: each post should include BOTH EN + ZH cards
    # for both sections: news + AI tools/model updates.
    card_news_en = CARD_DIR / f"{date.isoformat()}-en.png"
    card_news_zh = CARD_DIR / f"{date.isoformat()}-zh.png"
    card_ai_en = CARD_DIR / f"{date.isoformat()}-ai-en.png"
    card_ai_zh = CARD_DIR / f"{date.isoformat()}-ai-zh.png"
    print("==== Attach images (4 cards) ====")
    print(f"News EN: {card_news_en}")
    print(f"News ZH: {card_news_zh}")
    print(f"AI EN:   {card_ai_en}")
    print(f"AI ZH:   {card_ai_zh}")
    if not (card_news_en.exists() and card_news_zh.exists() and card_ai_en.exists() and card_ai_zh.exists()):
        print("(Tip) Run `./bin/digest build --date YYYY-MM-DD` first to generate the cards.")
    print("")

    if args.platform in ("linkedin", "both"):
        print("==== LinkedIn (copy/paste) ====")
        print(
            _format_linkedin_post(
                date,
                digest,
                site_url=args.site_url,
                text_mode=args.text,
                style=args.style,
            )
        )

    if args.platform in ("x", "both"):
        print("==== X thread (copy/paste) ====")
        tweets = _format_x_thread(
            date,
            digest,
            site_url=args.site_url,
            text_mode=args.text,
            style=args.style,
        )
        total = len(tweets)
        for i, t in enumerate(tweets, start=1):
            print(f"\n--- Tweet {i}/{total} ---\n{t}\n")


def main(argv: list[str]) -> int:
    parser = argparse.ArgumentParser(prog="digest")
    sub = parser.add_subparsers(dest="cmd", required=True)

    p_draft = sub.add_parser("draft", help="Draft a digest YAML from RSS/Atom sources")
    p_draft.add_argument("--date", required=True, help="YYYY-MM-DD (label for this issue)")
    p_draft.add_argument("--hours", type=int, default=36, help="Look back window (UTC), default: 36")
    p_draft.add_argument("--max-items", type=int, default=80, help="Max candidate items to write, default: 80")
    p_draft.add_argument("--force", action="store_true", help="Overwrite existing YYYY-MM-DD.yml")
    p_draft.set_defaults(func=cmd_draft)

    p_build = sub.add_parser("build", help="Build digest page from YAML")
    p_build.add_argument("--date", required=True, help="YYYY-MM-DD")
    p_build.set_defaults(func=cmd_build)

    p_social = sub.add_parser("social", help="Generate social post text (LinkedIn + X)")
    p_social.add_argument("--date", required=True, help="YYYY-MM-DD")
    p_social.add_argument(
        "--platform",
        choices=["both", "linkedin", "x"],
        default="both",
        help="Which post text to emit, default: both",
    )
    p_social.add_argument(
        "--text",
        choices=["en", "bilingual"],
        default="bilingual",
        help="Text mode for posts; note: cards are always EN+ZH. Default: bilingual",
    )
    p_social.add_argument(
        "--style",
        choices=["bulletin", "theme"],
        default="bulletin",
        help="Post layout: bulletin (default) or theme-first (theme + links).",
    )
    p_social.add_argument(
        "--site-url",
        default=DEFAULT_SITE_URL,
        help=f"Canonical site URL for digest links, default: {DEFAULT_SITE_URL}",
    )
    p_social.set_defaults(func=cmd_social)

    args = parser.parse_args(argv)
    args.func(args)
    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
