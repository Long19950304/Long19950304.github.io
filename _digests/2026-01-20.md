---
layout: page
title: "Daily Digest — 2026-01-20"
lang: en
permalink: /digest/2026-01-20/
digest_date: 2026-01-20
card_image: /assets/img/digests/2026-01-20-en.png
translation_key: digest-2026-01-20
---

![Daily digest card](/assets/img/digests/2026-01-20-en.png)

## Links

### CSS / AI & society

- [AI Sycophancy: How Users Flag and Respond](https://arxiv.org/abs/2601.10467v1) - Empirical signals for detecting/handling overly agreeable LLM behavior.
  - Take-home: Studies how users notice and react to sycophantic assistant behavior; offers observable signals and implications for trust and safety evaluation.
  - Discuss: How should assistants balance politeness vs disagreement without drifting into sycophancy?

### AI frontier

- [Differential Transformer V2](https://huggingface.co/blog/microsoft/diff-attn-v2) - A new attention variant aimed at more efficient long-context modeling.
  - Take-home: Proposes a differential attention mechanism (Diff Attn v2) to improve long-context efficiency; a useful reference point when benchmarking attention variants against standard Transformers.
  - Discuss: What long-context tasks would you use to evaluate an attention variant beyond perplexity?
- [OpenAI partners with Cerebras](https://openai.com/index/cerebras-partnership) - A notable infra/inference partnership; watch for speed/cost implications.
  - Take-home: Signals continued investment in specialized inference hardware partnerships; this can reshape latency/cost trade-offs and who gets access to fast inference at scale.
  - Discuss: Do you expect providers to diversify inference stacks (GPU/ASIC), and how should researchers report hardware?
- [Open Responses: What you need to know](https://huggingface.co/blog/open-responses) - A practical multi-model workflow for comparing and integrating LLM outputs.
  - Take-home: Presents an 'open responses' pattern for comparing multiple models and synthesizing outputs; useful for building robust pipelines with diversity + verification.
  - Discuss: Which parts of your workflow benefit most from multi-model redundancy vs one strong model?

### Education / learning sciences / edtech

- [The Conversational Exam: A Scalable Assessment Design for the AI Era](https://arxiv.org/abs/2601.10691v1) - An assessment format designed to stay meaningful when students have AI.
  - Take-home: Proposes conversational exams that evaluate reasoning through interaction rather than final answers, aiming to stay meaningful when students have access to AI tools.
  - Discuss: What guardrails would make conversational assessment fair, scalable, and less subjective?
- [Evaluating 21st-Century Competencies in Postsecondary Curricula with Large Language Models: Performance Benchmarking and Reasoning-Based Prompting Strategies](https://arxiv.org/abs/2601.10983v1) - Benchmarks LLMs for curricular competency evaluation and prompting strategies.
  - Take-home: Benchmarks LLMs for evaluating 21st-century competencies in curricula and tests reasoning-based prompting; suggests how automated curriculum auditing can be made more reliable.
  - Discuss: Would you trust LLM-based competency evaluation without human calibration—what evidence would you need?
- [aiPlato: A Novel AI Tutoring and Step-wise Feedback System for Physics Homework](https://arxiv.org/abs/2601.09965v1) - A tutoring design emphasizing step-wise feedback (closer to how students learn).
  - Take-home: An AI tutoring design that focuses on step-wise feedback rather than giving full solutions; a concrete pattern for scaffolding student learning.
  - Discuss: How do we design feedback that helps without encouraging over-reliance on the tutor?
- [Take Out Your Calculators: Estimating the Real Difficulty of Question Items with LLM Student Simulations](https://arxiv.org/abs/2601.09953v1) - Uses simulated "LLM students" to estimate item difficulty for assessment design.
  - Take-home: Uses simulated 'LLM students' to estimate item difficulty, offering a method idea for item analysis when real response data is limited.
  - Discuss: Where could LLM-student simulations mislead difficulty estimates, and how would you validate them?
