#!/usr/bin/env python3
"""
Daily digest generator for the personal site.

Workflow:
  1) Draft candidates from RSS/Atom feeds:
       personal-site/bin/digest draft --date 2026-01-20
  2) Edit the generated YAML (fill `why` fields, delete low-signal items).
  3) Build the digest page:
       personal-site/bin/digest build --date 2026-01-20
  4) (Optional) Generate social post text (X thread + LinkedIn copy):
       personal-site/bin/digest social --date 2026-01-20
"""

from __future__ import annotations

import argparse
import datetime as dt
import base64
import hashlib
import json
import os
import random
import re
import sys
import html
import socket
import shutil
import subprocess
import urllib.request
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable

import feedparser
import yaml
from PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter


REPO_ROOT = Path(__file__).resolve().parents[1]
DATA_SOURCES = REPO_ROOT / "_data" / "digest_sources.yml"
DATA_DIGESTS_DIR = REPO_ROOT / "_data" / "digests"
DIGESTS_DIR = REPO_ROOT / "_digests"
ZH_DIGESTS_DIR = REPO_ROOT / "_digests_zh"
CARD_DIR = REPO_ROOT / "assets" / "img" / "digests"
DEFAULT_SITE_URL = "https://long19950304.github.io"
FETCH_TIMEOUT_SECONDS = 8
FETCH_WORKERS = 8
DEFAULT_MAX_ITEMS_ON_SITE = 100
# Default to a model that is broadly available from more regions.
DEFAULT_TRANSLATE_MODEL = "deepseek/deepseek-chat"
DEFAULT_TRANSLATE_FALLBACK_MODELS = [
    "qwen/qwen-2.5-7b-instruct",
    "mistralai/mistral-7b-instruct",
]

# feedparser uses urllib under the hood and may otherwise block indefinitely on slow/blocked feeds.
socket.setdefaulttimeout(FETCH_TIMEOUT_SECONDS)


def _die(msg: str, code: int = 2) -> None:
    print(f"error: {msg}", file=sys.stderr)
    raise SystemExit(code)


def _ensure_dirs() -> None:
    DATA_DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    ZH_DIGESTS_DIR.mkdir(parents=True, exist_ok=True)
    CARD_DIR.mkdir(parents=True, exist_ok=True)

def _optimize_png(path: Path) -> None:
    """
    Make generated cards more web-friendly.

    - Keep it optional: if pngquant isn't installed, do nothing.
    - If optimization isn't smaller, keep the original (avoid quality regressions).
    - Write back in-place to preserve file owner/group on macOS volumes.
    """
    exe = shutil.which("pngquant")
    if not exe or not path.exists():
        return

    tmp = path.with_suffix(path.suffix + ".tmp")
    try:
        orig_size = path.stat().st_size
        subprocess.run(
            [exe, "--force", "--strip", "--speed", "1", "--quality", "80-95", "-o", str(tmp), str(path)],
            check=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        if tmp.exists() and tmp.stat().st_size < orig_size:
            path.write_bytes(tmp.read_bytes())
    except Exception as e:
        print(f"warn: pngquant failed for {path.name}: {e}", file=sys.stderr)
    finally:
        try:
            tmp.unlink()
        except FileNotFoundError:
            pass


def _parse_date(s: str) -> dt.date:
    try:
        return dt.date.fromisoformat(s)
    except ValueError:
        _die(f"invalid --date '{s}', expected YYYY-MM-DD")


def _load_sources() -> list[dict[str, Any]]:
    if not DATA_SOURCES.exists():
        _die(f"missing {DATA_SOURCES}")
    raw = yaml.safe_load(DATA_SOURCES.read_text(encoding="utf-8")) or {}
    sources = raw.get("sources", [])
    if not isinstance(sources, list) or not sources:
        _die(f"no sources defined in {DATA_SOURCES}")
    return sources


def _norm(s: str) -> str:
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s


def _pick_entry_time(entry: Any) -> dt.datetime | None:
    # feedparser yields either .published_parsed or .updated_parsed
    t = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
    if not t:
        return None
    return dt.datetime(*t[:6], tzinfo=dt.timezone.utc)


def _openrouter_key() -> str | None:
    # Keep secrets out of logs; just read env.
    return (os.environ.get("OPENROUTER_API_KEY") or "").strip() or None


_CJK_RE = re.compile(r"[\u4e00-\u9fff]")


def _has_cjk(s: str) -> bool:
    return bool(_CJK_RE.search(s or ""))


def _translate_en_to_zh(texts: list[str]) -> list[str]:
    """
    Translate a batch of short English snippets into natural Simplified Chinese.

    - Uses OpenRouter if `OPENROUTER_API_KEY` is available.
    - Returns a list of the same length/order.
    - If no API key, returns the original texts (so builds still work), but zh pages will fall back to English.
    """
    texts = [t.strip() for t in texts]
    if not texts:
        return []

    key = _openrouter_key()
    if not key:
        return texts

    model = (os.environ.get("DIGEST_TRANSLATE_MODEL") or DEFAULT_TRANSLATE_MODEL).strip()
    fallback_models_env = (os.environ.get("DIGEST_TRANSLATE_MODEL_FALLBACKS") or "").strip()
    fallback_models = (
        [m.strip() for m in fallback_models_env.split(",") if m.strip()]
        if fallback_models_env
        else list(DEFAULT_TRANSLATE_FALLBACK_MODELS)
    )
    models_to_try = [model] + [m for m in fallback_models if m != model]

    url = "https://openrouter.ai/api/v1/chat/completions"

    sys_msg = (
        "You are a professional bilingual editor. "
        "Translate English to natural, concise Simplified Chinese. "
        "Keep proper nouns/brand names as-is if commonly used. "
        "No hype, no emojis, no explanations."
    )

    # Single-item requests are much more reliable if we don't force JSON.
    messages: list[dict[str, str]]
    if len(texts) == 1:
        messages = [
            {"role": "system", "content": sys_msg},
            {
                "role": "user",
                "content": (
                    "Translate this to Simplified Chinese. Return ONLY the translated text.\n\n"
                    f"{texts[0]}"
                ),
            },
        ]
    else:
        messages = [
            {"role": "system", "content": sys_msg},
            {
                "role": "user",
                "content": (
                    "Translate the following snippets to Simplified Chinese. "
                    "Return ONLY a JSON array of strings with the same order/length.\n\n"
                    + "\n".join(f"{i+1}. {t}" for i, t in enumerate(texts))
                ),
            },
        ]

    last_err: str | None = None
    for model_to_try in models_to_try:
        payload = {
            "model": model_to_try,
            "temperature": 0,
            "messages": messages,
        }

        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode("utf-8"),
            headers={
                "Authorization": f"Bearer {key}",
                "Content-Type": "application/json",
                # Recommended by OpenRouter, optional but helps attribution.
                "HTTP-Referer": DEFAULT_SITE_URL,
                "X-Title": "personal-site digest translator",
            },
            method="POST",
        )

        try:
            with urllib.request.urlopen(req, timeout=30) as resp:
                raw = resp.read().decode("utf-8", errors="replace")
        except Exception as e:
            # Try the next model (some are region-locked).
            last_err = str(e)
            print(f"warn: translate failed for {model_to_try} ({e}); trying fallback", file=sys.stderr)
            continue

        try:
            data = json.loads(raw)
            content = data["choices"][0]["message"]["content"]
        except Exception as e:
            last_err = str(e)
            print(
                f"warn: translate parse failed for {model_to_try} ({e}); trying fallback",
                file=sys.stderr,
            )
            continue

        # Success.
        break
    else:
        # Exhausted all models.
        print(f"warn: translate failed ({last_err}); leaving zh as EN for this run", file=sys.stderr)
        return texts

    content = content.strip()
    content = re.sub(r"^```(?:json)?\s*", "", content)
    content = re.sub(r"\s*```$", "", content)

    if len(texts) == 1:
        return [content.strip().strip("\"'")]

    # Extract the first JSON array. If the model doesn't comply, recursively split the batch.
    try:
        start = content.find("[")
        end = content.rfind("]")
        if start == -1 or end == -1 or end <= start:
            raise ValueError("no JSON array found")
        arr = json.loads(content[start : end + 1])
        if not isinstance(arr, list) or len(arr) != len(texts):
            raise ValueError("invalid JSON array length/type")
        return [str(s).strip() for s in arr]
    except Exception:
        # Retry with smaller batches to improve reliability.
        if len(texts) <= 5:
            return [_translate_en_to_zh([t])[0] for t in texts]
        mid = len(texts) // 2
        left = _translate_en_to_zh(texts[:mid])
        right = _translate_en_to_zh(texts[mid:])
        return left + right


def _autofill_digest_zh_fields(digest: dict[str, Any], *, now: dt.datetime) -> bool:
    """
    Fill missing *_zh fields in digest items in-place.
    Returns True if anything changed.

    Only fills what is needed for the on-site list (capped to max_items), so we don't translate
    large drafts unnecessarily.
    """
    items = digest.get("items", [])
    if not isinstance(items, list) or not items:
        return False

    # Match the site output: rank + cap first.
    items_sorted = sorted(
        items,
        key=lambda it: (
            _importance_score(it, now=now),
            _parse_published_utc(it) or dt.datetime.min.replace(tzinfo=dt.timezone.utc),
        ),
        reverse=True,
    )
    max_items = int(digest.get("max_items", 0) or DEFAULT_MAX_ITEMS_ON_SITE)
    items_sorted = items_sorted[:max_items]

    # If a previous run wrote back an English "translation", clear it so we can retry properly.
    for it in items_sorted:
        for k_zh, k_en in [
            ("title_zh", "title"),
            ("why_zh", "why"),
            ("takeaway_zh", "takeaway"),
        ]:
            v = (it.get(k_zh) or "").strip()
            src = (it.get(k_en) or "").strip()
            if v and (not _has_cjk(v)) and src and (not _has_cjk(src)):
                it[k_zh] = ""

    # Collect translation jobs (dedupe by source text to reduce API calls).
    jobs_by_text: dict[str, list[tuple[dict[str, Any], str]]] = {}
    for it in items_sorted:
        for k_en, k_zh in [
            ("title", "title_zh"),
            ("why", "why_zh"),
            ("takeaway", "takeaway_zh"),
        ]:
            if (it.get(k_zh) or "").strip():
                continue
            src = (it.get(k_en) or "").strip()
            if not src:
                continue
            jobs_by_text.setdefault(src, []).append((it, k_zh))

    if not jobs_by_text:
        return False

    # Batch translate with stable ordering.
    changed = False
    batch_size = int(os.environ.get("DIGEST_TRANSLATE_BATCH", "20") or "20")
    src_unique = list(jobs_by_text.keys())
    for i in range(0, len(src_unique), batch_size):
        chunk_src = src_unique[i : i + batch_size]
        zh_texts = _translate_en_to_zh(chunk_src)
        for src, zh in zip(chunk_src, zh_texts, strict=True):
            zh = (zh or "").strip()
            # Don't write back non-Chinese "translations" (e.g., when a batch fails and echoes EN).
            if not zh or (not _has_cjk(zh) and not _has_cjk(src)):
                continue
            for it, k_zh in jobs_by_text.get(src, []):
                if zh != (it.get(k_zh) or "").strip():
                    it[k_zh] = zh
                    changed = True

    return changed


def _clean_title(title: str) -> str:
    title = re.sub(r"\s+", " ", title or "").strip()
    # arXiv RSS titles sometimes include extra whitespace/line breaks.
    return title


def _strip_html(s: str) -> str:
    # RSS summaries often include HTML tags. Keep a plain-text snippet for drafting.
    s = html.unescape(s or "")
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _first_sentence(s: str, max_len: int = 160) -> str:
    s = re.sub(r"\s+", " ", (s or "").strip())
    if not s:
        return ""
    # Heuristic sentence split. Good enough for drafting.
    m = re.split(r"(?<=[.!?])\s+", s, maxsplit=1)
    out = (m[0] if m else s).strip()
    if len(out) > max_len:
        out = out[: max_len - 1].rstrip() + "…"
    return out


def _truncate(s: str, max_len: int) -> str:
    s = re.sub(r"\s+", " ", (s or "").strip())
    if len(s) <= max_len:
        return s
    return s[: max_len - 1].rstrip() + "…"


def _draft_items(
    sources: list[dict[str, Any]],
    since_utc: dt.datetime,
    max_items: int,
) -> list[dict[str, Any]]:
    items: list[dict[str, Any]] = []

    def fetch(src: dict[str, Any]) -> tuple[dict[str, Any], Any | None]:
        url = (src.get("url") or "").strip()
        if not url:
            return (src, None)
        try:
            return (src, feedparser.parse(url))
        except Exception as e:
            name = src.get("name", "").strip() or src.get("id", "").strip() or "unknown"
            print(f"warn: failed to fetch '{name}' ({url}): {e}", file=sys.stderr)
            return (src, None)

    # Fetch feeds concurrently; slow/blocked feeds shouldn't stall the whole daily run.
    with ThreadPoolExecutor(max_workers=FETCH_WORKERS) as ex:
        futs = [ex.submit(fetch, s) for s in sources]
        for fut in as_completed(futs):
            src, feed = fut.result()
            if not feed:
                continue
            for e in getattr(feed, "entries", []) or []:
                when = _pick_entry_time(e)
                if when and when < since_utc:
                    continue
                title = _clean_title(getattr(e, "title", "") or "")
                link = (getattr(e, "link", "") or "").strip()
                summary = _strip_html(getattr(e, "summary", "") or getattr(e, "description", "") or "")
                if not title or not link:
                    continue
                # Provide draft-level snippets (user should rewrite for quality).
                takeaway = _truncate(summary, 360) if summary else ""
                why = _first_sentence(summary, 140) if summary else ""
                items.append(
                    {
                        "category": src.get("category", "").strip() or "Misc",
                        "title": title,
                        "why": why,
                        "takeaway": takeaway,
                        "prompt": "",
                        "url": link,
                        "source": src.get("name", "").strip() or src.get("id", "").strip(),
                        "published_utc": when.isoformat() if when else "",
                        # Chinese fields are intentionally left blank for human curation.
                        "title_zh": "",
                        "why_zh": "",
                        "takeaway_zh": "",
                        "prompt_zh": "",
                    }
                )

    # Deduplicate by URL, then by normalized title.
    seen_url: set[str] = set()
    seen_title: set[str] = set()
    deduped: list[dict[str, Any]] = []
    for it in sorted(items, key=lambda x: x.get("published_utc", ""), reverse=True):
        url = it.get("url", "")
        tnorm = _norm(it.get("title", ""))
        if url in seen_url or tnorm in seen_title:
            continue
        seen_url.add(url)
        seen_title.add(tnorm)
        deduped.append(it)

    return deduped[: max_items or 200]


def _save_yaml(path: Path, payload: Any) -> None:
    path.write_text(yaml.safe_dump(payload, sort_keys=False, allow_unicode=True), encoding="utf-8")


def _load_digest_yaml(path: Path) -> dict[str, Any]:
    if not path.exists():
        _die(f"missing digest file: {path}")
    raw = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
    if not isinstance(raw, dict):
        _die(f"invalid digest yaml: {path}")
    return raw


def _group_by_category(items: list[dict[str, Any]]) -> list[tuple[str, list[dict[str, Any]]]]:
    groups: dict[str, list[dict[str, Any]]] = {}
    for it in items:
        cat = (it.get("category", "") or "Misc").strip()
        groups.setdefault(cat, []).append(it)
    # Preserve a sensible, stable ordering: CSS first, then AI, then others.
    order = [
        "Business & economy",
        "Tech industry",
        "Security",
        "CSS / AI & society",
        "AI frontier",
        "Digital health / health comm",
        "Education / learning sciences / edtech",
        "Misc",
    ]
    ordered: list[tuple[str, list[dict[str, Any]]]] = []
    for k in order:
        if k in groups:
            ordered.append((k, groups.pop(k)))
    for k in sorted(groups.keys()):
        ordered.append((k, groups[k]))
    return ordered


CAT_ZH = {
    "Business & economy": "商业 / 经济",
    "Tech industry": "科技 / 产业",
    "Security": "安全 / 风险",
    "CSS / AI & society": "CSS / AI 与社会",
    "AI frontier": "AI 前沿",
    "Digital health / health comm": "数字健康 / 健康传播",
    "Education / learning sciences / edtech": "教育 / 学习科学 / 教育技术",
    "Misc": "其他",
}


def _parse_published_utc(it: dict[str, Any]) -> dt.datetime | None:
    s = (it.get("published_utc") or "").strip()
    if not s:
        return None
    try:
        return dt.datetime.fromisoformat(s.replace("Z", "+00:00"))
    except Exception:
        return None


def _score_now_for_date(date: dt.date) -> dt.datetime:
    # Use a stable "now" anchored to the digest date so rebuilds are deterministic.
    return dt.datetime.combine(date, dt.time(23, 59, 59), tzinfo=dt.timezone.utc)


def _importance_score(it: dict[str, Any], *, now: dt.datetime | None = None) -> int:
    """
    Rank items for the website digest list.

    This is intentionally heuristic: source quality + policy/market/AI-infra keywords,
    with a small recency bump.
    """
    src = (it.get("source") or "").lower()
    title = (it.get("title") or "").lower()
    cat = (it.get("category") or "").lower()

    score = 0

    # Source quality / typical impact.
    if "reuters" in src:
        score += 90
    elif "financial times" in src or src.startswith("ft"):
        score += 85
    elif "economist" in src:
        score += 75
    elif "bbc" in src:
        score += 70
    elif "mit technology review" in src:
        score += 68
    elif "guardian" in src:
        score += 62
    elif "ars technica" in src:
        score += 60
    elif "techcrunch" in src:
        score += 55
    elif "the verge" in src:
        score += 52
    elif "hacker news" in src:
        score += 15
    elif "arxiv" in src:
        score += 20
    elif "hugging face" in src:
        score += 22
    else:
        score += 30

    # Category nudge.
    if "security" in cat:
        score += 18
    elif "business" in cat or "economy" in cat or "markets" in cat:
        score += 14
    elif "tech" in cat:
        score += 12
    elif "ai frontier" in cat:
        score += 10
    elif "ai & society" in cat or "css" in cat:
        score += 8

    # Keyword signals.
    kw = {
        # Macro / markets
        "inflation": 10,
        "interest rate": 10,
        "house prices": 7,
        "stocks": 6,
        "bond": 6,
        "tariff": 10,
        "trade": 8,
        "sanction": 8,
        # Regulation / governance
        "antitrust": 10,
        "regulator": 8,
        "privacy": 7,
        "data": 6,
        "water": 6,
        "energy": 6,
        # AI / platforms / infra
        "openai": 10,
        "chatgpt": 9,
        "anthropic": 8,
        "nvidia": 8,
        "meta": 7,
        "tiktok": 6,
        "youtube": 6,
        "netflix": 6,
        "semiconductor": 8,
        "asml": 8,
        "chip": 7,
        # Geopolitics / conflict
        "ukraine": 10,
        "russia": 9,
        "greenland": 8,
        "nato": 7,
        "drone": 7,
        # Security-ish
        "breach": 10,
        "hack": 10,
        "ransom": 10,
        "vulnerability": 10,
        "zero-day": 10,
    }
    for k, w in kw.items():
        if k in title:
            score += w

    # Recency: small bump for newer items.
    when = _parse_published_utc(it)
    if when:
        ref = now or dt.datetime.now(dt.timezone.utc)
        age_h = (ref - when).total_seconds() / 3600.0
        if age_h <= 6:
            score += 6
        elif age_h <= 12:
            score += 4
        elif age_h <= 24:
            score += 2

    return score


def _pick_lang(it: dict[str, Any], key_en: str, key_zh: str, lang: str) -> str:
    if lang == "zh":
        v = (it.get(key_zh) or "").strip()
        if v:
            return v
        # Fallback to English if Chinese is missing.
    return (it.get(key_en) or "").strip()


def _font_paths(lang: str) -> dict[str, Path]:
    """
    Pick fonts that actually contain the glyphs we need.

    For zh, we must use a CJK font, otherwise Pillow will render tofu boxes.
    """
    if lang == "zh":
        candidates = [
            ("/System/Library/Fonts/Hiragino Sans GB.ttc", "regular"),
            ("/System/Library/Fonts/STHeiti Medium.ttc", "bold"),
            ("/System/Library/Fonts/STHeiti Light.ttc", "regular"),
            ("/System/Library/Fonts/Supplemental/Songti.ttc", "regular"),
        ]
    else:
        # Prefer widely available macOS fonts.
        candidates = [
            ("/System/Library/Fonts/SFNS.ttf", "regular"),
            ("/System/Library/Fonts/SFNSRounded.ttf", "bold"),
            ("/System/Library/Fonts/SFNSMono.ttf", "mono"),
            ("/System/Library/Fonts/Supplemental/Arial.ttf", "regular"),
            ("/System/Library/Fonts/Supplemental/Arial Bold.ttf", "bold"),
            ("/System/Library/Fonts/Supplemental/Arial Unicode.ttf", "regular"),
            ("/System/Library/Fonts/Supplemental/Helvetica.ttc", "regular"),
        ]
    out: dict[str, Path] = {}
    for fp, role in candidates:
        p = Path(fp)
        if p.exists() and role not in out:
            out[role] = p
    return out


def _as_bool(v: str | None) -> bool:
    return (v or "").strip().lower() in {"1", "true", "yes", "y", "on"}


def _openrouter_headers(*, title: str) -> dict[str, str]:
    key = _openrouter_key()
    if not key:
        return {}
    return {
        "Authorization": f"Bearer {key}",
        # These are recommended by OpenRouter; keep them stable and non-sensitive.
        "HTTP-Referer": "https://long19950304.github.io",
        "X-Title": title,
        "Content-Type": "application/json",
    }


def _openrouter_generate_image_png(*, model: str, prompt: str) -> bytes:
    """
    Generate a single PNG via an image-capable OpenRouter chat model.

    Many Gemini image models return base64 in `choices[0].message.images[0].image_url.url`.
    """
    key = _openrouter_key()
    if not key:
        raise RuntimeError("OPENROUTER_API_KEY missing")

    url = "https://openrouter.ai/api/v1/chat/completions"
    payload = {
        "model": model,
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            "Generate ONE single image. OUTPUT MUST BE IMAGE ONLY.\n"
                            "ABSOLUTELY NO TEXT: no letters, no words, no numbers, no symbols, no watermark, no logo.\n"
                            + prompt
                        ),
                    }
                ],
            }
        ],
        "modalities": ["image", "text"],
        "temperature": 0,
    }

    req = urllib.request.Request(
        url,
        data=json.dumps(payload).encode("utf-8"),
        headers=_openrouter_headers(title="Personal Site Digest Card BG") or {"Authorization": f"Bearer {key}", "Content-Type": "application/json"},
    )

    try:
        with urllib.request.urlopen(req, timeout=90) as resp:
            raw = resp.read()
    except Exception as e:
        raise RuntimeError(f"OpenRouter image request failed: {e}") from e

    result = json.loads(raw.decode("utf-8"))
    message = result["choices"][0]["message"]
    images = message.get("images") or []
    if not images:
        raise RuntimeError("OpenRouter response missing images payload")

    image_data = images[0]
    image_url = None
    if isinstance(image_data, dict):
        if isinstance(image_data.get("image_url"), dict):
            image_url = image_data["image_url"].get("url")
        image_url = image_url or image_data.get("url") or image_data.get("data")
    else:
        image_url = image_data

    if not image_url or not str(image_url).startswith("data:image"):
        raise RuntimeError("Unexpected image payload format")

    b64 = str(image_url).split(",", 1)[1]
    return base64.b64decode(b64)


def _resize_to_cover(im: Image.Image, w: int, h: int) -> Image.Image:
    """
    Resize without distortion: scale to cover target, then center-crop.
    This avoids "stretched" backgrounds when the source aspect ratio differs.
    """
    im = im.convert("RGB")
    scale = max(w / im.width, h / im.height)
    new_size = (max(1, int(round(im.width * scale))), max(1, int(round(im.height * scale))))
    im = im.resize(new_size, Image.Resampling.LANCZOS)
    left = max(0, (im.width - w) // 2)
    top = max(0, (im.height - h) // 2)
    return im.crop((left, top, left + w, top + h))


def _wrap(draw: ImageDraw.ImageDraw, text: str, font: ImageFont.FreeTypeFont, max_width: int) -> list[str]:
    """
    Simple word-wrapping that preserves spaces.

    Important: do NOT strip whitespace aggressively, otherwise words will run together
    in the rendered card (a common issue when splitting on whitespace).
    """
    text = re.sub(r"\s+", " ", (text or "").strip())
    if not text:
        return []

    words = text.split(" ")
    lines: list[str] = []
    cur = ""

    def flush() -> None:
        nonlocal cur
        if cur:
            lines.append(cur)
            cur = ""

    for w in words:
        test = w if not cur else f"{cur} {w}"
        if draw.textlength(test, font=font) <= max_width:
            cur = test
            continue

        # If a single token is too long, hard-wrap it by characters.
        if not cur and draw.textlength(w, font=font) > max_width:
            chunk = ""
            for ch in w:
                t2 = chunk + ch
                if draw.textlength(t2, font=font) <= max_width:
                    chunk = t2
                else:
                    if chunk:
                        lines.append(chunk)
                    chunk = ch
            if chunk:
                cur = chunk
            continue

        flush()
        cur = w

    flush()
    return lines


def _wrap_ellipsize(
    draw: ImageDraw.ImageDraw,
    text: str,
    font: ImageFont.FreeTypeFont,
    max_width: int,
    max_lines: int,
) -> list[str]:
    lines = _wrap(draw, text, font, max_width=max_width)
    if len(lines) <= max_lines:
        return lines
    lines = lines[:max_lines]
    last = lines[-1].rstrip()
    # Add ellipsis without overflowing too much.
    while last and draw.textlength(last + "…", font=font) > max_width:
        last = last[:-1].rstrip()
    lines[-1] = (last + "…") if last else "…"
    return lines


_URL_RE = re.compile(r"https?://\S+")


def _x_char_len(s: str) -> int:
    """
    Approximate X/Twitter length.

    X counts any URL as ~23 chars regardless of actual length. We approximate
    with 23 to split threads safely.
    """
    if not s:
        return 0
    urls = list(_URL_RE.findall(s))
    # Replace each URL with 23 chars in the count.
    return len(s) - sum(len(u) for u in urls) + 23 * len(urls)


def _digest_issue_urls(date: dt.date, site_url: str) -> dict[str, str]:
    base = (site_url or DEFAULT_SITE_URL).rstrip("/")
    return {
        "en": f"{base}/digest/{date.isoformat()}/",
        "zh": f"{base}/zh/digest/{date.isoformat()}/",
        "en_card": f"/assets/img/digests/{date.isoformat()}-en.png",
        "zh_card": f"/assets/img/digests/{date.isoformat()}-zh.png",
    }


def _format_item_bilingual(it: dict[str, Any]) -> str:
    t_en = (it.get("title") or "").strip()
    w_en = (it.get("why") or "").strip()
    take_en = (it.get("takeaway") or "").strip()
    p_en = (it.get("prompt") or "").strip()
    t_zh = (it.get("title_zh") or "").strip()
    w_zh = (it.get("why_zh") or "").strip()
    take_zh = (it.get("takeaway_zh") or "").strip()
    p_zh = (it.get("prompt_zh") or "").strip()
    url = (it.get("url") or "").strip()

    en = t_en
    if w_en:
        en = f"{en} — {w_en}"

    zh = t_zh or ""
    if zh and w_zh:
        zh = f"{zh} — {w_zh}"
    elif not zh and w_zh:
        zh = w_zh

    # Professional bulletin: one concise takeaway line (no "my take"/first-person labels).
    # LinkedIn will auto-link raw URLs, so we keep the URL on its own line.
    tl_en = _first_sentence(take_en, 200) if take_en else ""
    tl_zh = _first_sentence(take_zh, 140) if take_zh else ""

    lines: list[str] = []
    if en:
        lines.append(f"- {en} — {tl_en}" if tl_en else f"- {en}")
    if zh:
        lines.append(f"  {zh} — {tl_zh}" if tl_zh else f"  {zh}")
    if url:
        lines.append(f"  {url}")
    return "\n".join(lines).strip()


def _format_linkedin_post(
    date: dt.date,
    digest: dict[str, Any],
    site_url: str,
    *,
    text_mode: str = "bilingual",
    style: str = "bulletin",
) -> str:
    items = digest.get("items", [])
    groups = _group_by_category(items)
    urls = _digest_issue_urls(date, site_url)

    title_en = (digest.get("title") or f"Daily Digest — {date.isoformat()}").strip()
    title_zh = (digest.get("title_zh") or f"每日简报 — {date.isoformat()}").strip()
    note_en = (digest.get("note") or "").strip()
    note_zh = (digest.get("note_zh") or "").strip()

    def blurb(it: dict[str, Any], lang: str) -> str:
        # For social, prefer the short "why" line (less templated, avoids mid-sentence truncation).
        why = _pick_lang(it, "why", "why_zh", lang)
        if why:
            return why
        take = _pick_lang(it, "takeaway", "takeaway_zh", lang)
        return _first_sentence(take, 110 if lang != "zh" else 70)

    if style == "theme":
        # Theme-first format: a single short paragraph + a clean list of links.
        lines: list[str] = []
        if text_mode == "en":
            lines.extend(
                [
                    f"{title_en}",
                    "",
                    "Cards attached (EN + Chinese card).",
                    "",
                ]
            )
            if note_en:
                lines.extend([_truncate(note_en, 520), ""])
            lines.extend([f"Links (full list): {urls['en']}", "", "Links:", ""])
        else:
            lines.extend(
                [
                    f"{title_en} / {title_zh}",
                    "",
                    "EN + 中文 cards attached. Full lists:",
                    "附英文/中文两张卡片；完整列表：",
                    "",
                    f"EN list: {urls['en']}",
                    f"中文列表: {urls['zh']}",
                    "",
                ]
            )
            if note_en or note_zh:
                if note_en:
                    lines.append(_truncate(note_en, 520))
                if note_zh:
                    lines.append(_truncate(note_zh, 240))
                lines.append("")
            lines.extend(["Links / 链接：", ""])

        # Flatten items; keep it scannable.
        picks: list[tuple[str, str]] = []
        for _, cat_items in groups:
            for it in cat_items:
                t = (it.get("title") or "").strip()
                u = (it.get("url") or "").strip()
                if t and u:
                    picks.append((t, u))

        max_links = 10 if text_mode == "en" else 10
        for t, u in picks[:max_links]:
            lines.append(f"- {_truncate(t, 120)}")
            lines.append(f"  {u}")
        lines.append("")
        return "\n".join(lines).rstrip() + "\n"

    if text_mode == "en":
        lines: list[str] = [
            f"{title_en}",
            "",
            "Cards attached (EN + Chinese card).",
            "",
            f"Full list: {urls['en']}",
            "",
            "Highlights (clickable links):",
            "",
        ]
    else:
        lines = [
            f"{title_en} / {title_zh}",
            "",
            "EN + 中文 cards attached. Full lists:",
            "附英文/中文两张卡片；完整列表：",
        ]

        lines.extend(
            [
                "",
                f"EN list: {urls['en']}",
                f"中文列表: {urls['zh']}",
                "",
                "Bulletin (clickable links) / 快报（可点击链接）：",
                "",
            ]
        )

    if text_mode == "en":
        # LinkedIn EN-only: keep it scannable (no category blocks); rely on the attached card + full list.
        picks: list[dict[str, Any]] = []
        for _, cat_items in groups:
            for it in cat_items:
                t_en = (it.get("title") or "").strip()
                u = (it.get("url") or "").strip()
                if t_en and u:
                    picks.append(it)
        picks = picks[:3]

        for it in picks:
            t_en = (it.get("title") or "").strip()
            u = (it.get("url") or "").strip()
            b_en = _truncate(blurb(it, "en"), 140)
            head = f"- {_truncate(t_en, 120)}"
            if b_en:
                head += f" — {b_en}"
            lines.append(head)
            lines.append(f"  {u}")
            lines.append("")
    else:
        max_items_total = 10
        rendered = 0
        for cat, cat_items in groups:
            if not cat_items:
                continue
            lines.append(f"{cat} / {CAT_ZH.get(cat, cat)}")
            for it in cat_items:
                if rendered >= max_items_total:
                    break
                t_en = (it.get("title") or "").strip()
                t_zh = (it.get("title_zh") or "").strip()
                u = (it.get("url") or "").strip()
                if not t_en or not u:
                    continue
                b_en = blurb(it, "en")
                b_zh = blurb(it, "zh")
                head = f"- {t_en}"
                if t_zh and t_zh != t_en:
                    head += f" / {t_zh}"
                if b_en:
                    head += f" — {b_en}"
                if b_zh and b_zh != b_en:
                    head += f" / {b_zh}"
                lines.append(head)
                lines.append(f"  {u}")
                rendered += 1
            lines.append("")
            if rendered >= max_items_total:
                break

    if text_mode != "en":
        # Discussion prompts (max 2) at the end.
        prompts: list[tuple[str, str]] = []
        for it in items:
            p_en = (it.get("prompt") or "").strip()
            p_zh = (it.get("prompt_zh") or "").strip()
            if p_en or p_zh:
                prompts.append((p_en, p_zh))
        if prompts:
            lines.append("Open questions / 开放问题：")
            for p_en, p_zh in prompts[:2]:
                if p_en and p_zh:
                    lines.append(f"- {p_en} / {p_zh}")
                elif p_en:
                    lines.append(f"- {p_en}")
                elif p_zh:
                    lines.append(f"- {p_zh}")
            lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def _format_x_thread(
    date: dt.date,
    digest: dict[str, Any],
    site_url: str,
    *,
    text_mode: str = "bilingual",
    style: str = "bulletin",
) -> list[str]:
    items = digest.get("items", [])
    groups = _group_by_category(items)
    urls = _digest_issue_urls(date, site_url)
    note_en = (digest.get("note") or "").strip()
    note_zh = (digest.get("note_zh") or "").strip()

    if text_mode == "en":
        if style == "theme":
            # Theme tweet + link list in replies (so the first post stays readable).
            head: list[str] = [f"Daily Digest ({date.isoformat()}). Cards: EN + Chinese."]
            if note_en:
                head.append(_truncate(note_en, 240))
            head.append(f"Full list: {urls['en']}")
            head.append("Links below.")
            t0 = "\n".join(head).strip()

            lines: list[str] = []
            for _, cat_items in groups:
                for it in cat_items:
                    t = (it.get("title") or "").strip()
                    u = (it.get("url") or "").strip()
                    if t and u:
                        lines.append(f"{_truncate(t, 72)}: {u}")

            tweets: list[str] = [t0]
            cur: list[str] = []
            cur_len = 0
            max_len = 270
            for ln in lines[:12]:  # cap to avoid spammy threads
                # Use X-style length approximation (URLs ~23 chars) to pack more per tweet.
                add_len = _x_char_len(ln) + (1 if cur else 0)
                if cur and cur_len + add_len > max_len:
                    tweets.append("\n".join(cur).strip())
                    cur = [ln]
                    cur_len = _x_char_len(ln)
                else:
                    cur.append(ln)
                    cur_len += add_len
            if cur:
                tweets.append("\n".join(cur).strip())
            return tweets

        # Single post for X: keep it lightweight; attach BOTH cards here.
        picks: list[tuple[str, str]] = []
        for _, cat_items in groups:
            for it in cat_items:
                t = (it.get("title") or "").strip()
                u = (it.get("url") or "").strip()
                if t and u:
                    picks.append((t, u))
        picks = picks[:2]  # keep within X length limits

        lines = [
            f"Daily Digest ({date.isoformat()}). Cards: EN + Chinese.",
            f"Full list: {urls['en']}",
            "",
        ]
        for t, u in picks:
            # One-line items: readable, within length limits; details live on the digest page.
            lines.append(f"{_truncate(t, 72)}: {u}")
        return ["\n".join([ln for ln in lines if ln is not None]).strip()]

    if style == "theme":
        # Bilingual theme post + link list in replies.
        head: list[str] = [f"Daily Digest / 每日简报 ({date.isoformat()})"]
        head.append("Cards attached (EN + 中文).")
        if note_en:
            head.append(_truncate(note_en, 260))
        if note_zh:
            head.append(_truncate(note_zh, 200))
        head.append(f"EN list: {urls['en']}")
        head.append(f"中文列表: {urls['zh']}")
        head.append("Links below / 链接见串。")
        t0 = "\n".join(head).strip()

        lines: list[str] = []
        for _, cat_items in groups:
            for it in cat_items:
                t = (it.get("title") or "").strip()
                u = (it.get("url") or "").strip()
                if t and u:
                    lines.append(f"{_truncate(t, 72)}: {u}")

        tweets: list[str] = [t0]
        cur: list[str] = []
        cur_len = 0
        max_len = 270
        for ln in lines[:12]:
            add_len = _x_char_len(ln) + (1 if cur else 0)
            if cur and cur_len + add_len > max_len:
                tweets.append("\n".join(cur).strip())
                cur = [ln]
                cur_len = _x_char_len(ln)
            else:
                cur.append(ln)
                cur_len += add_len
        if cur:
            tweets.append("\n".join(cur).strip())
        return tweets

    # Tweet 1: short bilingual intro + canonical links (attach BOTH cards here).
    t0 = "\n".join(
        [
            f"Daily Digest / 每日简报 ({date.isoformat()})",
            "EN + 中文 cards attached.",
            f"EN: {urls['en']}",
            f"中文: {urls['zh']}",
            "",
            "Direct links in thread / 详细链接见串：",
        ]
    ).strip()

    tweets: list[str] = [t0]

    # Thread: compact per-item blocks (avoid orphan headings when splitting).
    blocks: list[str] = []
    for cat, cat_items in groups:
        if not cat_items:
            continue
        for it in cat_items[:10]:
            t_en = (it.get("title") or "").strip()
            w_en = (it.get("why") or "").strip()
            take_en = (it.get("takeaway") or "").strip()
            t_zh = (it.get("title_zh") or "").strip()
            w_zh = (it.get("why_zh") or "").strip()
            take_zh = (it.get("takeaway_zh") or "").strip()
            url = (it.get("url") or "").strip()

            cat_zh = CAT_ZH.get(cat, cat)
            cat_label = f"[{cat}]/{cat_zh}"

            tl_en = w_en or _first_sentence(take_en, 200)
            tl_zh = w_zh or _first_sentence(take_zh, 140)

            # Keep X readable: title + short notes + link.
            line = f"{cat_label} {t_en}".strip()
            if tl_en:
                line = f"{line}\nNote: {tl_en}"
            if t_zh:
                line = f"{line}\n{t_zh}"
            if tl_zh:
                line = f"{line}\n要点：{tl_zh}"
            if url:
                line = f"{line}\n{url}"
            blocks.append(line.strip())

    # Pack blocks into tweets with conservative length.
    cur: list[str] = []
    cur_len = 0
    max_len = 265  # keep headroom for platform differences
    for b in blocks:
        add = (b + "\n\n").strip() if b else ""
        if not add:
            continue
        add_len = _x_char_len(add) + (2 if cur else 0)
        if cur and cur_len + add_len > max_len:
            tweets.append("\n\n".join(cur).strip())
            cur = [b]
            cur_len = _x_char_len(b)
            continue
        if cur:
            cur_len += _x_char_len("\n\n" + b)
            cur.append(b)
        else:
            cur = [b]
            cur_len = _x_char_len(b)
    if cur:
        tweets.append("\n\n".join(cur).strip())

    return tweets


def render_card(date: dt.date, items: list[dict[str, Any]], out_path: Path, *, lang: str) -> None:
    # Default to square (near-1:1) to avoid excessive whitespace and to work well on both X and LinkedIn.
    # You can override with DIGEST_CARD_FORMAT=portrait (4:5).
    fmt = (os.environ.get("DIGEST_CARD_FORMAT") or "square").strip().lower()
    if fmt in {"portrait", "4:5"}:
        w, h = 1080, 1350
    else:
        w, h = 1080, 1080
    # Professional / restrained: clean background with subtle, consistent visual identity.
    # The background must never overpower the text.
    bg = (252, 252, 253)   # near-white
    ink = (15, 23, 42)     # slate-900
    sub = (51, 65, 85)     # slate-700
    faint = (226, 232, 240)
    accent = (37, 99, 235) # blue-600

    def _seed() -> int:
        # Stable per-date seed so the card "look" is consistent for a given issue.
        hexd = hashlib.sha256(date.isoformat().encode("utf-8")).hexdigest()[:8]
        return int(hexd, 16)

    def _make_background() -> Image.Image:
        # 1) Allow a user-provided background image (e.g., generated via an API) but soften it.
        bg_img = (os.environ.get("DIGEST_CARD_BG_IMAGE") or "").strip()
        if bg_img and Path(bg_img).exists():
            im = _resize_to_cover(Image.open(bg_img), w, h)
            # Soften and desaturate so text remains dominant.
            im = im.filter(ImageFilter.GaussianBlur(radius=10))
            im = ImageEnhance.Color(im).enhance(0.35)
            im = ImageEnhance.Brightness(im).enhance(1.15)
            im = ImageEnhance.Contrast(im).enhance(0.95)
            return im

        # 2) Optionally auto-generate a vivid background using an image-capable Gemini model
        # (then blur it so it behaves like a background, not a poster).
        if _as_bool(os.environ.get("DIGEST_CARD_BG_GENERATE")) and _openrouter_key():
            bg_cache_dir = CARD_DIR / "_bg"
            bg_cache_dir.mkdir(parents=True, exist_ok=True)
            cached = bg_cache_dir / f"{date.isoformat()}-bg.png"
            if cached.exists():
                im = _resize_to_cover(Image.open(cached), w, h)
            else:
                model = (os.environ.get("DIGEST_CARD_BG_MODEL") or "google/gemini-2.5-flash-image").strip()
                prompt = (os.environ.get("DIGEST_CARD_BG_PROMPT") or "").strip() or (
                    "Create an abstract, colorful background suitable for a professional news digest card. "
                    "Style: modern academic, rich color, soft gradients + subtle bokeh, faint data-network vibe. "
                    "No people, no objects, no logos. Square composition."
                )
                png = _openrouter_generate_image_png(model=model, prompt=prompt)
                cached.write_bytes(png)
                im = _resize_to_cover(Image.open(cached), w, h)

            # Make it behave like a background: blur + slight brighten; keep color but avoid overpowering text.
            im = im.filter(ImageFilter.GaussianBlur(radius=18))
            im = ImageEnhance.Color(im).enhance(0.80)
            im = ImageEnhance.Brightness(im).enhance(1.08)
            im = ImageEnhance.Contrast(im).enhance(0.95)
            return im

        # Default: subtle gradient + pattern.
        base = Image.new("RGB", (w, h), bg)
        # Light blue top → near-white bottom.
        top = Image.new("RGB", (w, h), (244, 247, 255))
        grad = Image.linear_gradient("L").resize((w, h))
        base = Image.composite(top, base, grad)

        style = (os.environ.get("DIGEST_CARD_STYLE") or "grid").strip().lower()
        rnd = random.Random(_seed())

        overlay = Image.new("RGBA", (w, h), (0, 0, 0, 0))
        od = ImageDraw.Draw(overlay)

        if style == "network":
            # Minimal "data network" in the top-right quadrant.
            nodes: list[tuple[int, int]] = []
            for _ in range(16):
                nx = rnd.randint(int(w * 0.52), w - 80)
                ny = rnd.randint(60, int(h * 0.45))
                nodes.append((nx, ny))
            for _ in range(20):
                a = rnd.choice(nodes)
                b = rnd.choice(nodes)
                if a == b:
                    continue
                od.line([a, b], fill=(*accent, 14), width=2)
            for (nx, ny) in nodes:
                od.ellipse((nx - 3, ny - 3, nx + 3, ny + 3), fill=(*accent, 30))
        else:
            # Subtle grid + dots (default).
            step = 60
            for gx in range(0, w + 1, step):
                od.line((gx, 0, gx, h), fill=(*accent, 8), width=2)
            for gy in range(0, h + 1, step):
                od.line((0, gy, w, gy), fill=(*accent, 6), width=2)
            for gx in range(step, w, step * 2):
                for gy in range(step, h, step * 2):
                    od.ellipse((gx - 2, gy - 2, gx + 2, gy + 2), fill=(*accent, 14))

        return Image.alpha_composite(base.convert("RGBA"), overlay).convert("RGB")

    img = _make_background()
    draw = ImageDraw.Draw(img)

    fonts = _font_paths(lang)
    font_title = ImageFont.truetype(str(fonts.get("bold", fonts.get("regular"))), 50) if fonts else ImageFont.load_default()
    font_h2 = ImageFont.truetype(str(fonts.get("bold", fonts.get("regular"))), 26) if fonts else ImageFont.load_default()
    font_small = ImageFont.truetype(str(fonts.get("regular", fonts.get("bold"))), 21) if fonts else ImageFont.load_default()
    font_item_title = ImageFont.truetype(str(fonts.get("bold", fonts.get("regular"))), 26) if fonts else ImageFont.load_default()
    font_item_body = ImageFont.truetype(str(fonts.get("regular", fonts.get("bold"))), 22) if fonts else ImageFont.load_default()
    font_mono = (
        ImageFont.truetype(str(fonts["mono"]), 20)
        if fonts and "mono" in fonts
        else font_small
    )

    # Slightly smaller margins so the "background" doesn't feel like extra empty area on square cards.
    pad = 44
    x = pad
    y = 44

    def round_rect_outline(x0: int, y0: int, x1: int, y1: int, r: int) -> None:
        # Monochrome pill with a thin outline (more "academic", less "poster-like").
        draw.rounded_rectangle((x0, y0, x1, y1), radius=r, outline=faint, width=2, fill=bg)

    # We build the content first, then draw a panel that fits the content height.
    # This avoids large empty space on square cards.
    content_top = 28
    content_left = pad - 12
    content_right = w - pad + 12

    # Header accent mark (brand cue).
    draw.rounded_rectangle((x - 10, y + 10, x - 4, y + 44), radius=4, fill=accent)

    title = "Daily Digest" if lang != "zh" else "每日简报"
    draw.text((x, y), title, fill=ink, font=font_title)
    y += 54

    date_str = date.isoformat()
    subtitle = "Top stories + sources (see full list on site)." if lang != "zh" else "今日要点与来源（完整版见网站）。"
    draw.text((x, y), date_str + "  ·  " + subtitle, fill=sub, font=font_small)
    y += 34

    # Top rule
    draw.line((x, y, w - pad, y), fill=faint, width=2)
    y += 20

    # Keep cards low-cognitive-load: show only top N items (titles + source), no long blurbs.
    default_max = 8 if h == w else 6
    max_items_total = int((os.environ.get("DIGEST_CARD_MAX_ITEMS") or "").strip() or default_max)
    now = _score_now_for_date(date)
    ranked = sorted(
        items,
        key=lambda it: (
            _importance_score(it, now=now),
            _parse_published_utc(it) or dt.datetime.min.replace(tzinfo=dt.timezone.utc),
        ),
        reverse=True,
    )[: max_items_total]

    y_items_start = y
    for idx, it in enumerate(ranked, start=1):
        t = _pick_lang(it, "title", "title_zh", lang)
        if not t:
            continue
        src = (it.get("source") or "").strip()
        cat_raw = (it.get("category") or "").strip()
        cat = (CAT_ZH.get(cat_raw, cat_raw) if lang == "zh" else cat_raw)

        num = f"{idx}."
        draw.text((x, y + 1), num, fill=sub, font=font_h2)
        tx = x + 46
        t_lines = _wrap_ellipsize(draw, t, font_item_title, max_width=w - pad - tx, max_lines=2)
        for ln in t_lines:
            draw.text((tx, y), ln, fill=ink, font=font_item_title)
            y += 34

        meta = " · ".join([m for m in [cat, src] if m]) if (cat or src) else ""
        if meta:
            draw.text((tx, y - 4), meta, fill=sub, font=font_small)
            y += 28

        y += 10
        if y > h - 120:
            break

    # Two-line footer: render the URL in mono to reduce OCR/link-detection mistakes
    # (some apps insert a space between letters and digits).
    footer1 = "Zhilong George Zhao · @longlalaland"
    digest_path = f"/digest/{date.isoformat()}/" if lang != "zh" else f"/zh/digest/{date.isoformat()}/"
    footer2 = DEFAULT_SITE_URL + digest_path
    footer_h = 54
    footer_gap = 18
    content_bottom = min(h - 52, y + footer_gap + footer_h + 16)

    # A translucent content panel keeps the text dominant even with vivid backgrounds.
    panel = Image.new("RGBA", (w, h), (0, 0, 0, 0))
    pd = ImageDraw.Draw(panel)
    pd.rounded_rectangle(
        (content_left, content_top, content_right, content_bottom),
        radius=28,
        fill=(255, 255, 255, 232),
        outline=(*faint, 255),
        width=2,
    )
    img = Image.alpha_composite(img.convert("RGBA"), panel).convert("RGB")
    draw = ImageDraw.Draw(img)

    # Re-draw content on top of the panel (panel was composited after the first pass).
    y = 52
    draw.rounded_rectangle((x - 10, y + 10, x - 4, y + 44), radius=4, fill=accent)
    draw.text((x, y), title, fill=ink, font=font_title)
    y += 54
    draw.text((x, y), date_str + "  ·  " + subtitle, fill=sub, font=font_small)
    y += 34
    draw.line((x, y, w - pad, y), fill=faint, width=2)
    y += 20

    for idx, it in enumerate(ranked, start=1):
        t = _pick_lang(it, "title", "title_zh", lang)
        if not t:
            continue
        src = (it.get("source") or "").strip()
        cat_raw = (it.get("category") or "").strip()
        cat = (CAT_ZH.get(cat_raw, cat_raw) if lang == "zh" else cat_raw)

        num = f"{idx}."
        draw.text((x, y + 1), num, fill=sub, font=font_h2)
        tx = x + 46
        t_lines = _wrap_ellipsize(draw, t, font_item_title, max_width=w - pad - tx, max_lines=2)
        for ln in t_lines:
            draw.text((tx, y), ln, fill=ink, font=font_item_title)
            y += 34

        meta = " · ".join([m for m in [cat, src] if m]) if (cat or src) else ""
        if meta:
            draw.text((tx, y - 4), meta, fill=sub, font=font_small)
            y += 28

        y += 10
        if y > content_bottom - 54:
            break

    # Footer (inside the panel; compact).
    fy = content_bottom - 50
    draw.line((x, fy - 14, w - pad, fy - 14), fill=faint, width=2)
    draw.text((x, fy - 2), footer1, fill=sub, font=font_small)
    draw.text((x, fy + 20), footer2, fill=sub, font=font_mono)

    out_path.parent.mkdir(parents=True, exist_ok=True)
    img.save(out_path, format="PNG", optimize=True, compress_level=9)
    _optimize_png(out_path)


def _md_for_lang(date: dt.date, digest: dict[str, Any], *, lang: str) -> Path:
    items = digest.get("items", [])
    if not isinstance(items, list):
        _die("digest.items must be a list")
    if not items:
        _die("digest has no items; skip build or add items first")

    # Markdown page (collection item)
    out_dir = ZH_DIGESTS_DIR if lang == "zh" else DIGESTS_DIR
    out_dir.mkdir(parents=True, exist_ok=True)
    md_path = out_dir / f"{date.isoformat()}.md"

    title = digest.get("title") or f"Daily Digest - {date.isoformat()}"
    if lang == "zh":
        title = digest.get("title_zh") or f"每日简报 - {date.isoformat()}"

    # Sort by importance (global list, not grouped).
    now = _score_now_for_date(date)
    ranked = sorted(
        items,
        key=lambda it: (
            _importance_score(it, now=now),
            _parse_published_utc(it) or dt.datetime.min.replace(tzinfo=dt.timezone.utc),
        ),
        reverse=True,
    )
    max_items = int(digest.get("max_items", 0) or DEFAULT_MAX_ITEMS_ON_SITE)

    # For zh pages, ensure visible text is Chinese (don't fall back to EN).
    if lang == "zh":
        items_sorted: list[dict[str, Any]] = []
        for it in ranked:
            t_zh = (it.get("title_zh") or "").strip()
            if not t_zh or not _has_cjk(t_zh):
                continue
            items_sorted.append(it)
            if len(items_sorted) >= max_items:
                break
    else:
        items_sorted = ranked[:max_items]

    top = items_sorted[:10]

    note = (digest.get("note_zh") if lang == "zh" else digest.get("note")) or ""
    note = note.strip()

    lines: list[str] = [
        "---",
        "layout: page",
        f'title: "{title}"',
        f"lang: {lang}",
        f"permalink: /{'zh/' if lang == 'zh' else ''}digest/{date.isoformat()}/",
        f"digest_date: {date.isoformat()}",
        f"translation_key: digest-{date.isoformat()}",
        "---",
        "",
    ]
    if note:
        lines.extend([note, ""])

    def _cat_label(v: str) -> str:
        v = (v or "Misc").strip() or "Misc"
        return v if lang != "zh" else CAT_ZH.get(v, v)

    def _short(it: dict[str, Any]) -> str:
        if lang == "zh":
            why = (it.get("why_zh") or "").strip()
            if why and _has_cjk(why):
                return why
            take = (it.get("takeaway_zh") or "").strip()
            return _first_sentence(take, 120) if take and _has_cjk(take) else ""
        why = _pick_lang(it, "why", "why_zh", lang)
        if why:
            return why
        take = _pick_lang(it, "takeaway", "takeaway_zh", lang)
        return _first_sentence(take, 160 if lang != "zh" else 120)

    def _pub(it: dict[str, Any]) -> str:
        when = _parse_published_utc(it)
        if not when:
            return ""
        return when.strftime("%Y-%m-%d %H:%M UTC")

    lines.extend(["## Top 10" if lang != "zh" else "## 最重要的 10 条", ""])

    for idx, it in enumerate(top, start=1):
        t = (it.get("title_zh") or "").strip() if lang == "zh" else _pick_lang(it, "title", "title_zh", lang)
        u = (it.get("url") or "").strip()
        if not t or not u:
            continue
        meta = " | ".join(
            x
            for x in [
                _cat_label(it.get("category", "")),
                (it.get("source") or "").strip(),
                _pub(it),
            ]
            if x
        )
        lines.append(f"{idx}. [{t}]({u})" + (f" — {meta}" if meta else ""))
        blurb = _short(it)
        if blurb:
            lines.append(f"   - {blurb}")
        lines.append("")

    lines.extend(["## Full list" if lang != "zh" else "## 全部列表", ""])
    if lang == "zh":
        lines.append("| # | 标题 | 类别 | 来源 | 时间 (UTC) |")
    else:
        lines.append("| # | Title | Category | Source | Published (UTC) |")
    lines.append("|---:|---|---|---|---|")
    for idx, it in enumerate(items_sorted, start=1):
        t = (it.get("title_zh") or "").strip() if lang == "zh" else _pick_lang(it, "title", "title_zh", lang)
        u = (it.get("url") or "").strip()
        if not t or not u:
            continue
        cat = _cat_label(it.get("category", ""))
        src = (it.get("source") or "").strip()
        pub = _pub(it)
        lines.append(f"| {idx} | [{t}]({u}) | {cat} | {src} | {pub} |")

    md_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
    return md_path


def build_digest(date: dt.date, digest: dict[str, Any]) -> list[tuple[str, Path]]:
    out: list[tuple[str, Path]] = []
    out.append(("en", _md_for_lang(date, digest, lang="en")))
    out.append(("zh", _md_for_lang(date, digest, lang="zh")))

    # Social cards (EN + ZH). These are referenced by `digest social`.
    items = digest.get("items", [])
    if isinstance(items, list) and items:
        render_card(date, items, CARD_DIR / f"{date.isoformat()}-en.png", lang="en")
        render_card(date, items, CARD_DIR / f"{date.isoformat()}-zh.png", lang="zh")

    return out


def cmd_draft(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    sources = _load_sources()
    # Since: last N hours from now (UTC). This keeps drafts useful for daily runs.
    since_utc = dt.datetime.now(dt.timezone.utc) - dt.timedelta(hours=args.hours)
    items = _draft_items(sources, since_utc=since_utc, max_items=args.max_items)

    out = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if out.exists() and not args.force:
        out = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"

    payload = {
        "date": date.isoformat(),
        "title": f"Daily Digest - {date.isoformat()}",
        "title_zh": f"每日简报 - {date.isoformat()}",
        "note": "",
        "note_zh": "",
        "items": items,
    }
    _save_yaml(out, payload)
    print(f"Wrote {out.relative_to(REPO_ROOT)} ({len(items)} items).")
    print("Next: edit `why` (short), `takeaway` (1–2 sentences), `prompt` (1 question), then run:")
    print(f"  {REPO_ROOT}/bin/digest build --date {date.isoformat()}")


def cmd_build(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    digest_path = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if not digest_path.exists():
        # If only draft exists, use it.
        draft = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"
        if draft.exists():
            digest_path = draft
        else:
            _die(f"missing digest YAML for {date.isoformat()} (expected {digest_path})")
    digest = _load_digest_yaml(digest_path)
    # Auto-fill missing zh fields so the Chinese site is fully Chinese by default.
    if _autofill_digest_zh_fields(digest, now=_score_now_for_date(date)):
        _save_yaml(digest_path, digest)
    results = build_digest(date, digest)
    for lang, md_path in results:
        print(f"Wrote {md_path.relative_to(REPO_ROOT)} ({lang})")


def cmd_social(args: argparse.Namespace) -> None:
    _ensure_dirs()
    date = _parse_date(args.date)
    digest_path = DATA_DIGESTS_DIR / f"{date.isoformat()}.yml"
    if not digest_path.exists():
        draft = DATA_DIGESTS_DIR / f"{date.isoformat()}.draft.yml"
        if draft.exists():
            digest_path = draft
        else:
            _die(f"missing digest YAML for {date.isoformat()} (expected {digest_path})")

    digest = _load_digest_yaml(digest_path)
    # Social requirement: each post should include BOTH EN + ZH card images.
    card_en = CARD_DIR / f"{date.isoformat()}-en.png"
    card_zh = CARD_DIR / f"{date.isoformat()}-zh.png"
    print("==== Attach images (EN + ZH) ====")
    print(f"EN card: {card_en}")
    print(f"ZH card: {card_zh}")
    if not card_en.exists() or not card_zh.exists():
        print("(Tip) Run `./bin/digest build --date YYYY-MM-DD` first to generate the cards.")
    print("")

    if args.platform in ("linkedin", "both"):
        print("==== LinkedIn (copy/paste) ====")
        print(
            _format_linkedin_post(
                date,
                digest,
                site_url=args.site_url,
                text_mode=args.text,
                style=args.style,
            )
        )

    if args.platform in ("x", "both"):
        print("==== X thread (copy/paste) ====")
        tweets = _format_x_thread(
            date,
            digest,
            site_url=args.site_url,
            text_mode=args.text,
            style=args.style,
        )
        total = len(tweets)
        for i, t in enumerate(tweets, start=1):
            print(f"\n--- Tweet {i}/{total} ---\n{t}\n")


def main(argv: list[str]) -> int:
    parser = argparse.ArgumentParser(prog="digest")
    sub = parser.add_subparsers(dest="cmd", required=True)

    p_draft = sub.add_parser("draft", help="Draft a digest YAML from RSS/Atom sources")
    p_draft.add_argument("--date", required=True, help="YYYY-MM-DD (label for this issue)")
    p_draft.add_argument("--hours", type=int, default=36, help="Look back window (UTC), default: 36")
    p_draft.add_argument("--max-items", type=int, default=40, help="Max candidate items to write, default: 40")
    p_draft.add_argument("--force", action="store_true", help="Overwrite existing YYYY-MM-DD.yml")
    p_draft.set_defaults(func=cmd_draft)

    p_build = sub.add_parser("build", help="Build digest page from YAML")
    p_build.add_argument("--date", required=True, help="YYYY-MM-DD")
    p_build.set_defaults(func=cmd_build)

    p_social = sub.add_parser("social", help="Generate social post text (LinkedIn + X)")
    p_social.add_argument("--date", required=True, help="YYYY-MM-DD")
    p_social.add_argument(
        "--platform",
        choices=["both", "linkedin", "x"],
        default="both",
        help="Which post text to emit, default: both",
    )
    p_social.add_argument(
        "--text",
        choices=["en", "bilingual"],
        default="bilingual",
        help="Text mode for posts; note: cards are always EN+ZH. Default: bilingual",
    )
    p_social.add_argument(
        "--style",
        choices=["bulletin", "theme"],
        default="bulletin",
        help="Post layout: bulletin (default) or theme-first (theme + links).",
    )
    p_social.add_argument(
        "--site-url",
        default=DEFAULT_SITE_URL,
        help=f"Canonical site URL for digest links, default: {DEFAULT_SITE_URL}",
    )
    p_social.set_defaults(func=cmd_social)

    args = parser.parse_args(argv)
    args.func(args)
    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
